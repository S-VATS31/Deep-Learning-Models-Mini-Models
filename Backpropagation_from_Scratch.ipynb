{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPAT/KlkfTbvZaVyu5gD0R4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S-VATS31/Deep-Learning-Models-Mini-Models/blob/main/Backpropagation_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Numpy**\n"
      ],
      "metadata": {
        "id": "12V3wJQSFclR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9-Xeh3kb4dF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network and Forward/Backward Propagation from Scratch**"
      ],
      "metadata": {
        "id": "bdeWXam6FgV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes):\n",
        "        # Set up Number of Layers and Layer Sizes\n",
        "        self.layers = len(layer_sizes)\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.params = self.initialize_params()\n",
        "\n",
        "    # Initialize Parameters\n",
        "    def initialize_params(self):\n",
        "        params = {}\n",
        "        for layer in range(1, self.layers):\n",
        "            # Xavier Initialization\n",
        "            params[f'W{layer}'] = np.random.randn(self.layer_sizes[layer], self.layer_sizes[layer-1]) * np.sqrt(2 / self.layer_sizes[layer-1])\n",
        "            params[f'b{layer}'] = np.zeros((self.layer_sizes[layer], 1))\n",
        "        return params\n",
        "\n",
        "    def weighted_sum(self, A_prev, W, b):\n",
        "        # Ensure A_prev is properly shaped (features Ã— examples)\n",
        "        if len(A_prev.shape) == 2 and A_prev.shape[1] != W.shape[1]:\n",
        "            A_prev = A_prev.T\n",
        "        Z = np.dot(W, A_prev) + b # Weighted Sum Calculation\n",
        "        return Z\n",
        "\n",
        "    def sigmoid_activation(self, Z): # Sigmoid Function\n",
        "        A = 1.0 / (1.0 + np.exp(-Z))\n",
        "        return A\n",
        "\n",
        "    def derivative_sigmoid(self, A): # Derivative of Sigmoid Function\n",
        "        dA = A * (1 - A)\n",
        "        return dA\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        # X should be features x examples\n",
        "        if X.shape[0] != self.layer_sizes[0]:\n",
        "            X = X.T\n",
        "\n",
        "        A = X # Set Initial Activation Values\n",
        "        cache = {'A0': A} # Set up Cache for Backpropagation\n",
        "\n",
        "        for layer in range(1, self.layers):\n",
        "            A_prev = A # Activation of Layer A - 1\n",
        "            W = self.params[f'W{layer}']\n",
        "            b = self.params[f'b{layer}']\n",
        "\n",
        "            Z = self.weighted_sum(A_prev, W, b) # Call Weighted sum Function\n",
        "            A = self.sigmoid_activation(Z) # Call Sigmoid Activation Function\n",
        "\n",
        "            cache[f'Z{layer}'] = Z # Store Z values in cache (Z)\n",
        "            cache[f'A{layer}'] = A # Store Activations in Cache (A)\n",
        "\n",
        "        return A, cache\n",
        "\n",
        "    def backward_propagation(self, X, y, alpha):\n",
        "        # Transpose if needed\n",
        "        if X.shape[0] != self.layer_sizes[0]:\n",
        "            X = X.T\n",
        "        if y.shape[0] != self.layer_sizes[-1]:\n",
        "            y = y.T\n",
        "\n",
        "        m = X.shape[1] # Number of examples\n",
        "        A_final, cache = self.forward_propagation(X) # Call forward propagation function\n",
        "\n",
        "        # Initialize dA for output layer\n",
        "        dA = (2/m) * (A_final - y) # Derivative of Loss W.R.T Output\n",
        "\n",
        "        for layer in range(self.layers - 1, 0, -1):\n",
        "            # Get the activation from previous layer\n",
        "            A_prev = cache[f'A{layer-1}'] if layer > 1 else X\n",
        "\n",
        "            # Calculate gradients\n",
        "            dZ = dA * self.derivative_sigmoid(cache[f'A{layer}'])\n",
        "            dW = (1/m) * np.dot(dZ, A_prev.T)\n",
        "            db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "\n",
        "            # Update parameters\n",
        "            self.params[f'W{layer}'] -= alpha * dW\n",
        "            self.params[f'b{layer}'] -= alpha * db\n",
        "\n",
        "            # Prepare dA for previous layer\n",
        "            if layer > 1:\n",
        "                dA = np.dot(self.params[f'W{layer}'].T, dZ)\n",
        "\n",
        "layer_sizes = [3, 5, 1]\n",
        "nn = NeuralNetwork(layer_sizes)\n",
        "\n",
        "# Random Data\n",
        "X = np.random.randn(10, 3) # 10 examples, 3 features\n",
        "y = np.random.randn(10, 1) # 10 examples, 1 output\n",
        "\n",
        "# Training Loop\n",
        "for i in range(1000):\n",
        "    A_final, _ = nn.forward_propagation(X)\n",
        "    nn.backward_propagation(X, y, alpha=1e-4)"
      ],
      "metadata": {
        "id": "lo1unn3ppNmM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}