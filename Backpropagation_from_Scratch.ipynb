{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOYwjMaOhWGVTtystRXWlqR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S-VATS31/Deep-Learning-Models-Mini-Models/blob/main/Backpropagation_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Numpy**\n"
      ],
      "metadata": {
        "id": "12V3wJQSFclR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N9-Xeh3kb4dF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network and Forward/Backward Propagation from Scratch**"
      ],
      "metadata": {
        "id": "bdeWXam6FgV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes):\n",
        "        # Set up Number of Layers and Layer Sizes\n",
        "        self.layers = len(layer_sizes)\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.params = self.initialize_params()\n",
        "\n",
        "    # Initialize Parameters\n",
        "    def initialize_params(self):\n",
        "        params = {}\n",
        "        for layer in range(1, self.layers):\n",
        "            # Xavier Initialization\n",
        "            params[f'W{layer}'] = np.random.randn(self.layer_sizes[layer], self.layer_sizes[layer-1]) * np.sqrt(2 / self.layer_sizes[layer-1])\n",
        "            params[f'b{layer}'] = np.zeros((self.layer_sizes[layer], 1))\n",
        "        return params\n",
        "\n",
        "    def weighted_sum(self, A_prev, W, b):\n",
        "        # Ensure A_prev is properly shaped (features Ã— examples)\n",
        "        if len(A_prev.shape) == 2 and A_prev.shape[1] != W.shape[1]:\n",
        "            A_prev = A_prev.T\n",
        "        Z = np.dot(W, A_prev) + b # Weighted Sum Calculation\n",
        "        return Z\n",
        "\n",
        "    def sigmoid_activation(self, Z): # Sigmoid Function\n",
        "        A = 1.0 / (1.0 + np.exp(-Z))\n",
        "        return A\n",
        "\n",
        "    def derivative_sigmoid(self, A): # Derivative of Sigmoid Function\n",
        "        dA = A * (1 - A)\n",
        "        return dA\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        # X should be features x examples\n",
        "        if X.shape[0] != self.layer_sizes[0]:\n",
        "            X = X.T\n",
        "\n",
        "        A = X # Set Initial Activation Values\n",
        "        cache = {'A0': A} # Set up Cache for Backpropagation\n",
        "\n",
        "        for layer in range(1, self.layers):\n",
        "            A_prev = A  # Activation of Layer A - 1\n",
        "            W = self.params[f'W{layer}']\n",
        "            b = self.params[f'b{layer}']\n",
        "\n",
        "            Z = self.weighted_sum(A_prev, W, b) # Call Weighted sum Function\n",
        "            A = self.sigmoid_activation(Z) # Call Sigmoid Activation Function\n",
        "\n",
        "            cache[f'Z{layer}'] = Z # Store Z values in cache (Z)\n",
        "            cache[f'A{layer}'] = A # Store Activations in Cache (A)\n",
        "\n",
        "        return A, cache\n",
        "\n",
        "    def backward_propagation(self, X, y, alpha):\n",
        "        # Transpose if needed\n",
        "        if X.shape[0] != self.layer_sizes[0]:\n",
        "            X = X.T\n",
        "        if y.shape[0] != self.layer_sizes[-1]:\n",
        "            y = y.T\n",
        "\n",
        "        m = X.shape[1]  # Number of examples\n",
        "        A_final, cache = self.forward_propagation(X)  # Call forward propagation function\n",
        "\n",
        "        # Initialize dA for output layer\n",
        "        dA = (2/m) * (A_final - y)  # Derivative of Loss W.R.T Output\n",
        "\n",
        "        for layer in range(self.layers - 1, 0, -1):\n",
        "            # Get the activation from previous layer\n",
        "            A_prev = cache[f'A{layer-1}'] if layer > 1 else X\n",
        "\n",
        "            # Calculate gradients\n",
        "            dZ = dA * self.derivative_sigmoid(cache[f'A{layer}'])\n",
        "            dW = (1/m) * np.dot(dZ, A_prev.T)\n",
        "            db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "\n",
        "            # Update parameters\n",
        "            self.params[f'W{layer}'] -= alpha * dW\n",
        "            self.params[f'b{layer}'] -= alpha * db\n",
        "\n",
        "            # Prepare dA for previous layer\n",
        "            if layer > 1:\n",
        "                dA = np.dot(self.params[f'W{layer}'].T, dZ)\n",
        "\n",
        "layer_sizes = [3, 5, 1]\n",
        "nn = NeuralNetwork(layer_sizes)\n",
        "\n",
        "# Random Data\n",
        "X = np.random.randn(10, 3)  # 10 examples, 3 features\n",
        "y = np.random.randn(10, 1)  # 10 examples, 1 output\n",
        "\n",
        "# Training Loop\n",
        "for i in range(1000):\n",
        "    A_final, _ = nn.forward_propagation(X)\n",
        "    nn.backward_propagation(X, y, alpha=1e-4)"
      ],
      "metadata": {
        "id": "lo1unn3ppNmM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "a656b2ed-965d-45cd-d504-63610be25ea0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "shapes (5,3) and (10,3) not aligned: 3 (dim 1) != 10 (dim 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6339c601ad52>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Training Loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mA_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-6339c601ad52>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'b{layer}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweighted_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call Weighted sum Function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call Sigmoid Activation Function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-6339c601ad52>\u001b[0m in \u001b[0;36mweighted_sum\u001b[0;34m(self, A_prev, W, b)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mA_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m \u001b[0;31m# Weighted Sum Calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (5,3) and (10,3) not aligned: 3 (dim 1) != 10 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**X** = Feature Matrix\n",
        "\n",
        "**y** = True Output\n",
        "\n"
      ],
      "metadata": {
        "id": "Rp-PAxaRL9qY"
      }
    }
  ]
}