{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN86ilfxlM7ZIKvORPAHj62"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_uhpVYHRmZR3"},"outputs":[],"source":["import numpy as np\n"]},{"cell_type":"code","source":["def gradient_descent(X, y, theta_weight, alpha=1e-3, epochs=200, tol=1e-7):\n","    m = len(X)\n","    loss_over_epochs = []\n","    residual_loss = float('inf')\n","\n","    # Training Loop\n","    for epoch in range(epochs):\n","        gradients = (1/m) * X.T @ (X @ theta_weight - y) # Gradient Computation\n","        theta_weight -= alpha * gradients # Gradient Descent Update Rule\n","        cost_func = (1/(2*m)) * np.sum((X @ theta_weight - y) ** 2) # Mean Squared Error Loss Function\n","        loss_over_epochs.append(cost_func) # Plotting for Visualization\n","\n","        print(f\"Epochs: {epoch+1}/{epochs}, Loss: {cost_func:.4f}\")\n","\n","        # Early Stopping\n","        if abs(residual_loss - cost_func) < tol: # Tolerance level\n","            print(f\"Model converged at epoch {epoch+1} with a loss of {abs(residual_loss - cost_func):.4f}\")\n","            break\n","\n","        residual_loss = cost_func # Update residual loss for next epoch check\n","\n","    return theta_weight, loss_over_epochs"],"metadata":{"id":"ceELYcELmiH2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m = 100  # Number of Examples\n","degree = 3  # Cubic Function\n","\n","# Generate random X values\n","X_original = np.random.randn(m, 1)\n","\n","# Create polynomial features\n","X_poly = np.zeros((m, degree + 1))\n","for i in range(degree + 1):\n","    X_poly[:, i] = X_original[:, 0] ** i\n","\n","# Cubic function with noise\n","y = 3 + 5 * X_original + 2 * X_original**2 - X_original**3 + np.random.randn(m, 1)\n","\n","# Initialize parameters based X shape\n","theta_weight = np.random.randn(degree + 2, 1)\n","\n","optimized_theta_weight, losses = gradient_descent(X, y, theta_weight, alpha=1e-3, epochs=200, tol=1e-7)\n","\n","print(f\"Optimized Weight (θ): {optimized_theta_weight[-1]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XNvojMXRpfLG","executionInfo":{"status":"ok","timestamp":1743567872916,"user_tz":420,"elapsed":2,"user":{"displayName":"Shaan Vats","userId":"07283510042735688811"}},"outputId":"73616f0c-3542-4dfa-d4ff-513649abaa5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epochs: 1/200, Loss: 14.6236\n","Epochs: 2/200, Loss: 14.4605\n","Epochs: 3/200, Loss: 14.3012\n","Epochs: 4/200, Loss: 14.1458\n","Epochs: 5/200, Loss: 13.9941\n","Epochs: 6/200, Loss: 13.8461\n","Epochs: 7/200, Loss: 13.7015\n","Epochs: 8/200, Loss: 13.5605\n","Epochs: 9/200, Loss: 13.4227\n","Epochs: 10/200, Loss: 13.2883\n","Epochs: 11/200, Loss: 13.1570\n","Epochs: 12/200, Loss: 13.0289\n","Epochs: 13/200, Loss: 12.9038\n","Epochs: 14/200, Loss: 12.7816\n","Epochs: 15/200, Loss: 12.6624\n","Epochs: 16/200, Loss: 12.5459\n","Epochs: 17/200, Loss: 12.4322\n","Epochs: 18/200, Loss: 12.3211\n","Epochs: 19/200, Loss: 12.2126\n","Epochs: 20/200, Loss: 12.1067\n","Epochs: 21/200, Loss: 12.0032\n","Epochs: 22/200, Loss: 11.9022\n","Epochs: 23/200, Loss: 11.8035\n","Epochs: 24/200, Loss: 11.7071\n","Epochs: 25/200, Loss: 11.6129\n","Epochs: 26/200, Loss: 11.5209\n","Epochs: 27/200, Loss: 11.4310\n","Epochs: 28/200, Loss: 11.3432\n","Epochs: 29/200, Loss: 11.2574\n","Epochs: 30/200, Loss: 11.1735\n","Epochs: 31/200, Loss: 11.0916\n","Epochs: 32/200, Loss: 11.0115\n","Epochs: 33/200, Loss: 10.9333\n","Epochs: 34/200, Loss: 10.8568\n","Epochs: 35/200, Loss: 10.7821\n","Epochs: 36/200, Loss: 10.7091\n","Epochs: 37/200, Loss: 10.6377\n","Epochs: 38/200, Loss: 10.5679\n","Epochs: 39/200, Loss: 10.4996\n","Epochs: 40/200, Loss: 10.4329\n","Epochs: 41/200, Loss: 10.3677\n","Epochs: 42/200, Loss: 10.3039\n","Epochs: 43/200, Loss: 10.2416\n","Epochs: 44/200, Loss: 10.1806\n","Epochs: 45/200, Loss: 10.1210\n","Epochs: 46/200, Loss: 10.0627\n","Epochs: 47/200, Loss: 10.0056\n","Epochs: 48/200, Loss: 9.9498\n","Epochs: 49/200, Loss: 9.8953\n","Epochs: 50/200, Loss: 9.8419\n","Epochs: 51/200, Loss: 9.7897\n","Epochs: 52/200, Loss: 9.7386\n","Epochs: 53/200, Loss: 9.6886\n","Epochs: 54/200, Loss: 9.6396\n","Epochs: 55/200, Loss: 9.5918\n","Epochs: 56/200, Loss: 9.5449\n","Epochs: 57/200, Loss: 9.4991\n","Epochs: 58/200, Loss: 9.4542\n","Epochs: 59/200, Loss: 9.4102\n","Epochs: 60/200, Loss: 9.3672\n","Epochs: 61/200, Loss: 9.3251\n","Epochs: 62/200, Loss: 9.2839\n","Epochs: 63/200, Loss: 9.2435\n","Epochs: 64/200, Loss: 9.2040\n","Epochs: 65/200, Loss: 9.1653\n","Epochs: 66/200, Loss: 9.1274\n","Epochs: 67/200, Loss: 9.0903\n","Epochs: 68/200, Loss: 9.0539\n","Epochs: 69/200, Loss: 9.0183\n","Epochs: 70/200, Loss: 8.9834\n","Epochs: 71/200, Loss: 8.9492\n","Epochs: 72/200, Loss: 8.9157\n","Epochs: 73/200, Loss: 8.8828\n","Epochs: 74/200, Loss: 8.8506\n","Epochs: 75/200, Loss: 8.8191\n","Epochs: 76/200, Loss: 8.7882\n","Epochs: 77/200, Loss: 8.7579\n","Epochs: 78/200, Loss: 8.7282\n","Epochs: 79/200, Loss: 8.6991\n","Epochs: 80/200, Loss: 8.6705\n","Epochs: 81/200, Loss: 8.6425\n","Epochs: 82/200, Loss: 8.6150\n","Epochs: 83/200, Loss: 8.5881\n","Epochs: 84/200, Loss: 8.5617\n","Epochs: 85/200, Loss: 8.5358\n","Epochs: 86/200, Loss: 8.5104\n","Epochs: 87/200, Loss: 8.4854\n","Epochs: 88/200, Loss: 8.4610\n","Epochs: 89/200, Loss: 8.4370\n","Epochs: 90/200, Loss: 8.4134\n","Epochs: 91/200, Loss: 8.3903\n","Epochs: 92/200, Loss: 8.3676\n","Epochs: 93/200, Loss: 8.3453\n","Epochs: 94/200, Loss: 8.3234\n","Epochs: 95/200, Loss: 8.3019\n","Epochs: 96/200, Loss: 8.2808\n","Epochs: 97/200, Loss: 8.2601\n","Epochs: 98/200, Loss: 8.2398\n","Epochs: 99/200, Loss: 8.2198\n","Epochs: 100/200, Loss: 8.2002\n","Epochs: 101/200, Loss: 8.1809\n","Epochs: 102/200, Loss: 8.1620\n","Epochs: 103/200, Loss: 8.1434\n","Epochs: 104/200, Loss: 8.1251\n","Epochs: 105/200, Loss: 8.1071\n","Epochs: 106/200, Loss: 8.0895\n","Epochs: 107/200, Loss: 8.0721\n","Epochs: 108/200, Loss: 8.0551\n","Epochs: 109/200, Loss: 8.0383\n","Epochs: 110/200, Loss: 8.0218\n","Epochs: 111/200, Loss: 8.0056\n","Epochs: 112/200, Loss: 7.9896\n","Epochs: 113/200, Loss: 7.9739\n","Epochs: 114/200, Loss: 7.9585\n","Epochs: 115/200, Loss: 7.9433\n","Epochs: 116/200, Loss: 7.9284\n","Epochs: 117/200, Loss: 7.9137\n","Epochs: 118/200, Loss: 7.8992\n","Epochs: 119/200, Loss: 7.8850\n","Epochs: 120/200, Loss: 7.8710\n","Epochs: 121/200, Loss: 7.8572\n","Epochs: 122/200, Loss: 7.8437\n","Epochs: 123/200, Loss: 7.8303\n","Epochs: 124/200, Loss: 7.8172\n","Epochs: 125/200, Loss: 7.8042\n","Epochs: 126/200, Loss: 7.7915\n","Epochs: 127/200, Loss: 7.7789\n","Epochs: 128/200, Loss: 7.7665\n","Epochs: 129/200, Loss: 7.7544\n","Epochs: 130/200, Loss: 7.7424\n","Epochs: 131/200, Loss: 7.7305\n","Epochs: 132/200, Loss: 7.7189\n","Epochs: 133/200, Loss: 7.7074\n","Epochs: 134/200, Loss: 7.6961\n","Epochs: 135/200, Loss: 7.6850\n","Epochs: 136/200, Loss: 7.6740\n","Epochs: 137/200, Loss: 7.6631\n","Epochs: 138/200, Loss: 7.6525\n","Epochs: 139/200, Loss: 7.6419\n","Epochs: 140/200, Loss: 7.6315\n","Epochs: 141/200, Loss: 7.6213\n","Epochs: 142/200, Loss: 7.6112\n","Epochs: 143/200, Loss: 7.6012\n","Epochs: 144/200, Loss: 7.5914\n","Epochs: 145/200, Loss: 7.5817\n","Epochs: 146/200, Loss: 7.5722\n","Epochs: 147/200, Loss: 7.5627\n","Epochs: 148/200, Loss: 7.5534\n","Epochs: 149/200, Loss: 7.5442\n","Epochs: 150/200, Loss: 7.5352\n","Epochs: 151/200, Loss: 7.5262\n","Epochs: 152/200, Loss: 7.5174\n","Epochs: 153/200, Loss: 7.5086\n","Epochs: 154/200, Loss: 7.5000\n","Epochs: 155/200, Loss: 7.4915\n","Epochs: 156/200, Loss: 7.4831\n","Epochs: 157/200, Loss: 7.4748\n","Epochs: 158/200, Loss: 7.4667\n","Epochs: 159/200, Loss: 7.4586\n","Epochs: 160/200, Loss: 7.4506\n","Epochs: 161/200, Loss: 7.4427\n","Epochs: 162/200, Loss: 7.4349\n","Epochs: 163/200, Loss: 7.4272\n","Epochs: 164/200, Loss: 7.4196\n","Epochs: 165/200, Loss: 7.4121\n","Epochs: 166/200, Loss: 7.4046\n","Epochs: 167/200, Loss: 7.3973\n","Epochs: 168/200, Loss: 7.3900\n","Epochs: 169/200, Loss: 7.3828\n","Epochs: 170/200, Loss: 7.3757\n","Epochs: 171/200, Loss: 7.3687\n","Epochs: 172/200, Loss: 7.3618\n","Epochs: 173/200, Loss: 7.3549\n","Epochs: 174/200, Loss: 7.3481\n","Epochs: 175/200, Loss: 7.3414\n","Epochs: 176/200, Loss: 7.3348\n","Epochs: 177/200, Loss: 7.3282\n","Epochs: 178/200, Loss: 7.3217\n","Epochs: 179/200, Loss: 7.3153\n","Epochs: 180/200, Loss: 7.3090\n","Epochs: 181/200, Loss: 7.3027\n","Epochs: 182/200, Loss: 7.2965\n","Epochs: 183/200, Loss: 7.2903\n","Epochs: 184/200, Loss: 7.2842\n","Epochs: 185/200, Loss: 7.2782\n","Epochs: 186/200, Loss: 7.2722\n","Epochs: 187/200, Loss: 7.2663\n","Epochs: 188/200, Loss: 7.2605\n","Epochs: 189/200, Loss: 7.2547\n","Epochs: 190/200, Loss: 7.2490\n","Epochs: 191/200, Loss: 7.2433\n","Epochs: 192/200, Loss: 7.2377\n","Epochs: 193/200, Loss: 7.2322\n","Epochs: 194/200, Loss: 7.2267\n","Epochs: 195/200, Loss: 7.2212\n","Epochs: 196/200, Loss: 7.2158\n","Epochs: 197/200, Loss: 7.2105\n","Epochs: 198/200, Loss: 7.2052\n","Epochs: 199/200, Loss: 7.2000\n","Epochs: 200/200, Loss: 7.1948\n","Optimized Weight (θ): [0.35370643]\n"]}]}]}