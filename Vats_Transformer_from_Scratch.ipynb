{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPjslYpvJ0oYe7Ux67v828k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S-VATS31/Deep_Learning_Models/blob/main/Vats_Transformer_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "gJZu9bVNTsQA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "KjSYzUsnYJcR"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level = logging.DEBUG, # Detailed info on bugs\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler('mha_debug.log') # Save to file\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create logger object\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sinusoidal Positional Encodings**"
      ],
      "metadata": {
        "id": "w-qGzQyTxIEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(torch.nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        \"\"\"Initialize sinusoidal positional encoding layer.\n",
        "\n",
        "        Generates fixed positional encodings using sine and cosine functions to provide\n",
        "        position information for sequences in transformer models. Encodings are computed\n",
        "        for a fixed sequence length and stored as a non-learnable buffer meaning it is\n",
        "        not passed through backpropagation.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): Dimensionality of the model's input/output representations.\n",
        "                Must be even for sine/cosine splitting.\n",
        "\n",
        "        Attributes:\n",
        "            PE (torch.Tensor): Positional encoding tensor, registered as a buffer (non-learnable).\n",
        "        \"\"\"\n",
        "        self.dropout = torch.nn.Dropout(p=dropout).to(device)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Apply sinusoidal positional encodings to the input tensor.\n",
        "\n",
        "        Adds precomputed positional encodings to the input tensor to incorporate\n",
        "        positional information.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            x (torch.Tensor): Input tensor with positional encodings added, using element-wise addition\n",
        "            as well as dropout to avoid overfitting.\n",
        "        \"\"\"\n",
        "        # Ensure x is on same device\n",
        "        x = x.to(device)\n",
        "\n",
        "        # Dynamically calculate sequence length\n",
        "        T = x.size(1)\n",
        "\n",
        "        # Create position indices for the current sequence length\n",
        "        position = torch.arange(0, T).unsqueeze(1).float().to(device) # [T, 1]\n",
        "\n",
        "        # Compute the denominator for the sinusoidal encoding\n",
        "        divisor = torch.exp(torch.arange(0, self.d_model, 2).float().to(device) * -(math.log(10000.0) / self.d_model)) # [d_model//2]\n",
        "\n",
        "        # Create Sine and Cosine encodings\n",
        "        PE = torch.zeros(T, self.d_model).to(device) # [T, d_model]\n",
        "\n",
        "        # Fill tensor with Sine and Cosine\n",
        "        PE[:, 0::2] = torch.sin(position * divisor) # Even indices --> Sine (2i)\n",
        "        PE[:, 1::2] = torch.cos(position * divisor) # Odd indices --> Cosine (2i+1)\n",
        "\n",
        "        # Add positional encodings to the input tensor and apply dropout\n",
        "        x = x + PE[:T, :] # Add positional encoding for the current sequence length\n",
        "        x = self.dropout(x) # Apply dropout to prevent overfitting\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ZM-jvx54xIcc"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Layer Normalization**"
      ],
      "metadata": {
        "id": "jwu7BDwdlsvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(torch.nn.Module):\n",
        "    def __init__(self, normalized_shape, eps=1e-6, dtype=torch.float32):\n",
        "        \"\"\"Initialize Layer Normalization module.\n",
        "\n",
        "        Layer Normalization normalizes the input tensor over the last dimension, stabilizing training by\n",
        "        reducing internal covariate shift. It applies learnable scaling (gamma) and shifting (beta) parameters\n",
        "        to the normalized tensor.\n",
        "\n",
        "        Args:\n",
        "            normalized_shape (torch.Tensor): Shape of the input tensor's normalized dimension(s).\n",
        "                For example, if the input is (batch, seq_len, features), this would be `features` or\n",
        "                a tuple of the last dimensions.\n",
        "            eps (float, optional): Small constant added to the variance to prevent division by zero.\n",
        "                Defaults to 1e-6.\n",
        "            dtype (torch.dtype, optional): Data type for the learnable parameters. Defaults to torch.float32.\n",
        "\n",
        "        Attributes:\n",
        "            gamma (torch.nn.Parameter): Learnable scaling factor, initialized to ones.\n",
        "            beta (torch.nn.Parameter): Learnable shift factor, initialized to zeros.\n",
        "            eps (float): Small constant for numerical stability.\n",
        "        \"\"\"\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.gamma = torch.nn.Parameter(torch.ones(normalized_shape, dtype=dtype).to(device)) # Scaling factor\n",
        "        self.beta = torch.nn.Parameter(torch.zeros(normalized_shape, dtype=dtype).to(device)) # Shifiting factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform Layer Normalization on the input tensor.\n",
        "\n",
        "        Normalizes the input tensor over the last dimension by subtracting the mean and dividing by the\n",
        "        standard deviation, then applies learnable scaling (gamma) and shifting (beta).\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (..., normalized_shape).\n",
        "\n",
        "        Returns:\n",
        "            x (torch.Tensor): Normalized and transformed tensor of the same shape as the input,\n",
        "                computed as `gamma * normalized_x + beta`.\n",
        "        \"\"\"\n",
        "        x = x.to(device) # Ensure x is on same device\n",
        "        mean = x.mean(dim=-1, keepdim=True) # Compute mean over the last dimension\n",
        "        var = x.var(dim=-1, unbiased=False, keepdim=True) # Compute variance over the last dimension\n",
        "        normalized_x = (x - mean) / torch.sqrt(var + self.eps) # Normalize the input\n",
        "        x = self.gamma * normalized_x + self.beta # Apply scaling and shifting\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "xNUdPNJ_lsll"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi Headed Attention**"
      ],
      "metadata": {
        "id": "te0u5pi0Tydp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        \"\"\"Initialize Multi-Head Attention module.\n",
        "\n",
        "        Implements multi-head self-attention mechanism for transformer models, allowing the model\n",
        "        to focus on different parts of the input sequence simultaneously. Includes positional\n",
        "        encoding, layer normalization, and dropout for regularization.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): Dimensionality of the model's input/output representations.\n",
        "                Must be divisible by num_heads.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            dropout (float, optional): Dropout probability for attention weights and output.\n",
        "                Defaults to 0.1.\n",
        "\n",
        "        Attributes:\n",
        "            d_k (int): Dimensionality of each attention head (d_model // num_heads).\n",
        "            W_Q (torch.nn.Linear): Linear projection for queries.\n",
        "            W_K (torch.nn.Linear): Linear projection for keys.\n",
        "            W_V (torch.nn.Linear): Linear projection for values.\n",
        "            W_O (torch.nn.Linear): Linear projection for output.\n",
        "            positional_encoding (PositionalEncoding): Positional encoding layer.\n",
        "            dropout (torch.nn.Dropout): Dropout layer for regularization.\n",
        "            layer_norm (LayerNorm): Layer normalization module.\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Weight Matrices\n",
        "        self.W_Q = torch.nn.Linear(d_model, d_model).to(device)\n",
        "        self.W_K = torch.nn.Linear(d_model, d_model).to(device)\n",
        "        self.W_V = torch.nn.Linear(d_model, d_model).to(device)\n",
        "        self.W_O = torch.nn.Linear(d_model, d_model).to(device)\n",
        "\n",
        "        self.positional_encoding = PositionalEncoding(d_model).to(device) # Give tokens respective positions\n",
        "        self.dropout = torch.nn.Dropout(p=dropout).to(device) # Dropout to prevent overfitting\n",
        "        self.layer_norm = LayerNorm(d_model).to(device) # Normalize and transform input tensor x\n",
        "\n",
        "    def forward(self, x, padding=None, causal=True):\n",
        "        \"\"\"Apply multi-head attention to the input tensor.\n",
        "\n",
        "        Computes scaled dot-product attention for multiple heads, incorporating positional\n",
        "        encodings, layer normalization, and optional padding masks. Returns the attention\n",
        "        output and attention weights. Applies autoregressive masking to prevent the transformer\n",
        "        from looking at future tokens.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
        "            padding (torch.Tensor, optional): Padding mask of shape (batch_size, seq_len).\n",
        "                Zeros indicate padded positions; ones indicate valid positions.\n",
        "                Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - torch.Tensor: Attention output of shape (batch_size, seq_len, d_model).\n",
        "                - torch.Tensor: Attention weights of shape (batch_size, num_heads, seq_len, seq_len).\n",
        "        \"\"\"\n",
        "        x = x.to(device) # Ensure x is on same device\n",
        "        residual = x # Residual connection\n",
        "        x = self.layer_norm(x) # Apply LayerNorm\n",
        "\n",
        "        # Dynamically calculate batch size and sequence length\n",
        "        B, T, _ = x.size()\n",
        "\n",
        "        # Apply Positional Encoding\n",
        "        x = self.positional_encoding(x) # Apply Positional encoding\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.W_Q(x) # Query: [B, T, d_model]\n",
        "        K = self.W_K(x) # Key:   [B, T, d_model]\n",
        "        V = self.W_V(x) # Value: [B, T, d_model]\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(B, T, self.num_heads, self.d_k).transpose(1, 2) # [B, num_heads, T, d_k]\n",
        "        K = K.view(B, T, self.num_heads, self.d_k).transpose(1, 2) # [B, num_heads, T, d_k]\n",
        "        V = V.view(B, T, self.num_heads, self.d_k).transpose(1, 2) # [B, num_heads, T, d_k]\n",
        "\n",
        "        # Log after reshaping\n",
        "        logger.debug(f\"Q shape: {Q.shape}, K shape: {K.shape}, V shape: {V.shape}\")\n",
        "\n",
        "        # Scaled Dot Product Attention\n",
        "        assert Q.shape[-1] == K.shape[-1], f\"Expected d_k {Q.shape[-1]}, but got {K.shape[-1]}\" # Ensure matrix multiplication is compatible\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # [B, num_heads, T, T]\n",
        "\n",
        "        # Log attention score\n",
        "        logger.debug(f\"attention_scores shape: {attention_scores.shape}\") # Shape\n",
        "        logger.debug(f\"attention_scores minimum: {attention_scores.min().item():.4f}, maximum: {attention_scores.max().item():.4f}, mean: {attention_scores.mean().item():.4f}\") # Statistics\n",
        "        if torch.isnan(attention_scores).any():\n",
        "            logger.warning(\"NaN found in attention_scores\")\n",
        "\n",
        "        # Autoregressive Masking\n",
        "        if causal:\n",
        "            autoregressive_mask = torch.tril(torch.ones(T, T)).to(device) # [T, T]\n",
        "            autoregressive_mask = autoregressive_mask.unsqueeze(0).unsqueeze(0) # [1, 1, T, T]\n",
        "            autoregressive_mask = autoregressive_mask.expand(B, self.num_heads, T, T) # [B, num_heads, T, T]\n",
        "\n",
        "            # Apply autoregressive mask\n",
        "            attention_scores = attention_scores.masked_fill(autoregressive_mask == 0, float('-inf'))\n",
        "\n",
        "            # Log autoregressive mask effect\n",
        "            logger.debug(f\"Autoregressive mask applied. Non-masked positions: {autoregressive_mask.sum().item()}\")\n",
        "\n",
        "        # Apply padding mask if given\n",
        "        if padding is not None:\n",
        "            padding = padding.to(device).unsqueeze(1).unsqueeze(2) # [B, 1, 1, T]\n",
        "            attention_scores = attention_scores.masked_fill(padding == 0, float('-inf')) # Mask padded positions\n",
        "\n",
        "            # Log padding mask effect\n",
        "            logger.debug(f\"Padding mask applied. Applied positions: {padding.sum().item()}\")\n",
        "\n",
        "        # Probability Distribution\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1) # [B, num_heads, T, T]\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Log attention weights\n",
        "        logger.debug(f\"attention_weights shape: {attention_weights.shape}\") # Shape\n",
        "        logger.debug(f\"attention_weights minimum: {attention_weights.min().item():.4f}, maximum: {attention_weights.max().item():.4f}, sum (â‰ˆ1): {attention_weights.sum().item():.4f}\") # Statistics\n",
        "\n",
        "        # Ensure matrix multiplication is compatible\n",
        "        assert attention_weights.shape[-1] == V.shape[-2], f\"Expected T {attention_weights.shape[-1]}, but got {V.shape[-2]}\"\n",
        "        attention_output = torch.matmul(attention_weights, V) # [B, num_heads, T, d_k]\n",
        "\n",
        "        # Concatenate the attention heads and apply final projection\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(B, T, self.d_model) # [B, T, d_model]\n",
        "        attention_output = self.dropout(attention_output) # Apply Dropout\n",
        "        attention_output = self.W_O(attention_output) # Final output projection\n",
        "\n",
        "        # Apply residual connection\n",
        "        final_output = attention_output + residual\n",
        "        return final_output, attention_weights\n"
      ],
      "metadata": {
        "id": "kXSTEDYsZCPB"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feedforward MLP**"
      ],
      "metadata": {
        "id": "WaAKtgWqkmGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardMLP(torch.nn.Module):\n",
        "    def __init__(self, d_ffn, d_model, dropout=0.1):\n",
        "        \"\"\"Feed-Forward Neural Network (FFN) module for Transformer models.\n",
        "\n",
        "        Implements a two-layer feed-forward neural network with approximate GELU activation, used\n",
        "        within Transformer architectures to process each token independently. Includes pre-layer\n",
        "        normalization, dropout for regularization, and a residual connection to stabilize training.\n",
        "\n",
        "        Args:\n",
        "            d_ffn (int): Dimensionality of the hidden layer in the feed-forward network.\n",
        "            d_model (int): Dimensionality of the model's input/output representations.\n",
        "            dropout (float, optional): Dropout probability for the output of the second linear layer.\n",
        "                Defaults to 0.1.\n",
        "\n",
        "        Attributes:\n",
        "            weight_matrix1 (torch.nn.Parameter): Weight matrix for the first linear layer, shape (d_model, d_ffn).\n",
        "            bias1 (torch.nn.Parameter): Bias for the first linear layer, shape (d_ffn).\n",
        "            weight_matrix2 (torch.nn.Parameter): Weight matrix for the second linear layer, shape (d_ffn, d_model).\n",
        "            bias2 (torch.nn.Parameter): Bias for the second linear layer, shape (d_model).\n",
        "            dropout (torch.nn.Dropout): Dropout layer for regularization.\n",
        "            layer_norm (LayerNorm): Layer normalization module.\n",
        "        \"\"\"\n",
        "        super(FeedForwardMLP, self).__init__()\n",
        "\n",
        "        # Linear layer 1\n",
        "        self.weight_matrix1 = torch.nn.Parameter(torch.randn(d_model, d_ffn).to(device) * math.sqrt(2.0 / d_model)) # [d_model, d_ffn]\n",
        "        self.bias1 = torch.nn.Parameter(torch.zeros(d_ffn).to(device)) # [d_ffn]\n",
        "\n",
        "        # Linear layer 2\n",
        "        self.weight_matrix2 = torch.nn.Parameter(torch.randn(d_ffn, d_model).to(device) * math.sqrt(2.0 / d_ffn)) # [d_ffn, d_model]\n",
        "        self.bias2 = torch.nn.Parameter(torch.zeros(d_model).to(device)) # [d_model]\n",
        "\n",
        "        # Dropout & LayerNorm\n",
        "        self.dropout = torch.nn.Dropout(dropout).to(device)\n",
        "        self.layer_norm = LayerNorm(d_model).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Apply the feed-forward neural network to the input tensor.\n",
        "\n",
        "        Processes the input through two linear layers with GELU activation between them,\n",
        "        incorporating pre-layer normalization, dropout, and a residual connection.\n",
        "        The module transforms each token independently across the sequence.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, seq_len, d_model).\n",
        "        \"\"\"\n",
        "        x = x.to(device) # Ensure x is on same device\n",
        "\n",
        "        # Dynamically calculate the batch size and sequence length\n",
        "        B, T, _ = x.shape\n",
        "\n",
        "        # Residual connection\n",
        "        residual = x\n",
        "\n",
        "        # PreNorm\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # First linear transformation\n",
        "        x = F.gelu(torch.matmul(x, self.weight_matrix1) + self.bias1)\n",
        "\n",
        "        # Second linear transformation\n",
        "        x = torch.matmul(x, self.weight_matrix2) + self.bias2\n",
        "\n",
        "        # Apply Dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply residual connection\n",
        "        x = residual + x\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "nu3wqlwwkqDq"
      },
      "execution_count": 71,
      "outputs": []
    }
  ]
}