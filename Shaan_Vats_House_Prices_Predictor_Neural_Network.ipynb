{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNcz8ssr2JyTEVF0O4S+wJk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S-VATS31/Deep-Learning-Models-Mini-Models/blob/main/Shaan_Vats_House_Prices_Predictor_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Necessary Libraries & Set up CUDA/CPU**"
      ],
      "metadata": {
        "id": "vaM8CdHho4lD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax2PtANRRmbE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create a Neural Network Class and Initialize Parameters**"
      ],
      "metadata": {
        "id": "iC6U01BcpADb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HousePricesNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layer1, hidden_layer2, hidden_layer3):\n",
        "        super(HousePricesNN, self).__init__()\n",
        "\n",
        "        # First Layer (Input Layer)\n",
        "        self.layer1 = nn.Linear(input_size, hidden_layer1)\n",
        "        self.relu1 = nn.LeakyReLU(0.1)\n",
        "        self.dropout1 = nn.Dropout(p=0.5)\n",
        "\n",
        "        # Second Layer (1st Hidden Layer)\n",
        "        self.layer2 = nn.Linear(hidden_layer1, hidden_layer2)\n",
        "        self.relu2 = nn.LeakyReLU(0.1)\n",
        "        self.dropout2 = nn.Dropout(p=0.4)\n",
        "\n",
        "        # Third Layer (2nd Hidden Layer)\n",
        "        self.layer3 = nn.Linear(hidden_layer2, hidden_layer3)\n",
        "        self.relu3 = nn.LeakyReLU(0.1)\n",
        "        self.dropout3 = nn.Dropout(p=0.3)\n",
        "\n",
        "        # Fourth Layer (Output Layer)\n",
        "        self.layer4 = nn.Linear(hidden_layer3, 1)\n",
        "\n",
        "        # Initialize the weight parameter\n",
        "        self.apply(self.init_weights_kaiming)\n",
        "\n",
        "    def init_weights_kaiming(self, layer):\n",
        "    # Checks if the layer is Linear (dense)\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            init.kaiming_uniform_(layer.weight, nonlinearity='leaky_relu')\n",
        "\n",
        "        # Checks if the layer has a bias term\n",
        "            if hasattr(layer, 'bias') and layer.bias is not None:\n",
        "                nn.init.zeros_(layer.bias)  # Initialize bias at 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.layer1(x)) # Input Layer -> 1st Hidden Layer\n",
        "        x= self.dropout1(x)\n",
        "        x = self.relu2(self.layer2(x)) # 1st Hidden Layer -> 2nd Hidden Layer\n",
        "        x= self.dropout2(x)\n",
        "        x = self.relu3(self.layer3(x)) # 2nd Hidden Layer -> 3rd Hidden Layer\n",
        "        x= self.dropout3(x)\n",
        "        x = self.layer4(x) # 3rd Hidden Layer -> Output Layer\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "YjcByFPsSBTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Class**"
      ],
      "metadata": {
        "id": "R7yU3q2WYxeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CaliforniaHousingDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    # Return the number of datapoints\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    # Return individual instances\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]"
      ],
      "metadata": {
        "id": "V6IWBLLCYyUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "EGQv6NyOpRFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch data\n",
        "california_housing = fetch_california_housing()\n",
        "\n",
        "# Raw data -> DataFrame\n",
        "df = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)\n",
        "\n",
        "# Add target variable\n",
        "df['Price'] = california_housing.target\n",
        "\n",
        "# Define X and y\n",
        "X = df.drop('Price', axis=1).to_numpy()\n",
        "y = df['Price'].to_numpy()\n",
        "\n",
        "# Scale X and y\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "X_normalized = scaler_X.fit_transform(X)\n",
        "y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Split into training/testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_normalized, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create instances of training and testing dataset\n",
        "train_dataset = CaliforniaHousingDataset(X_train, y_train)\n",
        "test_dataset = CaliforniaHousingDataset(X_test, y_test)\n",
        "\n",
        "# Create instances of training and test dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=96, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=96, shuffle=True)\n"
      ],
      "metadata": {
        "id": "C_-qprwAc_5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating Instances**"
      ],
      "metadata": {
        "id": "KamwvblJfsXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create an instance of the model\n",
        "house_prices_nn = HousePricesNN(input_size=8, hidden_layer1=64, hidden_layer2=32, hidden_layer3=16)\n",
        "\n",
        "# Set up the criterion\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Set up optimization algorithm\n",
        "optimizer = optim.Adam(house_prices_nn.parameters(), lr=0.0011)\n"
      ],
      "metadata": {
        "id": "6xu8YKR4frZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Training & Evaluation Process**"
      ],
      "metadata": {
        "id": "Qghj-10hul3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train and evaluate the model\n",
        "def train_eval_neural_net(house_prices_nn, criterion, optimizer, train_loader, test_loader, epochs):\n",
        "\n",
        "    # Make sure house_prices_nn is on the same device\n",
        "    house_prices_nn.to(device)\n",
        "\n",
        "    # Set the model to training mode\n",
        "    house_prices_nn.train()\n",
        "\n",
        "    # Empty lists to store average losses for plotting\n",
        "    train_loss_list = []\n",
        "    test_loss_list = []\n",
        "\n",
        "    # Global loop\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Initialize total train loss\n",
        "        total_train_loss = 0.0\n",
        "\n",
        "        # Training-Specific Loop\n",
        "        for X_train, y_train in train_loader:\n",
        "            X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "\n",
        "            # Turns y_train into a column vector\n",
        "            y_train = y_train.view(-1, 1)\n",
        "\n",
        "            optimizer.zero_grad() # Zero the gradients\n",
        "            y_pred = house_prices_nn(X_train) # Forward pass\n",
        "            loss = criterion(y_pred, y_train) # Measure loss function\n",
        "            loss.backward() # Perform backpropagation\n",
        "            optimizer.step() # Update weights\n",
        "\n",
        "            total_train_loss += loss.item() # item(): tensor -> scalar\n",
        "\n",
        "        # Calculate, store, & print training loss\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_loss_list.append(avg_train_loss)\n",
        "\n",
        "        # Set the model to evaluation mode\n",
        "        house_prices_nn.eval()\n",
        "\n",
        "        # Initalize total test loss\n",
        "        total_test_loss = 0.0\n",
        "\n",
        "        with torch.no_grad(): # Turn off gradient calulation for evaluation mode\n",
        "\n",
        "            # Testing-Specific Loop\n",
        "            for X_test, y_test in test_loader:\n",
        "                X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "                # Turns y_train into a column vector\n",
        "                y_test = y_test.view(-1, 1)\n",
        "\n",
        "                y_pred = house_prices_nn(X_test) # Forward pass\n",
        "                loss = criterion(y_pred, y_test) # Measure loss function\n",
        "                total_test_loss += loss.item() # item(): tensor -> scalar\n",
        "\n",
        "        # Calculate, store, & print testing loss\n",
        "        avg_test_loss = total_test_loss / len(test_loader)\n",
        "        test_loss_list.append(avg_test_loss)\n",
        "        print(f\"Epoch: {epoch+1}/{epochs} - Training Loss: {avg_train_loss:.4f}, Testing Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "    # Plot the training loss over epochs\n",
        "    plt.plot(range(epochs), train_loss_list, label='Training Loss', color='orange')\n",
        "    plt.plot(range(epochs), test_loss_list, label='Testing Loss', color='blue')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss vs. Testing Loss over Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Call training and evaluation function\n",
        "train_eval_neural_net(house_prices_nn, criterion, optimizer, train_loader, test_loader, epochs=1500)\n"
      ],
      "metadata": {
        "id": "bJObaxdhulX6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "22af086a-cf55-4151-f217-5661409df107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/1500 - Training Loss: 0.1439, Testing Loss: 0.0797\n",
            "Epoch: 2/1500 - Training Loss: 0.0347, Testing Loss: 0.0258\n",
            "Epoch: 3/1500 - Training Loss: 0.0244, Testing Loss: 0.0246\n",
            "Epoch: 4/1500 - Training Loss: 0.0234, Testing Loss: 0.0236\n",
            "Epoch: 5/1500 - Training Loss: 0.0227, Testing Loss: 0.0229\n",
            "Epoch: 6/1500 - Training Loss: 0.0220, Testing Loss: 0.0225\n",
            "Epoch: 7/1500 - Training Loss: 0.0215, Testing Loss: 0.0217\n",
            "Epoch: 8/1500 - Training Loss: 0.0210, Testing Loss: 0.0219\n",
            "Epoch: 9/1500 - Training Loss: 0.0207, Testing Loss: 0.0215\n",
            "Epoch: 10/1500 - Training Loss: 0.0201, Testing Loss: 0.0206\n",
            "Epoch: 11/1500 - Training Loss: 0.0198, Testing Loss: 0.0202\n",
            "Epoch: 12/1500 - Training Loss: 0.0195, Testing Loss: 0.0198\n",
            "Epoch: 13/1500 - Training Loss: 0.0191, Testing Loss: 0.0198\n",
            "Epoch: 14/1500 - Training Loss: 0.0187, Testing Loss: 0.0201\n",
            "Epoch: 15/1500 - Training Loss: 0.0186, Testing Loss: 0.0209\n",
            "Epoch: 16/1500 - Training Loss: 0.0183, Testing Loss: 0.0191\n",
            "Epoch: 17/1500 - Training Loss: 0.0182, Testing Loss: 0.0192\n",
            "Epoch: 18/1500 - Training Loss: 0.0179, Testing Loss: 0.0187\n",
            "Epoch: 19/1500 - Training Loss: 0.0179, Testing Loss: 0.0193\n",
            "Epoch: 20/1500 - Training Loss: 0.0173, Testing Loss: 0.0188\n",
            "Epoch: 21/1500 - Training Loss: 0.0172, Testing Loss: 0.0191\n",
            "Epoch: 22/1500 - Training Loss: 0.0173, Testing Loss: 0.0208\n",
            "Epoch: 23/1500 - Training Loss: 0.0172, Testing Loss: 0.0185\n",
            "Epoch: 24/1500 - Training Loss: 0.0169, Testing Loss: 0.0182\n",
            "Epoch: 25/1500 - Training Loss: 0.0167, Testing Loss: 0.0186\n",
            "Epoch: 26/1500 - Training Loss: 0.0165, Testing Loss: 0.0180\n",
            "Epoch: 27/1500 - Training Loss: 0.0165, Testing Loss: 0.0178\n",
            "Epoch: 28/1500 - Training Loss: 0.0166, Testing Loss: 0.0176\n",
            "Epoch: 29/1500 - Training Loss: 0.0162, Testing Loss: 0.0176\n",
            "Epoch: 30/1500 - Training Loss: 0.0163, Testing Loss: 0.0176\n",
            "Epoch: 31/1500 - Training Loss: 0.0161, Testing Loss: 0.0180\n",
            "Epoch: 32/1500 - Training Loss: 0.0161, Testing Loss: 0.0178\n",
            "Epoch: 33/1500 - Training Loss: 0.0159, Testing Loss: 0.0173\n",
            "Epoch: 34/1500 - Training Loss: 0.0160, Testing Loss: 0.0178\n",
            "Epoch: 35/1500 - Training Loss: 0.0157, Testing Loss: 0.0176\n",
            "Epoch: 36/1500 - Training Loss: 0.0158, Testing Loss: 0.0172\n",
            "Epoch: 37/1500 - Training Loss: 0.0156, Testing Loss: 0.0173\n",
            "Epoch: 38/1500 - Training Loss: 0.0159, Testing Loss: 0.0173\n",
            "Epoch: 39/1500 - Training Loss: 0.0156, Testing Loss: 0.0173\n",
            "Epoch: 40/1500 - Training Loss: 0.0153, Testing Loss: 0.0170\n",
            "Epoch: 41/1500 - Training Loss: 0.0152, Testing Loss: 0.0171\n",
            "Epoch: 42/1500 - Training Loss: 0.0153, Testing Loss: 0.0175\n",
            "Epoch: 43/1500 - Training Loss: 0.0155, Testing Loss: 0.0181\n",
            "Epoch: 44/1500 - Training Loss: 0.0150, Testing Loss: 0.0184\n",
            "Epoch: 45/1500 - Training Loss: 0.0150, Testing Loss: 0.0169\n",
            "Epoch: 46/1500 - Training Loss: 0.0149, Testing Loss: 0.0167\n",
            "Epoch: 47/1500 - Training Loss: 0.0150, Testing Loss: 0.0174\n",
            "Epoch: 48/1500 - Training Loss: 0.0148, Testing Loss: 0.0165\n",
            "Epoch: 49/1500 - Training Loss: 0.0149, Testing Loss: 0.0169\n",
            "Epoch: 50/1500 - Training Loss: 0.0151, Testing Loss: 0.0167\n",
            "Epoch: 51/1500 - Training Loss: 0.0149, Testing Loss: 0.0172\n",
            "Epoch: 52/1500 - Training Loss: 0.0146, Testing Loss: 0.0163\n",
            "Epoch: 53/1500 - Training Loss: 0.0145, Testing Loss: 0.0162\n",
            "Epoch: 54/1500 - Training Loss: 0.0145, Testing Loss: 0.0164\n",
            "Epoch: 55/1500 - Training Loss: 0.0146, Testing Loss: 0.0162\n",
            "Epoch: 56/1500 - Training Loss: 0.0146, Testing Loss: 0.0167\n",
            "Epoch: 57/1500 - Training Loss: 0.0145, Testing Loss: 0.0167\n",
            "Epoch: 58/1500 - Training Loss: 0.0144, Testing Loss: 0.0161\n",
            "Epoch: 59/1500 - Training Loss: 0.0144, Testing Loss: 0.0160\n",
            "Epoch: 60/1500 - Training Loss: 0.0142, Testing Loss: 0.0161\n",
            "Epoch: 61/1500 - Training Loss: 0.0140, Testing Loss: 0.0160\n",
            "Epoch: 62/1500 - Training Loss: 0.0140, Testing Loss: 0.0162\n",
            "Epoch: 63/1500 - Training Loss: 0.0142, Testing Loss: 0.0165\n",
            "Epoch: 64/1500 - Training Loss: 0.0144, Testing Loss: 0.0166\n",
            "Epoch: 65/1500 - Training Loss: 0.0141, Testing Loss: 0.0159\n",
            "Epoch: 66/1500 - Training Loss: 0.0145, Testing Loss: 0.0156\n",
            "Epoch: 67/1500 - Training Loss: 0.0139, Testing Loss: 0.0157\n",
            "Epoch: 68/1500 - Training Loss: 0.0137, Testing Loss: 0.0160\n",
            "Epoch: 69/1500 - Training Loss: 0.0138, Testing Loss: 0.0164\n",
            "Epoch: 70/1500 - Training Loss: 0.0140, Testing Loss: 0.0173\n",
            "Epoch: 71/1500 - Training Loss: 0.0138, Testing Loss: 0.0157\n",
            "Epoch: 72/1500 - Training Loss: 0.0137, Testing Loss: 0.0157\n",
            "Epoch: 73/1500 - Training Loss: 0.0136, Testing Loss: 0.0165\n",
            "Epoch: 74/1500 - Training Loss: 0.0137, Testing Loss: 0.0156\n",
            "Epoch: 75/1500 - Training Loss: 0.0139, Testing Loss: 0.0152\n",
            "Epoch: 76/1500 - Training Loss: 0.0136, Testing Loss: 0.0154\n",
            "Epoch: 77/1500 - Training Loss: 0.0137, Testing Loss: 0.0164\n",
            "Epoch: 78/1500 - Training Loss: 0.0134, Testing Loss: 0.0153\n",
            "Epoch: 79/1500 - Training Loss: 0.0136, Testing Loss: 0.0150\n",
            "Epoch: 80/1500 - Training Loss: 0.0138, Testing Loss: 0.0162\n",
            "Epoch: 81/1500 - Training Loss: 0.0136, Testing Loss: 0.0157\n",
            "Epoch: 82/1500 - Training Loss: 0.0134, Testing Loss: 0.0154\n",
            "Epoch: 83/1500 - Training Loss: 0.0134, Testing Loss: 0.0159\n",
            "Epoch: 84/1500 - Training Loss: 0.0137, Testing Loss: 0.0150\n",
            "Epoch: 85/1500 - Training Loss: 0.0134, Testing Loss: 0.0150\n",
            "Epoch: 86/1500 - Training Loss: 0.0135, Testing Loss: 0.0155\n",
            "Epoch: 87/1500 - Training Loss: 0.0132, Testing Loss: 0.0149\n",
            "Epoch: 88/1500 - Training Loss: 0.0135, Testing Loss: 0.0150\n",
            "Epoch: 89/1500 - Training Loss: 0.0130, Testing Loss: 0.0149\n",
            "Epoch: 90/1500 - Training Loss: 0.0130, Testing Loss: 0.0159\n",
            "Epoch: 91/1500 - Training Loss: 0.0130, Testing Loss: 0.0162\n",
            "Epoch: 92/1500 - Training Loss: 0.0130, Testing Loss: 0.0144\n",
            "Epoch: 93/1500 - Training Loss: 0.0132, Testing Loss: 0.0167\n",
            "Epoch: 94/1500 - Training Loss: 0.0135, Testing Loss: 0.0158\n",
            "Epoch: 95/1500 - Training Loss: 0.0131, Testing Loss: 0.0143\n",
            "Epoch: 96/1500 - Training Loss: 0.0127, Testing Loss: 0.0149\n",
            "Epoch: 97/1500 - Training Loss: 0.0128, Testing Loss: 0.0144\n",
            "Epoch: 98/1500 - Training Loss: 0.0130, Testing Loss: 0.0141\n",
            "Epoch: 99/1500 - Training Loss: 0.0130, Testing Loss: 0.0145\n",
            "Epoch: 100/1500 - Training Loss: 0.0128, Testing Loss: 0.0146\n",
            "Epoch: 101/1500 - Training Loss: 0.0128, Testing Loss: 0.0152\n",
            "Epoch: 102/1500 - Training Loss: 0.0132, Testing Loss: 0.0140\n",
            "Epoch: 103/1500 - Training Loss: 0.0127, Testing Loss: 0.0149\n",
            "Epoch: 104/1500 - Training Loss: 0.0129, Testing Loss: 0.0141\n",
            "Epoch: 105/1500 - Training Loss: 0.0126, Testing Loss: 0.0143\n",
            "Epoch: 106/1500 - Training Loss: 0.0127, Testing Loss: 0.0143\n",
            "Epoch: 107/1500 - Training Loss: 0.0127, Testing Loss: 0.0141\n",
            "Epoch: 108/1500 - Training Loss: 0.0129, Testing Loss: 0.0138\n",
            "Epoch: 109/1500 - Training Loss: 0.0126, Testing Loss: 0.0144\n",
            "Epoch: 110/1500 - Training Loss: 0.0127, Testing Loss: 0.0136\n",
            "Epoch: 111/1500 - Training Loss: 0.0126, Testing Loss: 0.0139\n",
            "Epoch: 112/1500 - Training Loss: 0.0127, Testing Loss: 0.0147\n",
            "Epoch: 113/1500 - Training Loss: 0.0125, Testing Loss: 0.0140\n",
            "Epoch: 114/1500 - Training Loss: 0.0124, Testing Loss: 0.0140\n",
            "Epoch: 115/1500 - Training Loss: 0.0127, Testing Loss: 0.0138\n",
            "Epoch: 116/1500 - Training Loss: 0.0125, Testing Loss: 0.0137\n",
            "Epoch: 117/1500 - Training Loss: 0.0126, Testing Loss: 0.0138\n",
            "Epoch: 118/1500 - Training Loss: 0.0124, Testing Loss: 0.0139\n",
            "Epoch: 119/1500 - Training Loss: 0.0128, Testing Loss: 0.0135\n",
            "Epoch: 120/1500 - Training Loss: 0.0122, Testing Loss: 0.0136\n",
            "Epoch: 121/1500 - Training Loss: 0.0123, Testing Loss: 0.0134\n",
            "Epoch: 122/1500 - Training Loss: 0.0123, Testing Loss: 0.0149\n",
            "Epoch: 123/1500 - Training Loss: 0.0122, Testing Loss: 0.0141\n",
            "Epoch: 124/1500 - Training Loss: 0.0122, Testing Loss: 0.0141\n",
            "Epoch: 125/1500 - Training Loss: 0.0123, Testing Loss: 0.0138\n",
            "Epoch: 126/1500 - Training Loss: 0.0126, Testing Loss: 0.0137\n",
            "Epoch: 127/1500 - Training Loss: 0.0124, Testing Loss: 0.0136\n",
            "Epoch: 128/1500 - Training Loss: 0.0122, Testing Loss: 0.0132\n",
            "Epoch: 129/1500 - Training Loss: 0.0120, Testing Loss: 0.0132\n",
            "Epoch: 130/1500 - Training Loss: 0.0120, Testing Loss: 0.0134\n",
            "Epoch: 131/1500 - Training Loss: 0.0121, Testing Loss: 0.0133\n",
            "Epoch: 132/1500 - Training Loss: 0.0120, Testing Loss: 0.0135\n",
            "Epoch: 133/1500 - Training Loss: 0.0119, Testing Loss: 0.0137\n",
            "Epoch: 134/1500 - Training Loss: 0.0123, Testing Loss: 0.0128\n",
            "Epoch: 135/1500 - Training Loss: 0.0119, Testing Loss: 0.0131\n",
            "Epoch: 136/1500 - Training Loss: 0.0119, Testing Loss: 0.0129\n",
            "Epoch: 137/1500 - Training Loss: 0.0120, Testing Loss: 0.0134\n",
            "Epoch: 138/1500 - Training Loss: 0.0120, Testing Loss: 0.0132\n",
            "Epoch: 139/1500 - Training Loss: 0.0119, Testing Loss: 0.0134\n",
            "Epoch: 140/1500 - Training Loss: 0.0118, Testing Loss: 0.0131\n",
            "Epoch: 141/1500 - Training Loss: 0.0120, Testing Loss: 0.0127\n",
            "Epoch: 142/1500 - Training Loss: 0.0118, Testing Loss: 0.0129\n",
            "Epoch: 143/1500 - Training Loss: 0.0118, Testing Loss: 0.0137\n",
            "Epoch: 144/1500 - Training Loss: 0.0117, Testing Loss: 0.0133\n",
            "Epoch: 145/1500 - Training Loss: 0.0118, Testing Loss: 0.0124\n",
            "Epoch: 146/1500 - Training Loss: 0.0119, Testing Loss: 0.0126\n",
            "Epoch: 147/1500 - Training Loss: 0.0118, Testing Loss: 0.0131\n",
            "Epoch: 148/1500 - Training Loss: 0.0118, Testing Loss: 0.0126\n",
            "Epoch: 149/1500 - Training Loss: 0.0118, Testing Loss: 0.0128\n",
            "Epoch: 150/1500 - Training Loss: 0.0121, Testing Loss: 0.0126\n",
            "Epoch: 151/1500 - Training Loss: 0.0117, Testing Loss: 0.0127\n",
            "Epoch: 152/1500 - Training Loss: 0.0115, Testing Loss: 0.0125\n",
            "Epoch: 153/1500 - Training Loss: 0.0114, Testing Loss: 0.0123\n",
            "Epoch: 154/1500 - Training Loss: 0.0116, Testing Loss: 0.0133\n",
            "Epoch: 155/1500 - Training Loss: 0.0116, Testing Loss: 0.0130\n",
            "Epoch: 156/1500 - Training Loss: 0.0116, Testing Loss: 0.0124\n",
            "Epoch: 157/1500 - Training Loss: 0.0116, Testing Loss: 0.0138\n",
            "Epoch: 158/1500 - Training Loss: 0.0116, Testing Loss: 0.0130\n",
            "Epoch: 159/1500 - Training Loss: 0.0116, Testing Loss: 0.0134\n",
            "Epoch: 160/1500 - Training Loss: 0.0116, Testing Loss: 0.0125\n",
            "Epoch: 161/1500 - Training Loss: 0.0115, Testing Loss: 0.0122\n",
            "Epoch: 162/1500 - Training Loss: 0.0114, Testing Loss: 0.0123\n",
            "Epoch: 163/1500 - Training Loss: 0.0113, Testing Loss: 0.0144\n",
            "Epoch: 164/1500 - Training Loss: 0.0117, Testing Loss: 0.0127\n",
            "Epoch: 165/1500 - Training Loss: 0.0115, Testing Loss: 0.0130\n",
            "Epoch: 166/1500 - Training Loss: 0.0114, Testing Loss: 0.0123\n",
            "Epoch: 167/1500 - Training Loss: 0.0114, Testing Loss: 0.0132\n",
            "Epoch: 168/1500 - Training Loss: 0.0115, Testing Loss: 0.0123\n",
            "Epoch: 169/1500 - Training Loss: 0.0114, Testing Loss: 0.0125\n",
            "Epoch: 170/1500 - Training Loss: 0.0115, Testing Loss: 0.0129\n",
            "Epoch: 171/1500 - Training Loss: 0.0113, Testing Loss: 0.0122\n",
            "Epoch: 172/1500 - Training Loss: 0.0113, Testing Loss: 0.0125\n",
            "Epoch: 173/1500 - Training Loss: 0.0112, Testing Loss: 0.0137\n",
            "Epoch: 174/1500 - Training Loss: 0.0114, Testing Loss: 0.0122\n",
            "Epoch: 175/1500 - Training Loss: 0.0113, Testing Loss: 0.0129\n",
            "Epoch: 176/1500 - Training Loss: 0.0114, Testing Loss: 0.0128\n",
            "Epoch: 177/1500 - Training Loss: 0.0113, Testing Loss: 0.0123\n",
            "Epoch: 178/1500 - Training Loss: 0.0114, Testing Loss: 0.0128\n",
            "Epoch: 179/1500 - Training Loss: 0.0117, Testing Loss: 0.0129\n",
            "Epoch: 180/1500 - Training Loss: 0.0113, Testing Loss: 0.0120\n",
            "Epoch: 181/1500 - Training Loss: 0.0113, Testing Loss: 0.0121\n",
            "Epoch: 182/1500 - Training Loss: 0.0115, Testing Loss: 0.0124\n",
            "Epoch: 183/1500 - Training Loss: 0.0112, Testing Loss: 0.0127\n",
            "Epoch: 184/1500 - Training Loss: 0.0113, Testing Loss: 0.0123\n",
            "Epoch: 185/1500 - Training Loss: 0.0113, Testing Loss: 0.0122\n",
            "Epoch: 186/1500 - Training Loss: 0.0115, Testing Loss: 0.0124\n",
            "Epoch: 187/1500 - Training Loss: 0.0112, Testing Loss: 0.0128\n",
            "Epoch: 188/1500 - Training Loss: 0.0114, Testing Loss: 0.0125\n",
            "Epoch: 189/1500 - Training Loss: 0.0110, Testing Loss: 0.0122\n",
            "Epoch: 190/1500 - Training Loss: 0.0114, Testing Loss: 0.0132\n",
            "Epoch: 191/1500 - Training Loss: 0.0113, Testing Loss: 0.0127\n",
            "Epoch: 192/1500 - Training Loss: 0.0113, Testing Loss: 0.0135\n",
            "Epoch: 193/1500 - Training Loss: 0.0112, Testing Loss: 0.0129\n",
            "Epoch: 194/1500 - Training Loss: 0.0110, Testing Loss: 0.0122\n",
            "Epoch: 195/1500 - Training Loss: 0.0114, Testing Loss: 0.0120\n",
            "Epoch: 196/1500 - Training Loss: 0.0110, Testing Loss: 0.0123\n",
            "Epoch: 197/1500 - Training Loss: 0.0111, Testing Loss: 0.0120\n",
            "Epoch: 198/1500 - Training Loss: 0.0116, Testing Loss: 0.0120\n",
            "Epoch: 199/1500 - Training Loss: 0.0111, Testing Loss: 0.0122\n",
            "Epoch: 200/1500 - Training Loss: 0.0110, Testing Loss: 0.0122\n",
            "Epoch: 201/1500 - Training Loss: 0.0112, Testing Loss: 0.0122\n",
            "Epoch: 202/1500 - Training Loss: 0.0108, Testing Loss: 0.0118\n",
            "Epoch: 203/1500 - Training Loss: 0.0110, Testing Loss: 0.0118\n",
            "Epoch: 204/1500 - Training Loss: 0.0110, Testing Loss: 0.0125\n",
            "Epoch: 205/1500 - Training Loss: 0.0110, Testing Loss: 0.0122\n",
            "Epoch: 206/1500 - Training Loss: 0.0110, Testing Loss: 0.0121\n",
            "Epoch: 207/1500 - Training Loss: 0.0111, Testing Loss: 0.0119\n",
            "Epoch: 208/1500 - Training Loss: 0.0109, Testing Loss: 0.0124\n",
            "Epoch: 209/1500 - Training Loss: 0.0113, Testing Loss: 0.0134\n",
            "Epoch: 210/1500 - Training Loss: 0.0114, Testing Loss: 0.0120\n",
            "Epoch: 211/1500 - Training Loss: 0.0110, Testing Loss: 0.0124\n",
            "Epoch: 212/1500 - Training Loss: 0.0111, Testing Loss: 0.0118\n",
            "Epoch: 213/1500 - Training Loss: 0.0109, Testing Loss: 0.0127\n",
            "Epoch: 214/1500 - Training Loss: 0.0108, Testing Loss: 0.0133\n",
            "Epoch: 215/1500 - Training Loss: 0.0111, Testing Loss: 0.0127\n",
            "Epoch: 216/1500 - Training Loss: 0.0110, Testing Loss: 0.0123\n",
            "Epoch: 217/1500 - Training Loss: 0.0108, Testing Loss: 0.0125\n",
            "Epoch: 218/1500 - Training Loss: 0.0108, Testing Loss: 0.0118\n",
            "Epoch: 219/1500 - Training Loss: 0.0110, Testing Loss: 0.0120\n",
            "Epoch: 220/1500 - Training Loss: 0.0110, Testing Loss: 0.0118\n",
            "Epoch: 221/1500 - Training Loss: 0.0108, Testing Loss: 0.0136\n",
            "Epoch: 222/1500 - Training Loss: 0.0111, Testing Loss: 0.0116\n",
            "Epoch: 223/1500 - Training Loss: 0.0107, Testing Loss: 0.0124\n",
            "Epoch: 224/1500 - Training Loss: 0.0109, Testing Loss: 0.0116\n",
            "Epoch: 225/1500 - Training Loss: 0.0107, Testing Loss: 0.0120\n",
            "Epoch: 226/1500 - Training Loss: 0.0110, Testing Loss: 0.0118\n",
            "Epoch: 227/1500 - Training Loss: 0.0108, Testing Loss: 0.0125\n",
            "Epoch: 228/1500 - Training Loss: 0.0108, Testing Loss: 0.0122\n",
            "Epoch: 229/1500 - Training Loss: 0.0106, Testing Loss: 0.0119\n",
            "Epoch: 230/1500 - Training Loss: 0.0107, Testing Loss: 0.0117\n",
            "Epoch: 231/1500 - Training Loss: 0.0105, Testing Loss: 0.0116\n",
            "Epoch: 232/1500 - Training Loss: 0.0108, Testing Loss: 0.0127\n",
            "Epoch: 233/1500 - Training Loss: 0.0107, Testing Loss: 0.0117\n",
            "Epoch: 234/1500 - Training Loss: 0.0109, Testing Loss: 0.0121\n",
            "Epoch: 235/1500 - Training Loss: 0.0108, Testing Loss: 0.0120\n",
            "Epoch: 236/1500 - Training Loss: 0.0108, Testing Loss: 0.0117\n",
            "Epoch: 237/1500 - Training Loss: 0.0108, Testing Loss: 0.0116\n",
            "Epoch: 238/1500 - Training Loss: 0.0105, Testing Loss: 0.0123\n",
            "Epoch: 239/1500 - Training Loss: 0.0106, Testing Loss: 0.0117\n",
            "Epoch: 240/1500 - Training Loss: 0.0106, Testing Loss: 0.0117\n",
            "Epoch: 241/1500 - Training Loss: 0.0106, Testing Loss: 0.0118\n",
            "Epoch: 242/1500 - Training Loss: 0.0108, Testing Loss: 0.0142\n",
            "Epoch: 243/1500 - Training Loss: 0.0107, Testing Loss: 0.0129\n",
            "Epoch: 244/1500 - Training Loss: 0.0105, Testing Loss: 0.0115\n",
            "Epoch: 245/1500 - Training Loss: 0.0107, Testing Loss: 0.0121\n",
            "Epoch: 246/1500 - Training Loss: 0.0106, Testing Loss: 0.0122\n",
            "Epoch: 247/1500 - Training Loss: 0.0108, Testing Loss: 0.0122\n",
            "Epoch: 248/1500 - Training Loss: 0.0109, Testing Loss: 0.0117\n",
            "Epoch: 249/1500 - Training Loss: 0.0107, Testing Loss: 0.0122\n",
            "Epoch: 250/1500 - Training Loss: 0.0108, Testing Loss: 0.0125\n",
            "Epoch: 251/1500 - Training Loss: 0.0106, Testing Loss: 0.0122\n",
            "Epoch: 252/1500 - Training Loss: 0.0105, Testing Loss: 0.0114\n",
            "Epoch: 253/1500 - Training Loss: 0.0105, Testing Loss: 0.0117\n",
            "Epoch: 254/1500 - Training Loss: 0.0108, Testing Loss: 0.0114\n",
            "Epoch: 255/1500 - Training Loss: 0.0105, Testing Loss: 0.0115\n",
            "Epoch: 256/1500 - Training Loss: 0.0106, Testing Loss: 0.0125\n",
            "Epoch: 257/1500 - Training Loss: 0.0106, Testing Loss: 0.0122\n",
            "Epoch: 258/1500 - Training Loss: 0.0107, Testing Loss: 0.0114\n",
            "Epoch: 259/1500 - Training Loss: 0.0104, Testing Loss: 0.0117\n",
            "Epoch: 260/1500 - Training Loss: 0.0104, Testing Loss: 0.0116\n",
            "Epoch: 261/1500 - Training Loss: 0.0105, Testing Loss: 0.0116\n",
            "Epoch: 262/1500 - Training Loss: 0.0107, Testing Loss: 0.0152\n",
            "Epoch: 263/1500 - Training Loss: 0.0108, Testing Loss: 0.0114\n",
            "Epoch: 264/1500 - Training Loss: 0.0107, Testing Loss: 0.0116\n",
            "Epoch: 265/1500 - Training Loss: 0.0107, Testing Loss: 0.0123\n",
            "Epoch: 266/1500 - Training Loss: 0.0113, Testing Loss: 0.0126\n",
            "Epoch: 267/1500 - Training Loss: 0.0105, Testing Loss: 0.0115\n",
            "Epoch: 268/1500 - Training Loss: 0.0104, Testing Loss: 0.0119\n",
            "Epoch: 269/1500 - Training Loss: 0.0104, Testing Loss: 0.0115\n",
            "Epoch: 270/1500 - Training Loss: 0.0104, Testing Loss: 0.0114\n",
            "Epoch: 271/1500 - Training Loss: 0.0103, Testing Loss: 0.0115\n",
            "Epoch: 272/1500 - Training Loss: 0.0105, Testing Loss: 0.0117\n",
            "Epoch: 273/1500 - Training Loss: 0.0105, Testing Loss: 0.0117\n",
            "Epoch: 274/1500 - Training Loss: 0.0106, Testing Loss: 0.0130\n",
            "Epoch: 275/1500 - Training Loss: 0.0106, Testing Loss: 0.0136\n",
            "Epoch: 276/1500 - Training Loss: 0.0106, Testing Loss: 0.0118\n",
            "Epoch: 277/1500 - Training Loss: 0.0104, Testing Loss: 0.0119\n",
            "Epoch: 278/1500 - Training Loss: 0.0103, Testing Loss: 0.0118\n",
            "Epoch: 279/1500 - Training Loss: 0.0104, Testing Loss: 0.0123\n",
            "Epoch: 280/1500 - Training Loss: 0.0104, Testing Loss: 0.0120\n",
            "Epoch: 281/1500 - Training Loss: 0.0105, Testing Loss: 0.0121\n",
            "Epoch: 282/1500 - Training Loss: 0.0105, Testing Loss: 0.0119\n",
            "Epoch: 283/1500 - Training Loss: 0.0105, Testing Loss: 0.0116\n",
            "Epoch: 284/1500 - Training Loss: 0.0104, Testing Loss: 0.0113\n",
            "Epoch: 285/1500 - Training Loss: 0.0102, Testing Loss: 0.0125\n",
            "Epoch: 286/1500 - Training Loss: 0.0103, Testing Loss: 0.0116\n",
            "Epoch: 287/1500 - Training Loss: 0.0105, Testing Loss: 0.0114\n",
            "Epoch: 288/1500 - Training Loss: 0.0107, Testing Loss: 0.0115\n",
            "Epoch: 289/1500 - Training Loss: 0.0103, Testing Loss: 0.0114\n",
            "Epoch: 290/1500 - Training Loss: 0.0106, Testing Loss: 0.0112\n",
            "Epoch: 291/1500 - Training Loss: 0.0103, Testing Loss: 0.0116\n",
            "Epoch: 292/1500 - Training Loss: 0.0105, Testing Loss: 0.0123\n",
            "Epoch: 293/1500 - Training Loss: 0.0105, Testing Loss: 0.0127\n",
            "Epoch: 294/1500 - Training Loss: 0.0103, Testing Loss: 0.0115\n",
            "Epoch: 295/1500 - Training Loss: 0.0103, Testing Loss: 0.0116\n",
            "Epoch: 296/1500 - Training Loss: 0.0105, Testing Loss: 0.0118\n",
            "Epoch: 297/1500 - Training Loss: 0.0103, Testing Loss: 0.0118\n",
            "Epoch: 298/1500 - Training Loss: 0.0103, Testing Loss: 0.0114\n",
            "Epoch: 299/1500 - Training Loss: 0.0107, Testing Loss: 0.0117\n",
            "Epoch: 300/1500 - Training Loss: 0.0102, Testing Loss: 0.0116\n",
            "Epoch: 301/1500 - Training Loss: 0.0106, Testing Loss: 0.0114\n",
            "Epoch: 302/1500 - Training Loss: 0.0105, Testing Loss: 0.0123\n",
            "Epoch: 303/1500 - Training Loss: 0.0106, Testing Loss: 0.0120\n",
            "Epoch: 304/1500 - Training Loss: 0.0103, Testing Loss: 0.0117\n",
            "Epoch: 305/1500 - Training Loss: 0.0107, Testing Loss: 0.0114\n",
            "Epoch: 306/1500 - Training Loss: 0.0102, Testing Loss: 0.0121\n",
            "Epoch: 307/1500 - Training Loss: 0.0104, Testing Loss: 0.0119\n",
            "Epoch: 308/1500 - Training Loss: 0.0102, Testing Loss: 0.0123\n",
            "Epoch: 309/1500 - Training Loss: 0.0102, Testing Loss: 0.0121\n",
            "Epoch: 310/1500 - Training Loss: 0.0103, Testing Loss: 0.0116\n",
            "Epoch: 311/1500 - Training Loss: 0.0102, Testing Loss: 0.0115\n",
            "Epoch: 312/1500 - Training Loss: 0.0105, Testing Loss: 0.0137\n",
            "Epoch: 313/1500 - Training Loss: 0.0106, Testing Loss: 0.0115\n",
            "Epoch: 314/1500 - Training Loss: 0.0106, Testing Loss: 0.0112\n",
            "Epoch: 315/1500 - Training Loss: 0.0104, Testing Loss: 0.0122\n",
            "Epoch: 316/1500 - Training Loss: 0.0103, Testing Loss: 0.0113\n",
            "Epoch: 317/1500 - Training Loss: 0.0104, Testing Loss: 0.0124\n",
            "Epoch: 318/1500 - Training Loss: 0.0102, Testing Loss: 0.0120\n",
            "Epoch: 319/1500 - Training Loss: 0.0105, Testing Loss: 0.0118\n",
            "Epoch: 320/1500 - Training Loss: 0.0101, Testing Loss: 0.0116\n",
            "Epoch: 321/1500 - Training Loss: 0.0105, Testing Loss: 0.0119\n",
            "Epoch: 322/1500 - Training Loss: 0.0109, Testing Loss: 0.0116\n",
            "Epoch: 323/1500 - Training Loss: 0.0104, Testing Loss: 0.0120\n",
            "Epoch: 324/1500 - Training Loss: 0.0103, Testing Loss: 0.0118\n",
            "Epoch: 325/1500 - Training Loss: 0.0102, Testing Loss: 0.0118\n",
            "Epoch: 326/1500 - Training Loss: 0.0103, Testing Loss: 0.0119\n",
            "Epoch: 327/1500 - Training Loss: 0.0100, Testing Loss: 0.0116\n",
            "Epoch: 328/1500 - Training Loss: 0.0103, Testing Loss: 0.0120\n",
            "Epoch: 329/1500 - Training Loss: 0.0101, Testing Loss: 0.0124\n",
            "Epoch: 330/1500 - Training Loss: 0.0103, Testing Loss: 0.0117\n",
            "Epoch: 331/1500 - Training Loss: 0.0102, Testing Loss: 0.0116\n",
            "Epoch: 332/1500 - Training Loss: 0.0101, Testing Loss: 0.0118\n",
            "Epoch: 333/1500 - Training Loss: 0.0104, Testing Loss: 0.0120\n",
            "Epoch: 334/1500 - Training Loss: 0.0105, Testing Loss: 0.0115\n",
            "Epoch: 335/1500 - Training Loss: 0.0104, Testing Loss: 0.0114\n",
            "Epoch: 336/1500 - Training Loss: 0.0101, Testing Loss: 0.0111\n",
            "Epoch: 337/1500 - Training Loss: 0.0103, Testing Loss: 0.0113\n",
            "Epoch: 338/1500 - Training Loss: 0.0102, Testing Loss: 0.0112\n",
            "Epoch: 339/1500 - Training Loss: 0.0101, Testing Loss: 0.0114\n",
            "Epoch: 340/1500 - Training Loss: 0.0100, Testing Loss: 0.0117\n",
            "Epoch: 341/1500 - Training Loss: 0.0101, Testing Loss: 0.0115\n",
            "Epoch: 342/1500 - Training Loss: 0.0101, Testing Loss: 0.0112\n",
            "Epoch: 343/1500 - Training Loss: 0.0100, Testing Loss: 0.0115\n",
            "Epoch: 344/1500 - Training Loss: 0.0101, Testing Loss: 0.0116\n",
            "Epoch: 345/1500 - Training Loss: 0.0102, Testing Loss: 0.0113\n",
            "Epoch: 346/1500 - Training Loss: 0.0101, Testing Loss: 0.0112\n",
            "Epoch: 347/1500 - Training Loss: 0.0101, Testing Loss: 0.0113\n",
            "Epoch: 348/1500 - Training Loss: 0.0101, Testing Loss: 0.0114\n",
            "Epoch: 349/1500 - Training Loss: 0.0103, Testing Loss: 0.0116\n",
            "Epoch: 350/1500 - Training Loss: 0.0103, Testing Loss: 0.0115\n",
            "Epoch: 351/1500 - Training Loss: 0.0102, Testing Loss: 0.0112\n",
            "Epoch: 352/1500 - Training Loss: 0.0100, Testing Loss: 0.0120\n",
            "Epoch: 353/1500 - Training Loss: 0.0105, Testing Loss: 0.0114\n",
            "Epoch: 354/1500 - Training Loss: 0.0101, Testing Loss: 0.0118\n",
            "Epoch: 355/1500 - Training Loss: 0.0100, Testing Loss: 0.0121\n",
            "Epoch: 356/1500 - Training Loss: 0.0103, Testing Loss: 0.0114\n",
            "Epoch: 357/1500 - Training Loss: 0.0101, Testing Loss: 0.0112\n",
            "Epoch: 358/1500 - Training Loss: 0.0100, Testing Loss: 0.0137\n",
            "Epoch: 359/1500 - Training Loss: 0.0101, Testing Loss: 0.0129\n",
            "Epoch: 360/1500 - Training Loss: 0.0102, Testing Loss: 0.0113\n",
            "Epoch: 361/1500 - Training Loss: 0.0101, Testing Loss: 0.0113\n",
            "Epoch: 362/1500 - Training Loss: 0.0103, Testing Loss: 0.0111\n",
            "Epoch: 363/1500 - Training Loss: 0.0101, Testing Loss: 0.0124\n",
            "Epoch: 364/1500 - Training Loss: 0.0102, Testing Loss: 0.0113\n",
            "Epoch: 365/1500 - Training Loss: 0.0111, Testing Loss: 0.0113\n",
            "Epoch: 366/1500 - Training Loss: 0.0102, Testing Loss: 0.0117\n",
            "Epoch: 367/1500 - Training Loss: 0.0103, Testing Loss: 0.0114\n",
            "Epoch: 368/1500 - Training Loss: 0.0100, Testing Loss: 0.0113\n",
            "Epoch: 369/1500 - Training Loss: 0.0100, Testing Loss: 0.0116\n",
            "Epoch: 370/1500 - Training Loss: 0.0102, Testing Loss: 0.0113\n",
            "Epoch: 371/1500 - Training Loss: 0.0100, Testing Loss: 0.0116\n",
            "Epoch: 372/1500 - Training Loss: 0.0101, Testing Loss: 0.0147\n",
            "Epoch: 373/1500 - Training Loss: 0.0099, Testing Loss: 0.0112\n",
            "Epoch: 374/1500 - Training Loss: 0.0101, Testing Loss: 0.0113\n",
            "Epoch: 375/1500 - Training Loss: 0.0103, Testing Loss: 0.0128\n",
            "Epoch: 376/1500 - Training Loss: 0.0099, Testing Loss: 0.0116\n",
            "Epoch: 377/1500 - Training Loss: 0.0099, Testing Loss: 0.0113\n",
            "Epoch: 378/1500 - Training Loss: 0.0099, Testing Loss: 0.0113\n",
            "Epoch: 379/1500 - Training Loss: 0.0101, Testing Loss: 0.0111\n",
            "Epoch: 380/1500 - Training Loss: 0.0100, Testing Loss: 0.0118\n",
            "Epoch: 381/1500 - Training Loss: 0.0101, Testing Loss: 0.0113\n",
            "Epoch: 382/1500 - Training Loss: 0.0099, Testing Loss: 0.0114\n",
            "Epoch: 383/1500 - Training Loss: 0.0100, Testing Loss: 0.0112\n",
            "Epoch: 384/1500 - Training Loss: 0.0098, Testing Loss: 0.0116\n",
            "Epoch: 385/1500 - Training Loss: 0.0099, Testing Loss: 0.0112\n",
            "Epoch: 386/1500 - Training Loss: 0.0098, Testing Loss: 0.0112\n",
            "Epoch: 387/1500 - Training Loss: 0.0100, Testing Loss: 0.0120\n",
            "Epoch: 388/1500 - Training Loss: 0.0110, Testing Loss: 0.0112\n",
            "Epoch: 389/1500 - Training Loss: 0.0099, Testing Loss: 0.0115\n",
            "Epoch: 390/1500 - Training Loss: 0.0100, Testing Loss: 0.0115\n",
            "Epoch: 391/1500 - Training Loss: 0.0098, Testing Loss: 0.0121\n",
            "Epoch: 392/1500 - Training Loss: 0.0101, Testing Loss: 0.0117\n",
            "Epoch: 393/1500 - Training Loss: 0.0101, Testing Loss: 0.0115\n",
            "Epoch: 394/1500 - Training Loss: 0.0100, Testing Loss: 0.0113\n",
            "Epoch: 395/1500 - Training Loss: 0.0100, Testing Loss: 0.0117\n",
            "Epoch: 396/1500 - Training Loss: 0.0099, Testing Loss: 0.0111\n",
            "Epoch: 397/1500 - Training Loss: 0.0100, Testing Loss: 0.0125\n",
            "Epoch: 398/1500 - Training Loss: 0.0098, Testing Loss: 0.0117\n",
            "Epoch: 399/1500 - Training Loss: 0.0100, Testing Loss: 0.0115\n",
            "Epoch: 400/1500 - Training Loss: 0.0102, Testing Loss: 0.0127\n",
            "Epoch: 401/1500 - Training Loss: 0.0100, Testing Loss: 0.0112\n",
            "Epoch: 402/1500 - Training Loss: 0.0097, Testing Loss: 0.0117\n",
            "Epoch: 403/1500 - Training Loss: 0.0099, Testing Loss: 0.0116\n",
            "Epoch: 404/1500 - Training Loss: 0.0098, Testing Loss: 0.0112\n",
            "Epoch: 405/1500 - Training Loss: 0.0098, Testing Loss: 0.0117\n",
            "Epoch: 406/1500 - Training Loss: 0.0099, Testing Loss: 0.0113\n",
            "Epoch: 407/1500 - Training Loss: 0.0098, Testing Loss: 0.0118\n",
            "Epoch: 408/1500 - Training Loss: 0.0097, Testing Loss: 0.0114\n",
            "Epoch: 409/1500 - Training Loss: 0.0098, Testing Loss: 0.0117\n",
            "Epoch: 410/1500 - Training Loss: 0.0101, Testing Loss: 0.0136\n",
            "Epoch: 411/1500 - Training Loss: 0.0100, Testing Loss: 0.0116\n",
            "Epoch: 412/1500 - Training Loss: 0.0097, Testing Loss: 0.0110\n",
            "Epoch: 413/1500 - Training Loss: 0.0097, Testing Loss: 0.0113\n",
            "Epoch: 414/1500 - Training Loss: 0.0098, Testing Loss: 0.0113\n",
            "Epoch: 415/1500 - Training Loss: 0.0098, Testing Loss: 0.0112\n",
            "Epoch: 416/1500 - Training Loss: 0.0099, Testing Loss: 0.0115\n",
            "Epoch: 417/1500 - Training Loss: 0.0099, Testing Loss: 0.0113\n",
            "Epoch: 418/1500 - Training Loss: 0.0100, Testing Loss: 0.0111\n",
            "Epoch: 419/1500 - Training Loss: 0.0099, Testing Loss: 0.0113\n",
            "Epoch: 420/1500 - Training Loss: 0.0098, Testing Loss: 0.0109\n",
            "Epoch: 421/1500 - Training Loss: 0.0097, Testing Loss: 0.0112\n",
            "Epoch: 422/1500 - Training Loss: 0.0101, Testing Loss: 0.0111\n",
            "Epoch: 423/1500 - Training Loss: 0.0098, Testing Loss: 0.0113\n",
            "Epoch: 424/1500 - Training Loss: 0.0097, Testing Loss: 0.0114\n",
            "Epoch: 425/1500 - Training Loss: 0.0096, Testing Loss: 0.0120\n",
            "Epoch: 426/1500 - Training Loss: 0.0097, Testing Loss: 0.0111\n",
            "Epoch: 427/1500 - Training Loss: 0.0097, Testing Loss: 0.0121\n",
            "Epoch: 428/1500 - Training Loss: 0.0097, Testing Loss: 0.0113\n",
            "Epoch: 429/1500 - Training Loss: 0.0102, Testing Loss: 0.0111\n",
            "Epoch: 430/1500 - Training Loss: 0.0097, Testing Loss: 0.0111\n",
            "Epoch: 431/1500 - Training Loss: 0.0099, Testing Loss: 0.0115\n",
            "Epoch: 432/1500 - Training Loss: 0.0100, Testing Loss: 0.0111\n",
            "Epoch: 433/1500 - Training Loss: 0.0098, Testing Loss: 0.0115\n",
            "Epoch: 434/1500 - Training Loss: 0.0096, Testing Loss: 0.0110\n",
            "Epoch: 435/1500 - Training Loss: 0.0097, Testing Loss: 0.0117\n",
            "Epoch: 436/1500 - Training Loss: 0.0097, Testing Loss: 0.0116\n",
            "Epoch: 437/1500 - Training Loss: 0.0097, Testing Loss: 0.0120\n",
            "Epoch: 438/1500 - Training Loss: 0.0096, Testing Loss: 0.0111\n",
            "Epoch: 439/1500 - Training Loss: 0.0098, Testing Loss: 0.0112\n",
            "Epoch: 440/1500 - Training Loss: 0.0098, Testing Loss: 0.0118\n",
            "Epoch: 441/1500 - Training Loss: 0.0098, Testing Loss: 0.0115\n",
            "Epoch: 442/1500 - Training Loss: 0.0098, Testing Loss: 0.0113\n",
            "Epoch: 443/1500 - Training Loss: 0.0096, Testing Loss: 0.0111\n",
            "Epoch: 444/1500 - Training Loss: 0.0099, Testing Loss: 0.0114\n",
            "Epoch: 445/1500 - Training Loss: 0.0097, Testing Loss: 0.0109\n",
            "Epoch: 446/1500 - Training Loss: 0.0096, Testing Loss: 0.0120\n",
            "Epoch: 447/1500 - Training Loss: 0.0097, Testing Loss: 0.0110\n",
            "Epoch: 448/1500 - Training Loss: 0.0095, Testing Loss: 0.0115\n",
            "Epoch: 449/1500 - Training Loss: 0.0096, Testing Loss: 0.0114\n",
            "Epoch: 450/1500 - Training Loss: 0.0096, Testing Loss: 0.0113\n",
            "Epoch: 451/1500 - Training Loss: 0.0097, Testing Loss: 0.0112\n",
            "Epoch: 452/1500 - Training Loss: 0.0096, Testing Loss: 0.0114\n",
            "Epoch: 453/1500 - Training Loss: 0.0097, Testing Loss: 0.0119\n",
            "Epoch: 454/1500 - Training Loss: 0.0098, Testing Loss: 0.0118\n",
            "Epoch: 455/1500 - Training Loss: 0.0095, Testing Loss: 0.0111\n",
            "Epoch: 456/1500 - Training Loss: 0.0097, Testing Loss: 0.0110\n",
            "Epoch: 457/1500 - Training Loss: 0.0096, Testing Loss: 0.0111\n",
            "Epoch: 458/1500 - Training Loss: 0.0096, Testing Loss: 0.0111\n",
            "Epoch: 459/1500 - Training Loss: 0.0100, Testing Loss: 0.0111\n",
            "Epoch: 460/1500 - Training Loss: 0.0097, Testing Loss: 0.0109\n",
            "Epoch: 461/1500 - Training Loss: 0.0096, Testing Loss: 0.0119\n",
            "Epoch: 462/1500 - Training Loss: 0.0097, Testing Loss: 0.0117\n",
            "Epoch: 463/1500 - Training Loss: 0.0096, Testing Loss: 0.0112\n",
            "Epoch: 464/1500 - Training Loss: 0.0096, Testing Loss: 0.0112\n",
            "Epoch: 465/1500 - Training Loss: 0.0097, Testing Loss: 0.0110\n",
            "Epoch: 466/1500 - Training Loss: 0.0096, Testing Loss: 0.0111\n",
            "Epoch: 467/1500 - Training Loss: 0.0096, Testing Loss: 0.0115\n",
            "Epoch: 468/1500 - Training Loss: 0.0098, Testing Loss: 0.0112\n",
            "Epoch: 469/1500 - Training Loss: 0.0099, Testing Loss: 0.0117\n",
            "Epoch: 470/1500 - Training Loss: 0.0098, Testing Loss: 0.0119\n",
            "Epoch: 471/1500 - Training Loss: 0.0096, Testing Loss: 0.0115\n",
            "Epoch: 472/1500 - Training Loss: 0.0096, Testing Loss: 0.0113\n",
            "Epoch: 473/1500 - Training Loss: 0.0095, Testing Loss: 0.0137\n",
            "Epoch: 474/1500 - Training Loss: 0.0098, Testing Loss: 0.0110\n",
            "Epoch: 475/1500 - Training Loss: 0.0097, Testing Loss: 0.0112\n",
            "Epoch: 476/1500 - Training Loss: 0.0095, Testing Loss: 0.0118\n",
            "Epoch: 477/1500 - Training Loss: 0.0096, Testing Loss: 0.0112\n",
            "Epoch: 478/1500 - Training Loss: 0.0096, Testing Loss: 0.0110\n",
            "Epoch: 479/1500 - Training Loss: 0.0101, Testing Loss: 0.0114\n",
            "Epoch: 480/1500 - Training Loss: 0.0096, Testing Loss: 0.0112\n",
            "Epoch: 481/1500 - Training Loss: 0.0096, Testing Loss: 0.0118\n",
            "Epoch: 482/1500 - Training Loss: 0.0097, Testing Loss: 0.0111\n",
            "Epoch: 483/1500 - Training Loss: 0.0095, Testing Loss: 0.0113\n",
            "Epoch: 484/1500 - Training Loss: 0.0096, Testing Loss: 0.0108\n",
            "Epoch: 485/1500 - Training Loss: 0.0095, Testing Loss: 0.0110\n",
            "Epoch: 486/1500 - Training Loss: 0.0096, Testing Loss: 0.0112\n",
            "Epoch: 487/1500 - Training Loss: 0.0097, Testing Loss: 0.0112\n",
            "Epoch: 488/1500 - Training Loss: 0.0094, Testing Loss: 0.0125\n",
            "Epoch: 489/1500 - Training Loss: 0.0097, Testing Loss: 0.0125\n",
            "Epoch: 490/1500 - Training Loss: 0.0095, Testing Loss: 0.0114\n",
            "Epoch: 491/1500 - Training Loss: 0.0100, Testing Loss: 0.0110\n",
            "Epoch: 492/1500 - Training Loss: 0.0095, Testing Loss: 0.0110\n",
            "Epoch: 493/1500 - Training Loss: 0.0095, Testing Loss: 0.0110\n",
            "Epoch: 494/1500 - Training Loss: 0.0095, Testing Loss: 0.0110\n",
            "Epoch: 495/1500 - Training Loss: 0.0094, Testing Loss: 0.0111\n",
            "Epoch: 496/1500 - Training Loss: 0.0096, Testing Loss: 0.0117\n",
            "Epoch: 497/1500 - Training Loss: 0.0094, Testing Loss: 0.0118\n",
            "Epoch: 498/1500 - Training Loss: 0.0095, Testing Loss: 0.0118\n",
            "Epoch: 499/1500 - Training Loss: 0.0094, Testing Loss: 0.0111\n",
            "Epoch: 500/1500 - Training Loss: 0.0095, Testing Loss: 0.0112\n",
            "Epoch: 501/1500 - Training Loss: 0.0095, Testing Loss: 0.0110\n",
            "Epoch: 502/1500 - Training Loss: 0.0095, Testing Loss: 0.0108\n",
            "Epoch: 503/1500 - Training Loss: 0.0099, Testing Loss: 0.0108\n",
            "Epoch: 504/1500 - Training Loss: 0.0095, Testing Loss: 0.0114\n",
            "Epoch: 505/1500 - Training Loss: 0.0098, Testing Loss: 0.0113\n",
            "Epoch: 506/1500 - Training Loss: 0.0095, Testing Loss: 0.0112\n",
            "Epoch: 507/1500 - Training Loss: 0.0095, Testing Loss: 0.0112\n",
            "Epoch: 508/1500 - Training Loss: 0.0096, Testing Loss: 0.0111\n",
            "Epoch: 509/1500 - Training Loss: 0.0095, Testing Loss: 0.0114\n",
            "Epoch: 510/1500 - Training Loss: 0.0093, Testing Loss: 0.0112\n",
            "Epoch: 511/1500 - Training Loss: 0.0094, Testing Loss: 0.0110\n",
            "Epoch: 512/1500 - Training Loss: 0.0095, Testing Loss: 0.0110\n",
            "Epoch: 513/1500 - Training Loss: 0.0094, Testing Loss: 0.0113\n",
            "Epoch: 514/1500 - Training Loss: 0.0095, Testing Loss: 0.0111\n",
            "Epoch: 515/1500 - Training Loss: 0.0093, Testing Loss: 0.0110\n",
            "Epoch: 516/1500 - Training Loss: 0.0098, Testing Loss: 0.0112\n",
            "Epoch: 517/1500 - Training Loss: 0.0095, Testing Loss: 0.0111\n",
            "Epoch: 518/1500 - Training Loss: 0.0096, Testing Loss: 0.0116\n",
            "Epoch: 519/1500 - Training Loss: 0.0095, Testing Loss: 0.0110\n",
            "Epoch: 520/1500 - Training Loss: 0.0094, Testing Loss: 0.0112\n",
            "Epoch: 521/1500 - Training Loss: 0.0094, Testing Loss: 0.0111\n",
            "Epoch: 522/1500 - Training Loss: 0.0094, Testing Loss: 0.0124\n",
            "Epoch: 523/1500 - Training Loss: 0.0097, Testing Loss: 0.0125\n",
            "Epoch: 524/1500 - Training Loss: 0.0094, Testing Loss: 0.0112\n",
            "Epoch: 525/1500 - Training Loss: 0.0092, Testing Loss: 0.0110\n",
            "Epoch: 526/1500 - Training Loss: 0.0095, Testing Loss: 0.0113\n",
            "Epoch: 527/1500 - Training Loss: 0.0093, Testing Loss: 0.0112\n",
            "Epoch: 528/1500 - Training Loss: 0.0093, Testing Loss: 0.0116\n",
            "Epoch: 529/1500 - Training Loss: 0.0095, Testing Loss: 0.0116\n",
            "Epoch: 530/1500 - Training Loss: 0.0093, Testing Loss: 0.0112\n",
            "Epoch: 531/1500 - Training Loss: 0.0093, Testing Loss: 0.0115\n",
            "Epoch: 532/1500 - Training Loss: 0.0095, Testing Loss: 0.0118\n",
            "Epoch: 533/1500 - Training Loss: 0.0095, Testing Loss: 0.0112\n",
            "Epoch: 534/1500 - Training Loss: 0.0095, Testing Loss: 0.0112\n",
            "Epoch: 535/1500 - Training Loss: 0.0092, Testing Loss: 0.0122\n",
            "Epoch: 536/1500 - Training Loss: 0.0093, Testing Loss: 0.0112\n",
            "Epoch: 537/1500 - Training Loss: 0.0094, Testing Loss: 0.0118\n",
            "Epoch: 538/1500 - Training Loss: 0.0094, Testing Loss: 0.0114\n",
            "Epoch: 539/1500 - Training Loss: 0.0094, Testing Loss: 0.0114\n",
            "Epoch: 540/1500 - Training Loss: 0.0092, Testing Loss: 0.0108\n",
            "Epoch: 541/1500 - Training Loss: 0.0094, Testing Loss: 0.0110\n",
            "Epoch: 542/1500 - Training Loss: 0.0095, Testing Loss: 0.0110\n",
            "Epoch: 543/1500 - Training Loss: 0.0094, Testing Loss: 0.0112\n",
            "Epoch: 544/1500 - Training Loss: 0.0097, Testing Loss: 0.0110\n",
            "Epoch: 545/1500 - Training Loss: 0.0093, Testing Loss: 0.0130\n",
            "Epoch: 546/1500 - Training Loss: 0.0095, Testing Loss: 0.0115\n",
            "Epoch: 547/1500 - Training Loss: 0.0094, Testing Loss: 0.0110\n",
            "Epoch: 548/1500 - Training Loss: 0.0094, Testing Loss: 0.0121\n",
            "Epoch: 549/1500 - Training Loss: 0.0094, Testing Loss: 0.0111\n",
            "Epoch: 550/1500 - Training Loss: 0.0092, Testing Loss: 0.0117\n",
            "Epoch: 551/1500 - Training Loss: 0.0092, Testing Loss: 0.0113\n",
            "Epoch: 552/1500 - Training Loss: 0.0097, Testing Loss: 0.0110\n",
            "Epoch: 553/1500 - Training Loss: 0.0093, Testing Loss: 0.0113\n",
            "Epoch: 554/1500 - Training Loss: 0.0093, Testing Loss: 0.0109\n",
            "Epoch: 555/1500 - Training Loss: 0.0092, Testing Loss: 0.0111\n",
            "Epoch: 556/1500 - Training Loss: 0.0094, Testing Loss: 0.0131\n",
            "Epoch: 557/1500 - Training Loss: 0.0093, Testing Loss: 0.0116\n",
            "Epoch: 558/1500 - Training Loss: 0.0092, Testing Loss: 0.0122\n",
            "Epoch: 559/1500 - Training Loss: 0.0096, Testing Loss: 0.0112\n",
            "Epoch: 560/1500 - Training Loss: 0.0093, Testing Loss: 0.0112\n",
            "Epoch: 561/1500 - Training Loss: 0.0091, Testing Loss: 0.0113\n",
            "Epoch: 562/1500 - Training Loss: 0.0093, Testing Loss: 0.0110\n",
            "Epoch: 563/1500 - Training Loss: 0.0093, Testing Loss: 0.0113\n",
            "Epoch: 564/1500 - Training Loss: 0.0093, Testing Loss: 0.0114\n",
            "Epoch: 565/1500 - Training Loss: 0.0092, Testing Loss: 0.0112\n",
            "Epoch: 566/1500 - Training Loss: 0.0092, Testing Loss: 0.0113\n",
            "Epoch: 567/1500 - Training Loss: 0.0092, Testing Loss: 0.0116\n",
            "Epoch: 568/1500 - Training Loss: 0.0092, Testing Loss: 0.0116\n",
            "Epoch: 569/1500 - Training Loss: 0.0092, Testing Loss: 0.0109\n",
            "Epoch: 570/1500 - Training Loss: 0.0095, Testing Loss: 0.0112\n",
            "Epoch: 571/1500 - Training Loss: 0.0092, Testing Loss: 0.0111\n",
            "Epoch: 572/1500 - Training Loss: 0.0093, Testing Loss: 0.0119\n",
            "Epoch: 573/1500 - Training Loss: 0.0093, Testing Loss: 0.0113\n",
            "Epoch: 574/1500 - Training Loss: 0.0092, Testing Loss: 0.0110\n",
            "Epoch: 575/1500 - Training Loss: 0.0093, Testing Loss: 0.0113\n",
            "Epoch: 576/1500 - Training Loss: 0.0092, Testing Loss: 0.0116\n",
            "Epoch: 577/1500 - Training Loss: 0.0092, Testing Loss: 0.0117\n",
            "Epoch: 578/1500 - Training Loss: 0.0094, Testing Loss: 0.0115\n",
            "Epoch: 579/1500 - Training Loss: 0.0092, Testing Loss: 0.0116\n",
            "Epoch: 580/1500 - Training Loss: 0.0092, Testing Loss: 0.0113\n",
            "Epoch: 581/1500 - Training Loss: 0.0092, Testing Loss: 0.0116\n",
            "Epoch: 582/1500 - Training Loss: 0.0094, Testing Loss: 0.0114\n",
            "Epoch: 583/1500 - Training Loss: 0.0093, Testing Loss: 0.0120\n",
            "Epoch: 584/1500 - Training Loss: 0.0092, Testing Loss: 0.0115\n",
            "Epoch: 585/1500 - Training Loss: 0.0092, Testing Loss: 0.0111\n",
            "Epoch: 586/1500 - Training Loss: 0.0092, Testing Loss: 0.0111\n",
            "Epoch: 587/1500 - Training Loss: 0.0092, Testing Loss: 0.0115\n",
            "Epoch: 588/1500 - Training Loss: 0.0092, Testing Loss: 0.0115\n",
            "Epoch: 589/1500 - Training Loss: 0.0093, Testing Loss: 0.0114\n",
            "Epoch: 590/1500 - Training Loss: 0.0094, Testing Loss: 0.0113\n",
            "Epoch: 591/1500 - Training Loss: 0.0091, Testing Loss: 0.0116\n",
            "Epoch: 592/1500 - Training Loss: 0.0091, Testing Loss: 0.0115\n",
            "Epoch: 593/1500 - Training Loss: 0.0091, Testing Loss: 0.0110\n",
            "Epoch: 594/1500 - Training Loss: 0.0092, Testing Loss: 0.0112\n",
            "Epoch: 595/1500 - Training Loss: 0.0091, Testing Loss: 0.0111\n",
            "Epoch: 596/1500 - Training Loss: 0.0093, Testing Loss: 0.0111\n",
            "Epoch: 597/1500 - Training Loss: 0.0091, Testing Loss: 0.0111\n",
            "Epoch: 598/1500 - Training Loss: 0.0090, Testing Loss: 0.0113\n",
            "Epoch: 599/1500 - Training Loss: 0.0093, Testing Loss: 0.0115\n",
            "Epoch: 600/1500 - Training Loss: 0.0096, Testing Loss: 0.0121\n",
            "Epoch: 601/1500 - Training Loss: 0.0092, Testing Loss: 0.0112\n",
            "Epoch: 602/1500 - Training Loss: 0.0091, Testing Loss: 0.0111\n",
            "Epoch: 603/1500 - Training Loss: 0.0092, Testing Loss: 0.0116\n",
            "Epoch: 604/1500 - Training Loss: 0.0092, Testing Loss: 0.0112\n",
            "Epoch: 605/1500 - Training Loss: 0.0093, Testing Loss: 0.0115\n",
            "Epoch: 606/1500 - Training Loss: 0.0092, Testing Loss: 0.0110\n",
            "Epoch: 607/1500 - Training Loss: 0.0096, Testing Loss: 0.0114\n",
            "Epoch: 608/1500 - Training Loss: 0.0091, Testing Loss: 0.0110\n",
            "Epoch: 609/1500 - Training Loss: 0.0092, Testing Loss: 0.0116\n",
            "Epoch: 610/1500 - Training Loss: 0.0091, Testing Loss: 0.0117\n",
            "Epoch: 611/1500 - Training Loss: 0.0092, Testing Loss: 0.0117\n",
            "Epoch: 612/1500 - Training Loss: 0.0092, Testing Loss: 0.0111\n",
            "Epoch: 613/1500 - Training Loss: 0.0091, Testing Loss: 0.0112\n",
            "Epoch: 614/1500 - Training Loss: 0.0091, Testing Loss: 0.0112\n",
            "Epoch: 615/1500 - Training Loss: 0.0090, Testing Loss: 0.0114\n",
            "Epoch: 616/1500 - Training Loss: 0.0091, Testing Loss: 0.0111\n",
            "Epoch: 617/1500 - Training Loss: 0.0093, Testing Loss: 0.0111\n",
            "Epoch: 618/1500 - Training Loss: 0.0091, Testing Loss: 0.0113\n",
            "Epoch: 619/1500 - Training Loss: 0.0092, Testing Loss: 0.0116\n",
            "Epoch: 620/1500 - Training Loss: 0.0093, Testing Loss: 0.0110\n",
            "Epoch: 621/1500 - Training Loss: 0.0091, Testing Loss: 0.0111\n",
            "Epoch: 622/1500 - Training Loss: 0.0090, Testing Loss: 0.0114\n",
            "Epoch: 623/1500 - Training Loss: 0.0091, Testing Loss: 0.0112\n",
            "Epoch: 624/1500 - Training Loss: 0.0089, Testing Loss: 0.0116\n",
            "Epoch: 625/1500 - Training Loss: 0.0091, Testing Loss: 0.0114\n",
            "Epoch: 626/1500 - Training Loss: 0.0092, Testing Loss: 0.0113\n",
            "Epoch: 627/1500 - Training Loss: 0.0093, Testing Loss: 0.0110\n",
            "Epoch: 628/1500 - Training Loss: 0.0091, Testing Loss: 0.0112\n",
            "Epoch: 629/1500 - Training Loss: 0.0091, Testing Loss: 0.0111\n",
            "Epoch: 630/1500 - Training Loss: 0.0090, Testing Loss: 0.0117\n",
            "Epoch: 631/1500 - Training Loss: 0.0090, Testing Loss: 0.0112\n",
            "Epoch: 632/1500 - Training Loss: 0.0091, Testing Loss: 0.0112\n",
            "Epoch: 633/1500 - Training Loss: 0.0093, Testing Loss: 0.0110\n",
            "Epoch: 634/1500 - Training Loss: 0.0091, Testing Loss: 0.0116\n",
            "Epoch: 635/1500 - Training Loss: 0.0090, Testing Loss: 0.0113\n",
            "Epoch: 636/1500 - Training Loss: 0.0091, Testing Loss: 0.0112\n",
            "Epoch: 637/1500 - Training Loss: 0.0090, Testing Loss: 0.0125\n",
            "Epoch: 638/1500 - Training Loss: 0.0090, Testing Loss: 0.0114\n",
            "Epoch: 639/1500 - Training Loss: 0.0091, Testing Loss: 0.0109\n",
            "Epoch: 640/1500 - Training Loss: 0.0093, Testing Loss: 0.0111\n",
            "Epoch: 641/1500 - Training Loss: 0.0091, Testing Loss: 0.0114\n",
            "Epoch: 642/1500 - Training Loss: 0.0091, Testing Loss: 0.0110\n",
            "Epoch: 643/1500 - Training Loss: 0.0091, Testing Loss: 0.0112\n",
            "Epoch: 644/1500 - Training Loss: 0.0091, Testing Loss: 0.0113\n",
            "Epoch: 645/1500 - Training Loss: 0.0091, Testing Loss: 0.0110\n",
            "Epoch: 646/1500 - Training Loss: 0.0090, Testing Loss: 0.0111\n",
            "Epoch: 647/1500 - Training Loss: 0.0091, Testing Loss: 0.0109\n",
            "Epoch: 648/1500 - Training Loss: 0.0090, Testing Loss: 0.0112\n",
            "Epoch: 649/1500 - Training Loss: 0.0092, Testing Loss: 0.0116\n",
            "Epoch: 650/1500 - Training Loss: 0.0091, Testing Loss: 0.0113\n",
            "Epoch: 651/1500 - Training Loss: 0.0098, Testing Loss: 0.0113\n",
            "Epoch: 652/1500 - Training Loss: 0.0090, Testing Loss: 0.0110\n",
            "Epoch: 653/1500 - Training Loss: 0.0092, Testing Loss: 0.0116\n",
            "Epoch: 654/1500 - Training Loss: 0.0090, Testing Loss: 0.0117\n",
            "Epoch: 655/1500 - Training Loss: 0.0090, Testing Loss: 0.0112\n",
            "Epoch: 656/1500 - Training Loss: 0.0090, Testing Loss: 0.0110\n",
            "Epoch: 657/1500 - Training Loss: 0.0090, Testing Loss: 0.0114\n",
            "Epoch: 658/1500 - Training Loss: 0.0092, Testing Loss: 0.0111\n",
            "Epoch: 659/1500 - Training Loss: 0.0091, Testing Loss: 0.0111\n",
            "Epoch: 660/1500 - Training Loss: 0.0090, Testing Loss: 0.0113\n",
            "Epoch: 661/1500 - Training Loss: 0.0089, Testing Loss: 0.0119\n",
            "Epoch: 662/1500 - Training Loss: 0.0091, Testing Loss: 0.0120\n",
            "Epoch: 663/1500 - Training Loss: 0.0090, Testing Loss: 0.0119\n",
            "Epoch: 664/1500 - Training Loss: 0.0093, Testing Loss: 0.0115\n",
            "Epoch: 665/1500 - Training Loss: 0.0091, Testing Loss: 0.0110\n",
            "Epoch: 666/1500 - Training Loss: 0.0089, Testing Loss: 0.0113\n",
            "Epoch: 667/1500 - Training Loss: 0.0090, Testing Loss: 0.0112\n",
            "Epoch: 668/1500 - Training Loss: 0.0092, Testing Loss: 0.0124\n",
            "Epoch: 669/1500 - Training Loss: 0.0089, Testing Loss: 0.0112\n",
            "Epoch: 670/1500 - Training Loss: 0.0091, Testing Loss: 0.0116\n",
            "Epoch: 671/1500 - Training Loss: 0.0091, Testing Loss: 0.0114\n",
            "Epoch: 672/1500 - Training Loss: 0.0091, Testing Loss: 0.0113\n",
            "Epoch: 673/1500 - Training Loss: 0.0090, Testing Loss: 0.0110\n",
            "Epoch: 674/1500 - Training Loss: 0.0090, Testing Loss: 0.0111\n",
            "Epoch: 675/1500 - Training Loss: 0.0092, Testing Loss: 0.0112\n",
            "Epoch: 676/1500 - Training Loss: 0.0089, Testing Loss: 0.0128\n",
            "Epoch: 677/1500 - Training Loss: 0.0091, Testing Loss: 0.0112\n",
            "Epoch: 678/1500 - Training Loss: 0.0091, Testing Loss: 0.0112\n",
            "Epoch: 679/1500 - Training Loss: 0.0089, Testing Loss: 0.0110\n",
            "Epoch: 680/1500 - Training Loss: 0.0089, Testing Loss: 0.0113\n",
            "Epoch: 681/1500 - Training Loss: 0.0090, Testing Loss: 0.0109\n",
            "Epoch: 682/1500 - Training Loss: 0.0090, Testing Loss: 0.0118\n",
            "Epoch: 683/1500 - Training Loss: 0.0091, Testing Loss: 0.0110\n",
            "Epoch: 684/1500 - Training Loss: 0.0090, Testing Loss: 0.0108\n",
            "Epoch: 685/1500 - Training Loss: 0.0089, Testing Loss: 0.0113\n",
            "Epoch: 686/1500 - Training Loss: 0.0090, Testing Loss: 0.0109\n",
            "Epoch: 687/1500 - Training Loss: 0.0090, Testing Loss: 0.0120\n",
            "Epoch: 688/1500 - Training Loss: 0.0091, Testing Loss: 0.0115\n",
            "Epoch: 689/1500 - Training Loss: 0.0090, Testing Loss: 0.0111\n",
            "Epoch: 690/1500 - Training Loss: 0.0090, Testing Loss: 0.0111\n",
            "Epoch: 691/1500 - Training Loss: 0.0089, Testing Loss: 0.0117\n",
            "Epoch: 692/1500 - Training Loss: 0.0090, Testing Loss: 0.0116\n",
            "Epoch: 693/1500 - Training Loss: 0.0089, Testing Loss: 0.0112\n",
            "Epoch: 694/1500 - Training Loss: 0.0090, Testing Loss: 0.0113\n",
            "Epoch: 695/1500 - Training Loss: 0.0090, Testing Loss: 0.0120\n",
            "Epoch: 696/1500 - Training Loss: 0.0091, Testing Loss: 0.0111\n",
            "Epoch: 697/1500 - Training Loss: 0.0090, Testing Loss: 0.0118\n",
            "Epoch: 698/1500 - Training Loss: 0.0090, Testing Loss: 0.0118\n",
            "Epoch: 699/1500 - Training Loss: 0.0090, Testing Loss: 0.0117\n",
            "Epoch: 700/1500 - Training Loss: 0.0088, Testing Loss: 0.0109\n",
            "Epoch: 701/1500 - Training Loss: 0.0090, Testing Loss: 0.0112\n",
            "Epoch: 702/1500 - Training Loss: 0.0091, Testing Loss: 0.0113\n",
            "Epoch: 703/1500 - Training Loss: 0.0090, Testing Loss: 0.0111\n",
            "Epoch: 704/1500 - Training Loss: 0.0087, Testing Loss: 0.0109\n",
            "Epoch: 705/1500 - Training Loss: 0.0089, Testing Loss: 0.0115\n",
            "Epoch: 706/1500 - Training Loss: 0.0089, Testing Loss: 0.0114\n",
            "Epoch: 707/1500 - Training Loss: 0.0089, Testing Loss: 0.0114\n",
            "Epoch: 708/1500 - Training Loss: 0.0088, Testing Loss: 0.0111\n",
            "Epoch: 709/1500 - Training Loss: 0.0089, Testing Loss: 0.0114\n",
            "Epoch: 710/1500 - Training Loss: 0.0090, Testing Loss: 0.0109\n",
            "Epoch: 711/1500 - Training Loss: 0.0089, Testing Loss: 0.0110\n",
            "Epoch: 712/1500 - Training Loss: 0.0092, Testing Loss: 0.0114\n",
            "Epoch: 713/1500 - Training Loss: 0.0090, Testing Loss: 0.0113\n",
            "Epoch: 714/1500 - Training Loss: 0.0090, Testing Loss: 0.0112\n",
            "Epoch: 715/1500 - Training Loss: 0.0089, Testing Loss: 0.0116\n",
            "Epoch: 716/1500 - Training Loss: 0.0090, Testing Loss: 0.0110\n",
            "Epoch: 717/1500 - Training Loss: 0.0089, Testing Loss: 0.0111\n",
            "Epoch: 718/1500 - Training Loss: 0.0089, Testing Loss: 0.0113\n",
            "Epoch: 719/1500 - Training Loss: 0.0089, Testing Loss: 0.0109\n",
            "Epoch: 720/1500 - Training Loss: 0.0089, Testing Loss: 0.0116\n",
            "Epoch: 721/1500 - Training Loss: 0.0089, Testing Loss: 0.0113\n",
            "Epoch: 722/1500 - Training Loss: 0.0089, Testing Loss: 0.0111\n",
            "Epoch: 723/1500 - Training Loss: 0.0089, Testing Loss: 0.0115\n",
            "Epoch: 724/1500 - Training Loss: 0.0088, Testing Loss: 0.0113\n",
            "Epoch: 725/1500 - Training Loss: 0.0089, Testing Loss: 0.0110\n",
            "Epoch: 726/1500 - Training Loss: 0.0088, Testing Loss: 0.0112\n",
            "Epoch: 727/1500 - Training Loss: 0.0088, Testing Loss: 0.0112\n",
            "Epoch: 728/1500 - Training Loss: 0.0089, Testing Loss: 0.0108\n",
            "Epoch: 729/1500 - Training Loss: 0.0090, Testing Loss: 0.0111\n",
            "Epoch: 730/1500 - Training Loss: 0.0088, Testing Loss: 0.0112\n",
            "Epoch: 731/1500 - Training Loss: 0.0091, Testing Loss: 0.0111\n",
            "Epoch: 732/1500 - Training Loss: 0.0089, Testing Loss: 0.0109\n",
            "Epoch: 733/1500 - Training Loss: 0.0092, Testing Loss: 0.0110\n",
            "Epoch: 734/1500 - Training Loss: 0.0090, Testing Loss: 0.0107\n",
            "Epoch: 735/1500 - Training Loss: 0.0090, Testing Loss: 0.0111\n",
            "Epoch: 736/1500 - Training Loss: 0.0089, Testing Loss: 0.0115\n",
            "Epoch: 737/1500 - Training Loss: 0.0088, Testing Loss: 0.0110\n",
            "Epoch: 738/1500 - Training Loss: 0.0092, Testing Loss: 0.0110\n",
            "Epoch: 739/1500 - Training Loss: 0.0088, Testing Loss: 0.0111\n",
            "Epoch: 740/1500 - Training Loss: 0.0091, Testing Loss: 0.0115\n",
            "Epoch: 741/1500 - Training Loss: 0.0089, Testing Loss: 0.0112\n",
            "Epoch: 742/1500 - Training Loss: 0.0090, Testing Loss: 0.0109\n",
            "Epoch: 743/1500 - Training Loss: 0.0088, Testing Loss: 0.0120\n",
            "Epoch: 744/1500 - Training Loss: 0.0089, Testing Loss: 0.0111\n",
            "Epoch: 745/1500 - Training Loss: 0.0088, Testing Loss: 0.0113\n",
            "Epoch: 746/1500 - Training Loss: 0.0087, Testing Loss: 0.0109\n",
            "Epoch: 747/1500 - Training Loss: 0.0090, Testing Loss: 0.0108\n",
            "Epoch: 748/1500 - Training Loss: 0.0087, Testing Loss: 0.0117\n",
            "Epoch: 749/1500 - Training Loss: 0.0088, Testing Loss: 0.0112\n",
            "Epoch: 750/1500 - Training Loss: 0.0091, Testing Loss: 0.0113\n",
            "Epoch: 751/1500 - Training Loss: 0.0088, Testing Loss: 0.0108\n",
            "Epoch: 752/1500 - Training Loss: 0.0088, Testing Loss: 0.0113\n",
            "Epoch: 753/1500 - Training Loss: 0.0087, Testing Loss: 0.0116\n",
            "Epoch: 754/1500 - Training Loss: 0.0089, Testing Loss: 0.0111\n",
            "Epoch: 755/1500 - Training Loss: 0.0087, Testing Loss: 0.0114\n",
            "Epoch: 756/1500 - Training Loss: 0.0089, Testing Loss: 0.0106\n",
            "Epoch: 757/1500 - Training Loss: 0.0087, Testing Loss: 0.0108\n",
            "Epoch: 758/1500 - Training Loss: 0.0088, Testing Loss: 0.0112\n",
            "Epoch: 759/1500 - Training Loss: 0.0087, Testing Loss: 0.0109\n",
            "Epoch: 760/1500 - Training Loss: 0.0089, Testing Loss: 0.0112\n",
            "Epoch: 761/1500 - Training Loss: 0.0089, Testing Loss: 0.0109\n",
            "Epoch: 762/1500 - Training Loss: 0.0087, Testing Loss: 0.0110\n",
            "Epoch: 763/1500 - Training Loss: 0.0089, Testing Loss: 0.0112\n",
            "Epoch: 764/1500 - Training Loss: 0.0089, Testing Loss: 0.0111\n",
            "Epoch: 765/1500 - Training Loss: 0.0088, Testing Loss: 0.0115\n",
            "Epoch: 766/1500 - Training Loss: 0.0088, Testing Loss: 0.0111\n",
            "Epoch: 767/1500 - Training Loss: 0.0091, Testing Loss: 0.0113\n",
            "Epoch: 768/1500 - Training Loss: 0.0088, Testing Loss: 0.0117\n",
            "Epoch: 769/1500 - Training Loss: 0.0087, Testing Loss: 0.0114\n",
            "Epoch: 770/1500 - Training Loss: 0.0087, Testing Loss: 0.0111\n",
            "Epoch: 771/1500 - Training Loss: 0.0088, Testing Loss: 0.0114\n",
            "Epoch: 772/1500 - Training Loss: 0.0088, Testing Loss: 0.0110\n",
            "Epoch: 773/1500 - Training Loss: 0.0086, Testing Loss: 0.0114\n",
            "Epoch: 774/1500 - Training Loss: 0.0088, Testing Loss: 0.0127\n",
            "Epoch: 775/1500 - Training Loss: 0.0089, Testing Loss: 0.0113\n",
            "Epoch: 776/1500 - Training Loss: 0.0089, Testing Loss: 0.0111\n",
            "Epoch: 777/1500 - Training Loss: 0.0088, Testing Loss: 0.0118\n",
            "Epoch: 778/1500 - Training Loss: 0.0088, Testing Loss: 0.0109\n",
            "Epoch: 779/1500 - Training Loss: 0.0087, Testing Loss: 0.0122\n",
            "Epoch: 780/1500 - Training Loss: 0.0088, Testing Loss: 0.0107\n",
            "Epoch: 781/1500 - Training Loss: 0.0088, Testing Loss: 0.0117\n",
            "Epoch: 782/1500 - Training Loss: 0.0088, Testing Loss: 0.0110\n",
            "Epoch: 783/1500 - Training Loss: 0.0087, Testing Loss: 0.0110\n",
            "Epoch: 784/1500 - Training Loss: 0.0088, Testing Loss: 0.0113\n",
            "Epoch: 785/1500 - Training Loss: 0.0088, Testing Loss: 0.0112\n",
            "Epoch: 786/1500 - Training Loss: 0.0088, Testing Loss: 0.0116\n",
            "Epoch: 787/1500 - Training Loss: 0.0088, Testing Loss: 0.0119\n",
            "Epoch: 788/1500 - Training Loss: 0.0088, Testing Loss: 0.0113\n",
            "Epoch: 789/1500 - Training Loss: 0.0089, Testing Loss: 0.0109\n",
            "Epoch: 790/1500 - Training Loss: 0.0089, Testing Loss: 0.0115\n",
            "Epoch: 791/1500 - Training Loss: 0.0090, Testing Loss: 0.0114\n",
            "Epoch: 792/1500 - Training Loss: 0.0089, Testing Loss: 0.0109\n",
            "Epoch: 793/1500 - Training Loss: 0.0088, Testing Loss: 0.0113\n",
            "Epoch: 794/1500 - Training Loss: 0.0087, Testing Loss: 0.0111\n",
            "Epoch: 795/1500 - Training Loss: 0.0086, Testing Loss: 0.0111\n",
            "Epoch: 796/1500 - Training Loss: 0.0087, Testing Loss: 0.0108\n",
            "Epoch: 797/1500 - Training Loss: 0.0087, Testing Loss: 0.0115\n",
            "Epoch: 798/1500 - Training Loss: 0.0089, Testing Loss: 0.0108\n",
            "Epoch: 799/1500 - Training Loss: 0.0086, Testing Loss: 0.0113\n",
            "Epoch: 800/1500 - Training Loss: 0.0089, Testing Loss: 0.0109\n",
            "Epoch: 801/1500 - Training Loss: 0.0087, Testing Loss: 0.0108\n",
            "Epoch: 802/1500 - Training Loss: 0.0089, Testing Loss: 0.0111\n",
            "Epoch: 803/1500 - Training Loss: 0.0087, Testing Loss: 0.0109\n",
            "Epoch: 804/1500 - Training Loss: 0.0086, Testing Loss: 0.0111\n",
            "Epoch: 805/1500 - Training Loss: 0.0088, Testing Loss: 0.0109\n",
            "Epoch: 806/1500 - Training Loss: 0.0087, Testing Loss: 0.0141\n",
            "Epoch: 807/1500 - Training Loss: 0.0086, Testing Loss: 0.0109\n",
            "Epoch: 808/1500 - Training Loss: 0.0088, Testing Loss: 0.0108\n",
            "Epoch: 809/1500 - Training Loss: 0.0087, Testing Loss: 0.0108\n",
            "Epoch: 810/1500 - Training Loss: 0.0087, Testing Loss: 0.0113\n",
            "Epoch: 811/1500 - Training Loss: 0.0086, Testing Loss: 0.0117\n",
            "Epoch: 812/1500 - Training Loss: 0.0087, Testing Loss: 0.0117\n",
            "Epoch: 813/1500 - Training Loss: 0.0087, Testing Loss: 0.0111\n",
            "Epoch: 814/1500 - Training Loss: 0.0086, Testing Loss: 0.0110\n",
            "Epoch: 815/1500 - Training Loss: 0.0087, Testing Loss: 0.0111\n",
            "Epoch: 816/1500 - Training Loss: 0.0086, Testing Loss: 0.0109\n",
            "Epoch: 817/1500 - Training Loss: 0.0088, Testing Loss: 0.0108\n",
            "Epoch: 818/1500 - Training Loss: 0.0087, Testing Loss: 0.0123\n",
            "Epoch: 819/1500 - Training Loss: 0.0089, Testing Loss: 0.0111\n",
            "Epoch: 820/1500 - Training Loss: 0.0087, Testing Loss: 0.0110\n",
            "Epoch: 821/1500 - Training Loss: 0.0086, Testing Loss: 0.0107\n",
            "Epoch: 822/1500 - Training Loss: 0.0086, Testing Loss: 0.0112\n",
            "Epoch: 823/1500 - Training Loss: 0.0088, Testing Loss: 0.0107\n",
            "Epoch: 824/1500 - Training Loss: 0.0087, Testing Loss: 0.0115\n",
            "Epoch: 825/1500 - Training Loss: 0.0086, Testing Loss: 0.0109\n",
            "Epoch: 826/1500 - Training Loss: 0.0087, Testing Loss: 0.0114\n",
            "Epoch: 827/1500 - Training Loss: 0.0087, Testing Loss: 0.0108\n",
            "Epoch: 828/1500 - Training Loss: 0.0088, Testing Loss: 0.0111\n",
            "Epoch: 829/1500 - Training Loss: 0.0087, Testing Loss: 0.0108\n",
            "Epoch: 830/1500 - Training Loss: 0.0085, Testing Loss: 0.0111\n",
            "Epoch: 831/1500 - Training Loss: 0.0086, Testing Loss: 0.0108\n",
            "Epoch: 832/1500 - Training Loss: 0.0086, Testing Loss: 0.0115\n",
            "Epoch: 833/1500 - Training Loss: 0.0088, Testing Loss: 0.0121\n",
            "Epoch: 834/1500 - Training Loss: 0.0087, Testing Loss: 0.0113\n",
            "Epoch: 835/1500 - Training Loss: 0.0088, Testing Loss: 0.0112\n",
            "Epoch: 836/1500 - Training Loss: 0.0085, Testing Loss: 0.0109\n",
            "Epoch: 837/1500 - Training Loss: 0.0085, Testing Loss: 0.0110\n",
            "Epoch: 838/1500 - Training Loss: 0.0088, Testing Loss: 0.0111\n",
            "Epoch: 839/1500 - Training Loss: 0.0086, Testing Loss: 0.0109\n",
            "Epoch: 840/1500 - Training Loss: 0.0086, Testing Loss: 0.0109\n",
            "Epoch: 841/1500 - Training Loss: 0.0087, Testing Loss: 0.0107\n",
            "Epoch: 842/1500 - Training Loss: 0.0087, Testing Loss: 0.0109\n",
            "Epoch: 843/1500 - Training Loss: 0.0087, Testing Loss: 0.0112\n",
            "Epoch: 844/1500 - Training Loss: 0.0086, Testing Loss: 0.0113\n",
            "Epoch: 845/1500 - Training Loss: 0.0087, Testing Loss: 0.0108\n",
            "Epoch: 846/1500 - Training Loss: 0.0086, Testing Loss: 0.0108\n",
            "Epoch: 847/1500 - Training Loss: 0.0087, Testing Loss: 0.0111\n",
            "Epoch: 848/1500 - Training Loss: 0.0086, Testing Loss: 0.0117\n",
            "Epoch: 849/1500 - Training Loss: 0.0085, Testing Loss: 0.0112\n",
            "Epoch: 850/1500 - Training Loss: 0.0085, Testing Loss: 0.0110\n",
            "Epoch: 851/1500 - Training Loss: 0.0088, Testing Loss: 0.0110\n",
            "Epoch: 852/1500 - Training Loss: 0.0087, Testing Loss: 0.0107\n",
            "Epoch: 853/1500 - Training Loss: 0.0087, Testing Loss: 0.0122\n",
            "Epoch: 854/1500 - Training Loss: 0.0086, Testing Loss: 0.0122\n",
            "Epoch: 855/1500 - Training Loss: 0.0087, Testing Loss: 0.0108\n",
            "Epoch: 856/1500 - Training Loss: 0.0086, Testing Loss: 0.0110\n",
            "Epoch: 857/1500 - Training Loss: 0.0086, Testing Loss: 0.0107\n",
            "Epoch: 858/1500 - Training Loss: 0.0086, Testing Loss: 0.0128\n",
            "Epoch: 859/1500 - Training Loss: 0.0086, Testing Loss: 0.0119\n",
            "Epoch: 860/1500 - Training Loss: 0.0087, Testing Loss: 0.0108\n",
            "Epoch: 861/1500 - Training Loss: 0.0087, Testing Loss: 0.0108\n",
            "Epoch: 862/1500 - Training Loss: 0.0085, Testing Loss: 0.0114\n",
            "Epoch: 863/1500 - Training Loss: 0.0086, Testing Loss: 0.0107\n",
            "Epoch: 864/1500 - Training Loss: 0.0087, Testing Loss: 0.0111\n",
            "Epoch: 865/1500 - Training Loss: 0.0087, Testing Loss: 0.0109\n",
            "Epoch: 866/1500 - Training Loss: 0.0085, Testing Loss: 0.0108\n",
            "Epoch: 867/1500 - Training Loss: 0.0086, Testing Loss: 0.0110\n",
            "Epoch: 868/1500 - Training Loss: 0.0086, Testing Loss: 0.0110\n",
            "Epoch: 869/1500 - Training Loss: 0.0085, Testing Loss: 0.0106\n",
            "Epoch: 870/1500 - Training Loss: 0.0088, Testing Loss: 0.0115\n",
            "Epoch: 871/1500 - Training Loss: 0.0086, Testing Loss: 0.0110\n",
            "Epoch: 872/1500 - Training Loss: 0.0086, Testing Loss: 0.0116\n",
            "Epoch: 873/1500 - Training Loss: 0.0085, Testing Loss: 0.0114\n",
            "Epoch: 874/1500 - Training Loss: 0.0085, Testing Loss: 0.0109\n",
            "Epoch: 875/1500 - Training Loss: 0.0086, Testing Loss: 0.0109\n",
            "Epoch: 876/1500 - Training Loss: 0.0086, Testing Loss: 0.0110\n",
            "Epoch: 877/1500 - Training Loss: 0.0086, Testing Loss: 0.0112\n",
            "Epoch: 878/1500 - Training Loss: 0.0087, Testing Loss: 0.0114\n",
            "Epoch: 879/1500 - Training Loss: 0.0086, Testing Loss: 0.0109\n",
            "Epoch: 880/1500 - Training Loss: 0.0086, Testing Loss: 0.0111\n",
            "Epoch: 881/1500 - Training Loss: 0.0085, Testing Loss: 0.0111\n",
            "Epoch: 882/1500 - Training Loss: 0.0086, Testing Loss: 0.0108\n",
            "Epoch: 883/1500 - Training Loss: 0.0085, Testing Loss: 0.0110\n",
            "Epoch: 884/1500 - Training Loss: 0.0084, Testing Loss: 0.0109\n",
            "Epoch: 885/1500 - Training Loss: 0.0085, Testing Loss: 0.0109\n",
            "Epoch: 886/1500 - Training Loss: 0.0087, Testing Loss: 0.0129\n",
            "Epoch: 887/1500 - Training Loss: 0.0088, Testing Loss: 0.0108\n",
            "Epoch: 888/1500 - Training Loss: 0.0086, Testing Loss: 0.0109\n",
            "Epoch: 889/1500 - Training Loss: 0.0084, Testing Loss: 0.0107\n",
            "Epoch: 890/1500 - Training Loss: 0.0086, Testing Loss: 0.0110\n",
            "Epoch: 891/1500 - Training Loss: 0.0086, Testing Loss: 0.0111\n",
            "Epoch: 892/1500 - Training Loss: 0.0085, Testing Loss: 0.0110\n",
            "Epoch: 893/1500 - Training Loss: 0.0086, Testing Loss: 0.0116\n",
            "Epoch: 894/1500 - Training Loss: 0.0086, Testing Loss: 0.0107\n",
            "Epoch: 895/1500 - Training Loss: 0.0086, Testing Loss: 0.0110\n",
            "Epoch: 896/1500 - Training Loss: 0.0085, Testing Loss: 0.0112\n",
            "Epoch: 897/1500 - Training Loss: 0.0086, Testing Loss: 0.0112\n",
            "Epoch: 898/1500 - Training Loss: 0.0084, Testing Loss: 0.0107\n",
            "Epoch: 899/1500 - Training Loss: 0.0086, Testing Loss: 0.0111\n",
            "Epoch: 900/1500 - Training Loss: 0.0085, Testing Loss: 0.0112\n",
            "Epoch: 901/1500 - Training Loss: 0.0084, Testing Loss: 0.0109\n",
            "Epoch: 902/1500 - Training Loss: 0.0086, Testing Loss: 0.0112\n",
            "Epoch: 903/1500 - Training Loss: 0.0084, Testing Loss: 0.0109\n",
            "Epoch: 904/1500 - Training Loss: 0.0085, Testing Loss: 0.0109\n",
            "Epoch: 905/1500 - Training Loss: 0.0086, Testing Loss: 0.0123\n",
            "Epoch: 906/1500 - Training Loss: 0.0087, Testing Loss: 0.0114\n",
            "Epoch: 907/1500 - Training Loss: 0.0085, Testing Loss: 0.0112\n",
            "Epoch: 908/1500 - Training Loss: 0.0085, Testing Loss: 0.0115\n",
            "Epoch: 909/1500 - Training Loss: 0.0085, Testing Loss: 0.0110\n",
            "Epoch: 910/1500 - Training Loss: 0.0085, Testing Loss: 0.0107\n",
            "Epoch: 911/1500 - Training Loss: 0.0086, Testing Loss: 0.0109\n",
            "Epoch: 912/1500 - Training Loss: 0.0084, Testing Loss: 0.0113\n",
            "Epoch: 913/1500 - Training Loss: 0.0087, Testing Loss: 0.0115\n",
            "Epoch: 914/1500 - Training Loss: 0.0085, Testing Loss: 0.0113\n",
            "Epoch: 915/1500 - Training Loss: 0.0087, Testing Loss: 0.0110\n",
            "Epoch: 916/1500 - Training Loss: 0.0087, Testing Loss: 0.0108\n",
            "Epoch: 917/1500 - Training Loss: 0.0089, Testing Loss: 0.0110\n",
            "Epoch: 918/1500 - Training Loss: 0.0084, Testing Loss: 0.0115\n",
            "Epoch: 919/1500 - Training Loss: 0.0085, Testing Loss: 0.0110\n",
            "Epoch: 920/1500 - Training Loss: 0.0084, Testing Loss: 0.0110\n",
            "Epoch: 921/1500 - Training Loss: 0.0085, Testing Loss: 0.0107\n",
            "Epoch: 922/1500 - Training Loss: 0.0084, Testing Loss: 0.0107\n",
            "Epoch: 923/1500 - Training Loss: 0.0086, Testing Loss: 0.0108\n",
            "Epoch: 924/1500 - Training Loss: 0.0084, Testing Loss: 0.0108\n",
            "Epoch: 925/1500 - Training Loss: 0.0084, Testing Loss: 0.0108\n",
            "Epoch: 926/1500 - Training Loss: 0.0083, Testing Loss: 0.0110\n",
            "Epoch: 927/1500 - Training Loss: 0.0085, Testing Loss: 0.0110\n",
            "Epoch: 928/1500 - Training Loss: 0.0084, Testing Loss: 0.0110\n",
            "Epoch: 929/1500 - Training Loss: 0.0085, Testing Loss: 0.0112\n",
            "Epoch: 930/1500 - Training Loss: 0.0084, Testing Loss: 0.0110\n",
            "Epoch: 931/1500 - Training Loss: 0.0085, Testing Loss: 0.0110\n",
            "Epoch: 932/1500 - Training Loss: 0.0088, Testing Loss: 0.0107\n",
            "Epoch: 933/1500 - Training Loss: 0.0084, Testing Loss: 0.0108\n",
            "Epoch: 934/1500 - Training Loss: 0.0084, Testing Loss: 0.0107\n",
            "Epoch: 935/1500 - Training Loss: 0.0085, Testing Loss: 0.0113\n",
            "Epoch: 936/1500 - Training Loss: 0.0084, Testing Loss: 0.0109\n",
            "Epoch: 937/1500 - Training Loss: 0.0082, Testing Loss: 0.0107\n",
            "Epoch: 938/1500 - Training Loss: 0.0084, Testing Loss: 0.0110\n",
            "Epoch: 939/1500 - Training Loss: 0.0088, Testing Loss: 0.0121\n",
            "Epoch: 940/1500 - Training Loss: 0.0084, Testing Loss: 0.0116\n",
            "Epoch: 941/1500 - Training Loss: 0.0086, Testing Loss: 0.0109\n",
            "Epoch: 942/1500 - Training Loss: 0.0084, Testing Loss: 0.0110\n",
            "Epoch: 943/1500 - Training Loss: 0.0084, Testing Loss: 0.0108\n",
            "Epoch: 944/1500 - Training Loss: 0.0085, Testing Loss: 0.0107\n",
            "Epoch: 945/1500 - Training Loss: 0.0084, Testing Loss: 0.0109\n",
            "Epoch: 946/1500 - Training Loss: 0.0083, Testing Loss: 0.0108\n",
            "Epoch: 947/1500 - Training Loss: 0.0084, Testing Loss: 0.0120\n",
            "Epoch: 948/1500 - Training Loss: 0.0084, Testing Loss: 0.0109\n",
            "Epoch: 949/1500 - Training Loss: 0.0082, Testing Loss: 0.0108\n",
            "Epoch: 950/1500 - Training Loss: 0.0088, Testing Loss: 0.0112\n",
            "Epoch: 951/1500 - Training Loss: 0.0087, Testing Loss: 0.0111\n",
            "Epoch: 952/1500 - Training Loss: 0.0084, Testing Loss: 0.0111\n",
            "Epoch: 953/1500 - Training Loss: 0.0084, Testing Loss: 0.0107\n",
            "Epoch: 954/1500 - Training Loss: 0.0084, Testing Loss: 0.0108\n",
            "Epoch: 955/1500 - Training Loss: 0.0083, Testing Loss: 0.0114\n",
            "Epoch: 956/1500 - Training Loss: 0.0083, Testing Loss: 0.0107\n",
            "Epoch: 957/1500 - Training Loss: 0.0082, Testing Loss: 0.0112\n",
            "Epoch: 958/1500 - Training Loss: 0.0084, Testing Loss: 0.0112\n",
            "Epoch: 959/1500 - Training Loss: 0.0084, Testing Loss: 0.0120\n",
            "Epoch: 960/1500 - Training Loss: 0.0083, Testing Loss: 0.0114\n",
            "Epoch: 961/1500 - Training Loss: 0.0084, Testing Loss: 0.0119\n",
            "Epoch: 962/1500 - Training Loss: 0.0086, Testing Loss: 0.0112\n",
            "Epoch: 963/1500 - Training Loss: 0.0084, Testing Loss: 0.0108\n",
            "Epoch: 964/1500 - Training Loss: 0.0084, Testing Loss: 0.0110\n",
            "Epoch: 965/1500 - Training Loss: 0.0083, Testing Loss: 0.0113\n",
            "Epoch: 966/1500 - Training Loss: 0.0083, Testing Loss: 0.0105\n",
            "Epoch: 967/1500 - Training Loss: 0.0083, Testing Loss: 0.0112\n",
            "Epoch: 968/1500 - Training Loss: 0.0083, Testing Loss: 0.0109\n",
            "Epoch: 969/1500 - Training Loss: 0.0084, Testing Loss: 0.0108\n",
            "Epoch: 970/1500 - Training Loss: 0.0084, Testing Loss: 0.0110\n",
            "Epoch: 971/1500 - Training Loss: 0.0085, Testing Loss: 0.0108\n",
            "Epoch: 972/1500 - Training Loss: 0.0084, Testing Loss: 0.0111\n",
            "Epoch: 973/1500 - Training Loss: 0.0084, Testing Loss: 0.0111\n",
            "Epoch: 974/1500 - Training Loss: 0.0084, Testing Loss: 0.0111\n",
            "Epoch: 975/1500 - Training Loss: 0.0082, Testing Loss: 0.0106\n",
            "Epoch: 976/1500 - Training Loss: 0.0083, Testing Loss: 0.0111\n",
            "Epoch: 977/1500 - Training Loss: 0.0085, Testing Loss: 0.0112\n",
            "Epoch: 978/1500 - Training Loss: 0.0085, Testing Loss: 0.0111\n",
            "Epoch: 979/1500 - Training Loss: 0.0085, Testing Loss: 0.0106\n",
            "Epoch: 980/1500 - Training Loss: 0.0084, Testing Loss: 0.0110\n",
            "Epoch: 981/1500 - Training Loss: 0.0083, Testing Loss: 0.0117\n",
            "Epoch: 982/1500 - Training Loss: 0.0083, Testing Loss: 0.0108\n",
            "Epoch: 983/1500 - Training Loss: 0.0082, Testing Loss: 0.0109\n",
            "Epoch: 984/1500 - Training Loss: 0.0083, Testing Loss: 0.0110\n",
            "Epoch: 985/1500 - Training Loss: 0.0083, Testing Loss: 0.0109\n",
            "Epoch: 986/1500 - Training Loss: 0.0084, Testing Loss: 0.0110\n",
            "Epoch: 987/1500 - Training Loss: 0.0083, Testing Loss: 0.0112\n",
            "Epoch: 988/1500 - Training Loss: 0.0084, Testing Loss: 0.0108\n",
            "Epoch: 989/1500 - Training Loss: 0.0084, Testing Loss: 0.0113\n",
            "Epoch: 990/1500 - Training Loss: 0.0085, Testing Loss: 0.0111\n",
            "Epoch: 991/1500 - Training Loss: 0.0084, Testing Loss: 0.0112\n",
            "Epoch: 992/1500 - Training Loss: 0.0082, Testing Loss: 0.0112\n",
            "Epoch: 993/1500 - Training Loss: 0.0086, Testing Loss: 0.0110\n",
            "Epoch: 994/1500 - Training Loss: 0.0084, Testing Loss: 0.0110\n",
            "Epoch: 995/1500 - Training Loss: 0.0083, Testing Loss: 0.0122\n",
            "Epoch: 996/1500 - Training Loss: 0.0084, Testing Loss: 0.0110\n",
            "Epoch: 997/1500 - Training Loss: 0.0083, Testing Loss: 0.0110\n",
            "Epoch: 998/1500 - Training Loss: 0.0082, Testing Loss: 0.0114\n",
            "Epoch: 999/1500 - Training Loss: 0.0085, Testing Loss: 0.0109\n",
            "Epoch: 1000/1500 - Training Loss: 0.0083, Testing Loss: 0.0113\n",
            "Epoch: 1001/1500 - Training Loss: 0.0081, Testing Loss: 0.0110\n",
            "Epoch: 1002/1500 - Training Loss: 0.0086, Testing Loss: 0.0109\n",
            "Epoch: 1003/1500 - Training Loss: 0.0083, Testing Loss: 0.0107\n",
            "Epoch: 1004/1500 - Training Loss: 0.0083, Testing Loss: 0.0113\n",
            "Epoch: 1005/1500 - Training Loss: 0.0083, Testing Loss: 0.0109\n",
            "Epoch: 1006/1500 - Training Loss: 0.0084, Testing Loss: 0.0112\n",
            "Epoch: 1007/1500 - Training Loss: 0.0083, Testing Loss: 0.0112\n",
            "Epoch: 1008/1500 - Training Loss: 0.0083, Testing Loss: 0.0105\n",
            "Epoch: 1009/1500 - Training Loss: 0.0084, Testing Loss: 0.0112\n",
            "Epoch: 1010/1500 - Training Loss: 0.0082, Testing Loss: 0.0108\n",
            "Epoch: 1011/1500 - Training Loss: 0.0082, Testing Loss: 0.0110\n",
            "Epoch: 1012/1500 - Training Loss: 0.0083, Testing Loss: 0.0114\n",
            "Epoch: 1013/1500 - Training Loss: 0.0083, Testing Loss: 0.0110\n",
            "Epoch: 1014/1500 - Training Loss: 0.0083, Testing Loss: 0.0114\n",
            "Epoch: 1015/1500 - Training Loss: 0.0084, Testing Loss: 0.0109\n",
            "Epoch: 1016/1500 - Training Loss: 0.0083, Testing Loss: 0.0110\n",
            "Epoch: 1017/1500 - Training Loss: 0.0085, Testing Loss: 0.0111\n",
            "Epoch: 1018/1500 - Training Loss: 0.0082, Testing Loss: 0.0108\n",
            "Epoch: 1019/1500 - Training Loss: 0.0082, Testing Loss: 0.0117\n",
            "Epoch: 1020/1500 - Training Loss: 0.0084, Testing Loss: 0.0117\n",
            "Epoch: 1021/1500 - Training Loss: 0.0082, Testing Loss: 0.0119\n",
            "Epoch: 1022/1500 - Training Loss: 0.0082, Testing Loss: 0.0114\n",
            "Epoch: 1023/1500 - Training Loss: 0.0084, Testing Loss: 0.0111\n",
            "Epoch: 1024/1500 - Training Loss: 0.0080, Testing Loss: 0.0109\n",
            "Epoch: 1025/1500 - Training Loss: 0.0083, Testing Loss: 0.0112\n",
            "Epoch: 1026/1500 - Training Loss: 0.0083, Testing Loss: 0.0109\n",
            "Epoch: 1027/1500 - Training Loss: 0.0082, Testing Loss: 0.0111\n",
            "Epoch: 1028/1500 - Training Loss: 0.0083, Testing Loss: 0.0110\n",
            "Epoch: 1029/1500 - Training Loss: 0.0083, Testing Loss: 0.0109\n",
            "Epoch: 1030/1500 - Training Loss: 0.0082, Testing Loss: 0.0110\n",
            "Epoch: 1031/1500 - Training Loss: 0.0082, Testing Loss: 0.0111\n",
            "Epoch: 1032/1500 - Training Loss: 0.0083, Testing Loss: 0.0109\n",
            "Epoch: 1033/1500 - Training Loss: 0.0082, Testing Loss: 0.0110\n",
            "Epoch: 1034/1500 - Training Loss: 0.0085, Testing Loss: 0.0110\n",
            "Epoch: 1035/1500 - Training Loss: 0.0082, Testing Loss: 0.0107\n",
            "Epoch: 1036/1500 - Training Loss: 0.0084, Testing Loss: 0.0107\n",
            "Epoch: 1037/1500 - Training Loss: 0.0083, Testing Loss: 0.0109\n",
            "Epoch: 1038/1500 - Training Loss: 0.0083, Testing Loss: 0.0110\n",
            "Epoch: 1039/1500 - Training Loss: 0.0080, Testing Loss: 0.0112\n",
            "Epoch: 1040/1500 - Training Loss: 0.0082, Testing Loss: 0.0109\n",
            "Epoch: 1041/1500 - Training Loss: 0.0082, Testing Loss: 0.0114\n",
            "Epoch: 1042/1500 - Training Loss: 0.0083, Testing Loss: 0.0108\n",
            "Epoch: 1043/1500 - Training Loss: 0.0081, Testing Loss: 0.0109\n",
            "Epoch: 1044/1500 - Training Loss: 0.0082, Testing Loss: 0.0106\n",
            "Epoch: 1045/1500 - Training Loss: 0.0082, Testing Loss: 0.0107\n",
            "Epoch: 1046/1500 - Training Loss: 0.0082, Testing Loss: 0.0109\n",
            "Epoch: 1047/1500 - Training Loss: 0.0083, Testing Loss: 0.0109\n",
            "Epoch: 1048/1500 - Training Loss: 0.0082, Testing Loss: 0.0122\n",
            "Epoch: 1049/1500 - Training Loss: 0.0083, Testing Loss: 0.0111\n",
            "Epoch: 1050/1500 - Training Loss: 0.0084, Testing Loss: 0.0108\n",
            "Epoch: 1051/1500 - Training Loss: 0.0084, Testing Loss: 0.0109\n",
            "Epoch: 1052/1500 - Training Loss: 0.0081, Testing Loss: 0.0110\n",
            "Epoch: 1053/1500 - Training Loss: 0.0082, Testing Loss: 0.0108\n",
            "Epoch: 1054/1500 - Training Loss: 0.0082, Testing Loss: 0.0111\n",
            "Epoch: 1055/1500 - Training Loss: 0.0082, Testing Loss: 0.0108\n",
            "Epoch: 1056/1500 - Training Loss: 0.0081, Testing Loss: 0.0115\n",
            "Epoch: 1057/1500 - Training Loss: 0.0083, Testing Loss: 0.0111\n",
            "Epoch: 1058/1500 - Training Loss: 0.0086, Testing Loss: 0.0111\n",
            "Epoch: 1059/1500 - Training Loss: 0.0082, Testing Loss: 0.0109\n",
            "Epoch: 1060/1500 - Training Loss: 0.0081, Testing Loss: 0.0113\n",
            "Epoch: 1061/1500 - Training Loss: 0.0082, Testing Loss: 0.0111\n",
            "Epoch: 1062/1500 - Training Loss: 0.0082, Testing Loss: 0.0114\n",
            "Epoch: 1063/1500 - Training Loss: 0.0082, Testing Loss: 0.0107\n",
            "Epoch: 1064/1500 - Training Loss: 0.0083, Testing Loss: 0.0115\n",
            "Epoch: 1065/1500 - Training Loss: 0.0082, Testing Loss: 0.0108\n",
            "Epoch: 1066/1500 - Training Loss: 0.0080, Testing Loss: 0.0110\n",
            "Epoch: 1067/1500 - Training Loss: 0.0082, Testing Loss: 0.0107\n",
            "Epoch: 1068/1500 - Training Loss: 0.0082, Testing Loss: 0.0109\n",
            "Epoch: 1069/1500 - Training Loss: 0.0083, Testing Loss: 0.0110\n",
            "Epoch: 1070/1500 - Training Loss: 0.0083, Testing Loss: 0.0110\n",
            "Epoch: 1071/1500 - Training Loss: 0.0081, Testing Loss: 0.0110\n",
            "Epoch: 1072/1500 - Training Loss: 0.0082, Testing Loss: 0.0106\n",
            "Epoch: 1073/1500 - Training Loss: 0.0081, Testing Loss: 0.0111\n",
            "Epoch: 1074/1500 - Training Loss: 0.0082, Testing Loss: 0.0112\n",
            "Epoch: 1075/1500 - Training Loss: 0.0083, Testing Loss: 0.0108\n",
            "Epoch: 1076/1500 - Training Loss: 0.0081, Testing Loss: 0.0109\n",
            "Epoch: 1077/1500 - Training Loss: 0.0081, Testing Loss: 0.0110\n",
            "Epoch: 1078/1500 - Training Loss: 0.0082, Testing Loss: 0.0107\n",
            "Epoch: 1079/1500 - Training Loss: 0.0086, Testing Loss: 0.0108\n",
            "Epoch: 1080/1500 - Training Loss: 0.0082, Testing Loss: 0.0108\n",
            "Epoch: 1081/1500 - Training Loss: 0.0081, Testing Loss: 0.0112\n",
            "Epoch: 1082/1500 - Training Loss: 0.0082, Testing Loss: 0.0108\n",
            "Epoch: 1083/1500 - Training Loss: 0.0081, Testing Loss: 0.0114\n",
            "Epoch: 1084/1500 - Training Loss: 0.0082, Testing Loss: 0.0107\n",
            "Epoch: 1085/1500 - Training Loss: 0.0083, Testing Loss: 0.0110\n",
            "Epoch: 1086/1500 - Training Loss: 0.0081, Testing Loss: 0.0107\n",
            "Epoch: 1087/1500 - Training Loss: 0.0081, Testing Loss: 0.0120\n",
            "Epoch: 1088/1500 - Training Loss: 0.0083, Testing Loss: 0.0112\n",
            "Epoch: 1089/1500 - Training Loss: 0.0082, Testing Loss: 0.0112\n",
            "Epoch: 1090/1500 - Training Loss: 0.0082, Testing Loss: 0.0105\n",
            "Epoch: 1091/1500 - Training Loss: 0.0083, Testing Loss: 0.0110\n",
            "Epoch: 1092/1500 - Training Loss: 0.0079, Testing Loss: 0.0114\n",
            "Epoch: 1093/1500 - Training Loss: 0.0080, Testing Loss: 0.0117\n",
            "Epoch: 1094/1500 - Training Loss: 0.0080, Testing Loss: 0.0114\n",
            "Epoch: 1095/1500 - Training Loss: 0.0082, Testing Loss: 0.0113\n",
            "Epoch: 1096/1500 - Training Loss: 0.0081, Testing Loss: 0.0109\n",
            "Epoch: 1097/1500 - Training Loss: 0.0081, Testing Loss: 0.0112\n",
            "Epoch: 1098/1500 - Training Loss: 0.0081, Testing Loss: 0.0114\n",
            "Epoch: 1099/1500 - Training Loss: 0.0081, Testing Loss: 0.0111\n",
            "Epoch: 1100/1500 - Training Loss: 0.0081, Testing Loss: 0.0113\n",
            "Epoch: 1101/1500 - Training Loss: 0.0082, Testing Loss: 0.0111\n",
            "Epoch: 1102/1500 - Training Loss: 0.0081, Testing Loss: 0.0110\n",
            "Epoch: 1103/1500 - Training Loss: 0.0083, Testing Loss: 0.0112\n",
            "Epoch: 1104/1500 - Training Loss: 0.0082, Testing Loss: 0.0110\n",
            "Epoch: 1105/1500 - Training Loss: 0.0081, Testing Loss: 0.0108\n",
            "Epoch: 1106/1500 - Training Loss: 0.0085, Testing Loss: 0.0116\n",
            "Epoch: 1107/1500 - Training Loss: 0.0081, Testing Loss: 0.0109\n",
            "Epoch: 1108/1500 - Training Loss: 0.0080, Testing Loss: 0.0111\n",
            "Epoch: 1109/1500 - Training Loss: 0.0081, Testing Loss: 0.0108\n",
            "Epoch: 1110/1500 - Training Loss: 0.0081, Testing Loss: 0.0110\n",
            "Epoch: 1111/1500 - Training Loss: 0.0082, Testing Loss: 0.0112\n",
            "Epoch: 1112/1500 - Training Loss: 0.0081, Testing Loss: 0.0107\n",
            "Epoch: 1113/1500 - Training Loss: 0.0084, Testing Loss: 0.0109\n",
            "Epoch: 1114/1500 - Training Loss: 0.0080, Testing Loss: 0.0112\n",
            "Epoch: 1115/1500 - Training Loss: 0.0082, Testing Loss: 0.0120\n",
            "Epoch: 1116/1500 - Training Loss: 0.0083, Testing Loss: 0.0108\n",
            "Epoch: 1117/1500 - Training Loss: 0.0081, Testing Loss: 0.0111\n",
            "Epoch: 1118/1500 - Training Loss: 0.0081, Testing Loss: 0.0110\n",
            "Epoch: 1119/1500 - Training Loss: 0.0080, Testing Loss: 0.0110\n",
            "Epoch: 1120/1500 - Training Loss: 0.0082, Testing Loss: 0.0109\n",
            "Epoch: 1121/1500 - Training Loss: 0.0080, Testing Loss: 0.0109\n",
            "Epoch: 1122/1500 - Training Loss: 0.0080, Testing Loss: 0.0110\n",
            "Epoch: 1123/1500 - Training Loss: 0.0082, Testing Loss: 0.0109\n",
            "Epoch: 1124/1500 - Training Loss: 0.0081, Testing Loss: 0.0113\n",
            "Epoch: 1125/1500 - Training Loss: 0.0081, Testing Loss: 0.0107\n",
            "Epoch: 1126/1500 - Training Loss: 0.0080, Testing Loss: 0.0112\n",
            "Epoch: 1127/1500 - Training Loss: 0.0081, Testing Loss: 0.0107\n",
            "Epoch: 1128/1500 - Training Loss: 0.0084, Testing Loss: 0.0115\n",
            "Epoch: 1129/1500 - Training Loss: 0.0081, Testing Loss: 0.0112\n",
            "Epoch: 1130/1500 - Training Loss: 0.0081, Testing Loss: 0.0108\n",
            "Epoch: 1131/1500 - Training Loss: 0.0083, Testing Loss: 0.0114\n",
            "Epoch: 1132/1500 - Training Loss: 0.0081, Testing Loss: 0.0112\n",
            "Epoch: 1133/1500 - Training Loss: 0.0082, Testing Loss: 0.0113\n",
            "Epoch: 1134/1500 - Training Loss: 0.0080, Testing Loss: 0.0108\n",
            "Epoch: 1135/1500 - Training Loss: 0.0080, Testing Loss: 0.0108\n",
            "Epoch: 1136/1500 - Training Loss: 0.0080, Testing Loss: 0.0112\n",
            "Epoch: 1137/1500 - Training Loss: 0.0080, Testing Loss: 0.0116\n",
            "Epoch: 1138/1500 - Training Loss: 0.0080, Testing Loss: 0.0111\n",
            "Epoch: 1139/1500 - Training Loss: 0.0080, Testing Loss: 0.0111\n",
            "Epoch: 1140/1500 - Training Loss: 0.0080, Testing Loss: 0.0114\n",
            "Epoch: 1141/1500 - Training Loss: 0.0085, Testing Loss: 0.0114\n",
            "Epoch: 1142/1500 - Training Loss: 0.0080, Testing Loss: 0.0110\n",
            "Epoch: 1143/1500 - Training Loss: 0.0081, Testing Loss: 0.0113\n",
            "Epoch: 1144/1500 - Training Loss: 0.0081, Testing Loss: 0.0111\n",
            "Epoch: 1145/1500 - Training Loss: 0.0081, Testing Loss: 0.0111\n",
            "Epoch: 1146/1500 - Training Loss: 0.0081, Testing Loss: 0.0116\n",
            "Epoch: 1147/1500 - Training Loss: 0.0080, Testing Loss: 0.0117\n",
            "Epoch: 1148/1500 - Training Loss: 0.0082, Testing Loss: 0.0110\n",
            "Epoch: 1149/1500 - Training Loss: 0.0082, Testing Loss: 0.0110\n",
            "Epoch: 1150/1500 - Training Loss: 0.0083, Testing Loss: 0.0108\n",
            "Epoch: 1151/1500 - Training Loss: 0.0081, Testing Loss: 0.0109\n",
            "Epoch: 1152/1500 - Training Loss: 0.0080, Testing Loss: 0.0108\n",
            "Epoch: 1153/1500 - Training Loss: 0.0080, Testing Loss: 0.0109\n",
            "Epoch: 1154/1500 - Training Loss: 0.0080, Testing Loss: 0.0111\n",
            "Epoch: 1155/1500 - Training Loss: 0.0082, Testing Loss: 0.0114\n",
            "Epoch: 1156/1500 - Training Loss: 0.0080, Testing Loss: 0.0115\n",
            "Epoch: 1157/1500 - Training Loss: 0.0080, Testing Loss: 0.0112\n",
            "Epoch: 1158/1500 - Training Loss: 0.0081, Testing Loss: 0.0111\n",
            "Epoch: 1159/1500 - Training Loss: 0.0079, Testing Loss: 0.0116\n",
            "Epoch: 1160/1500 - Training Loss: 0.0080, Testing Loss: 0.0110\n",
            "Epoch: 1161/1500 - Training Loss: 0.0080, Testing Loss: 0.0110\n",
            "Epoch: 1162/1500 - Training Loss: 0.0080, Testing Loss: 0.0112\n",
            "Epoch: 1163/1500 - Training Loss: 0.0080, Testing Loss: 0.0117\n",
            "Epoch: 1164/1500 - Training Loss: 0.0082, Testing Loss: 0.0113\n",
            "Epoch: 1165/1500 - Training Loss: 0.0081, Testing Loss: 0.0112\n",
            "Epoch: 1166/1500 - Training Loss: 0.0080, Testing Loss: 0.0110\n",
            "Epoch: 1167/1500 - Training Loss: 0.0083, Testing Loss: 0.0115\n",
            "Epoch: 1168/1500 - Training Loss: 0.0082, Testing Loss: 0.0111\n",
            "Epoch: 1169/1500 - Training Loss: 0.0081, Testing Loss: 0.0111\n",
            "Epoch: 1170/1500 - Training Loss: 0.0080, Testing Loss: 0.0119\n",
            "Epoch: 1171/1500 - Training Loss: 0.0079, Testing Loss: 0.0113\n",
            "Epoch: 1172/1500 - Training Loss: 0.0081, Testing Loss: 0.0111\n",
            "Epoch: 1173/1500 - Training Loss: 0.0080, Testing Loss: 0.0113\n",
            "Epoch: 1174/1500 - Training Loss: 0.0082, Testing Loss: 0.0109\n",
            "Epoch: 1175/1500 - Training Loss: 0.0080, Testing Loss: 0.0107\n",
            "Epoch: 1176/1500 - Training Loss: 0.0079, Testing Loss: 0.0109\n",
            "Epoch: 1177/1500 - Training Loss: 0.0080, Testing Loss: 0.0116\n",
            "Epoch: 1178/1500 - Training Loss: 0.0080, Testing Loss: 0.0110\n",
            "Epoch: 1179/1500 - Training Loss: 0.0080, Testing Loss: 0.0109\n",
            "Epoch: 1180/1500 - Training Loss: 0.0079, Testing Loss: 0.0114\n",
            "Epoch: 1181/1500 - Training Loss: 0.0080, Testing Loss: 0.0110\n",
            "Epoch: 1182/1500 - Training Loss: 0.0079, Testing Loss: 0.0113\n",
            "Epoch: 1183/1500 - Training Loss: 0.0081, Testing Loss: 0.0117\n",
            "Epoch: 1184/1500 - Training Loss: 0.0079, Testing Loss: 0.0109\n",
            "Epoch: 1185/1500 - Training Loss: 0.0080, Testing Loss: 0.0110\n",
            "Epoch: 1186/1500 - Training Loss: 0.0080, Testing Loss: 0.0109\n",
            "Epoch: 1187/1500 - Training Loss: 0.0079, Testing Loss: 0.0111\n",
            "Epoch: 1188/1500 - Training Loss: 0.0080, Testing Loss: 0.0110\n",
            "Epoch: 1189/1500 - Training Loss: 0.0081, Testing Loss: 0.0113\n",
            "Epoch: 1190/1500 - Training Loss: 0.0081, Testing Loss: 0.0111\n",
            "Epoch: 1191/1500 - Training Loss: 0.0081, Testing Loss: 0.0111\n",
            "Epoch: 1192/1500 - Training Loss: 0.0080, Testing Loss: 0.0112\n",
            "Epoch: 1193/1500 - Training Loss: 0.0083, Testing Loss: 0.0122\n",
            "Epoch: 1194/1500 - Training Loss: 0.0081, Testing Loss: 0.0114\n",
            "Epoch: 1195/1500 - Training Loss: 0.0081, Testing Loss: 0.0110\n",
            "Epoch: 1196/1500 - Training Loss: 0.0080, Testing Loss: 0.0108\n",
            "Epoch: 1197/1500 - Training Loss: 0.0080, Testing Loss: 0.0115\n",
            "Epoch: 1198/1500 - Training Loss: 0.0080, Testing Loss: 0.0114\n",
            "Epoch: 1199/1500 - Training Loss: 0.0079, Testing Loss: 0.0110\n",
            "Epoch: 1200/1500 - Training Loss: 0.0083, Testing Loss: 0.0115\n",
            "Epoch: 1201/1500 - Training Loss: 0.0078, Testing Loss: 0.0110\n",
            "Epoch: 1202/1500 - Training Loss: 0.0079, Testing Loss: 0.0110\n",
            "Epoch: 1203/1500 - Training Loss: 0.0081, Testing Loss: 0.0117\n",
            "Epoch: 1204/1500 - Training Loss: 0.0080, Testing Loss: 0.0113\n",
            "Epoch: 1205/1500 - Training Loss: 0.0080, Testing Loss: 0.0110\n",
            "Epoch: 1206/1500 - Training Loss: 0.0080, Testing Loss: 0.0115\n",
            "Epoch: 1207/1500 - Training Loss: 0.0081, Testing Loss: 0.0116\n",
            "Epoch: 1208/1500 - Training Loss: 0.0080, Testing Loss: 0.0109\n",
            "Epoch: 1209/1500 - Training Loss: 0.0079, Testing Loss: 0.0109\n",
            "Epoch: 1210/1500 - Training Loss: 0.0079, Testing Loss: 0.0116\n",
            "Epoch: 1211/1500 - Training Loss: 0.0079, Testing Loss: 0.0112\n",
            "Epoch: 1212/1500 - Training Loss: 0.0083, Testing Loss: 0.0108\n",
            "Epoch: 1213/1500 - Training Loss: 0.0079, Testing Loss: 0.0110\n",
            "Epoch: 1214/1500 - Training Loss: 0.0081, Testing Loss: 0.0116\n",
            "Epoch: 1215/1500 - Training Loss: 0.0080, Testing Loss: 0.0116\n",
            "Epoch: 1216/1500 - Training Loss: 0.0080, Testing Loss: 0.0110\n",
            "Epoch: 1217/1500 - Training Loss: 0.0082, Testing Loss: 0.0110\n",
            "Epoch: 1218/1500 - Training Loss: 0.0081, Testing Loss: 0.0107\n",
            "Epoch: 1219/1500 - Training Loss: 0.0080, Testing Loss: 0.0108\n",
            "Epoch: 1220/1500 - Training Loss: 0.0080, Testing Loss: 0.0110\n",
            "Epoch: 1221/1500 - Training Loss: 0.0078, Testing Loss: 0.0109\n",
            "Epoch: 1222/1500 - Training Loss: 0.0080, Testing Loss: 0.0114\n",
            "Epoch: 1223/1500 - Training Loss: 0.0078, Testing Loss: 0.0108\n",
            "Epoch: 1224/1500 - Training Loss: 0.0078, Testing Loss: 0.0113\n",
            "Epoch: 1225/1500 - Training Loss: 0.0079, Testing Loss: 0.0119\n",
            "Epoch: 1226/1500 - Training Loss: 0.0080, Testing Loss: 0.0113\n",
            "Epoch: 1227/1500 - Training Loss: 0.0082, Testing Loss: 0.0109\n",
            "Epoch: 1228/1500 - Training Loss: 0.0079, Testing Loss: 0.0110\n",
            "Epoch: 1229/1500 - Training Loss: 0.0079, Testing Loss: 0.0109\n",
            "Epoch: 1230/1500 - Training Loss: 0.0079, Testing Loss: 0.0110\n",
            "Epoch: 1231/1500 - Training Loss: 0.0081, Testing Loss: 0.0115\n",
            "Epoch: 1232/1500 - Training Loss: 0.0079, Testing Loss: 0.0113\n",
            "Epoch: 1233/1500 - Training Loss: 0.0081, Testing Loss: 0.0112\n",
            "Epoch: 1234/1500 - Training Loss: 0.0079, Testing Loss: 0.0112\n",
            "Epoch: 1235/1500 - Training Loss: 0.0078, Testing Loss: 0.0116\n",
            "Epoch: 1236/1500 - Training Loss: 0.0079, Testing Loss: 0.0111\n",
            "Epoch: 1237/1500 - Training Loss: 0.0081, Testing Loss: 0.0107\n",
            "Epoch: 1238/1500 - Training Loss: 0.0080, Testing Loss: 0.0113\n",
            "Epoch: 1239/1500 - Training Loss: 0.0078, Testing Loss: 0.0113\n",
            "Epoch: 1240/1500 - Training Loss: 0.0078, Testing Loss: 0.0110\n",
            "Epoch: 1241/1500 - Training Loss: 0.0080, Testing Loss: 0.0116\n",
            "Epoch: 1242/1500 - Training Loss: 0.0080, Testing Loss: 0.0114\n",
            "Epoch: 1243/1500 - Training Loss: 0.0080, Testing Loss: 0.0109\n",
            "Epoch: 1244/1500 - Training Loss: 0.0080, Testing Loss: 0.0115\n",
            "Epoch: 1245/1500 - Training Loss: 0.0079, Testing Loss: 0.0113\n",
            "Epoch: 1246/1500 - Training Loss: 0.0080, Testing Loss: 0.0111\n",
            "Epoch: 1247/1500 - Training Loss: 0.0080, Testing Loss: 0.0110\n",
            "Epoch: 1248/1500 - Training Loss: 0.0079, Testing Loss: 0.0113\n",
            "Epoch: 1249/1500 - Training Loss: 0.0078, Testing Loss: 0.0116\n",
            "Epoch: 1250/1500 - Training Loss: 0.0080, Testing Loss: 0.0118\n",
            "Epoch: 1251/1500 - Training Loss: 0.0083, Testing Loss: 0.0109\n",
            "Epoch: 1252/1500 - Training Loss: 0.0080, Testing Loss: 0.0113\n",
            "Epoch: 1253/1500 - Training Loss: 0.0079, Testing Loss: 0.0110\n",
            "Epoch: 1254/1500 - Training Loss: 0.0079, Testing Loss: 0.0110\n",
            "Epoch: 1255/1500 - Training Loss: 0.0078, Testing Loss: 0.0110\n",
            "Epoch: 1256/1500 - Training Loss: 0.0079, Testing Loss: 0.0114\n",
            "Epoch: 1257/1500 - Training Loss: 0.0080, Testing Loss: 0.0116\n",
            "Epoch: 1258/1500 - Training Loss: 0.0078, Testing Loss: 0.0113\n",
            "Epoch: 1259/1500 - Training Loss: 0.0078, Testing Loss: 0.0111\n",
            "Epoch: 1260/1500 - Training Loss: 0.0079, Testing Loss: 0.0111\n",
            "Epoch: 1261/1500 - Training Loss: 0.0079, Testing Loss: 0.0114\n",
            "Epoch: 1262/1500 - Training Loss: 0.0081, Testing Loss: 0.0111\n",
            "Epoch: 1263/1500 - Training Loss: 0.0077, Testing Loss: 0.0113\n",
            "Epoch: 1264/1500 - Training Loss: 0.0080, Testing Loss: 0.0111\n",
            "Epoch: 1265/1500 - Training Loss: 0.0080, Testing Loss: 0.0111\n",
            "Epoch: 1266/1500 - Training Loss: 0.0079, Testing Loss: 0.0110\n",
            "Epoch: 1267/1500 - Training Loss: 0.0078, Testing Loss: 0.0112\n",
            "Epoch: 1268/1500 - Training Loss: 0.0078, Testing Loss: 0.0110\n",
            "Epoch: 1269/1500 - Training Loss: 0.0079, Testing Loss: 0.0115\n",
            "Epoch: 1270/1500 - Training Loss: 0.0081, Testing Loss: 0.0109\n",
            "Epoch: 1271/1500 - Training Loss: 0.0082, Testing Loss: 0.0113\n",
            "Epoch: 1272/1500 - Training Loss: 0.0079, Testing Loss: 0.0113\n",
            "Epoch: 1273/1500 - Training Loss: 0.0079, Testing Loss: 0.0116\n",
            "Epoch: 1274/1500 - Training Loss: 0.0079, Testing Loss: 0.0112\n",
            "Epoch: 1275/1500 - Training Loss: 0.0078, Testing Loss: 0.0112\n",
            "Epoch: 1276/1500 - Training Loss: 0.0078, Testing Loss: 0.0115\n",
            "Epoch: 1277/1500 - Training Loss: 0.0079, Testing Loss: 0.0113\n",
            "Epoch: 1278/1500 - Training Loss: 0.0078, Testing Loss: 0.0111\n",
            "Epoch: 1279/1500 - Training Loss: 0.0079, Testing Loss: 0.0110\n",
            "Epoch: 1280/1500 - Training Loss: 0.0078, Testing Loss: 0.0111\n",
            "Epoch: 1281/1500 - Training Loss: 0.0079, Testing Loss: 0.0117\n",
            "Epoch: 1282/1500 - Training Loss: 0.0080, Testing Loss: 0.0109\n",
            "Epoch: 1283/1500 - Training Loss: 0.0079, Testing Loss: 0.0116\n",
            "Epoch: 1284/1500 - Training Loss: 0.0078, Testing Loss: 0.0113\n",
            "Epoch: 1285/1500 - Training Loss: 0.0080, Testing Loss: 0.0115\n",
            "Epoch: 1286/1500 - Training Loss: 0.0080, Testing Loss: 0.0111\n",
            "Epoch: 1287/1500 - Training Loss: 0.0079, Testing Loss: 0.0112\n",
            "Epoch: 1288/1500 - Training Loss: 0.0081, Testing Loss: 0.0110\n",
            "Epoch: 1289/1500 - Training Loss: 0.0078, Testing Loss: 0.0125\n",
            "Epoch: 1290/1500 - Training Loss: 0.0079, Testing Loss: 0.0118\n",
            "Epoch: 1291/1500 - Training Loss: 0.0079, Testing Loss: 0.0113\n",
            "Epoch: 1292/1500 - Training Loss: 0.0078, Testing Loss: 0.0111\n",
            "Epoch: 1293/1500 - Training Loss: 0.0077, Testing Loss: 0.0113\n",
            "Epoch: 1294/1500 - Training Loss: 0.0080, Testing Loss: 0.0110\n",
            "Epoch: 1295/1500 - Training Loss: 0.0079, Testing Loss: 0.0110\n",
            "Epoch: 1296/1500 - Training Loss: 0.0078, Testing Loss: 0.0114\n",
            "Epoch: 1297/1500 - Training Loss: 0.0080, Testing Loss: 0.0109\n",
            "Epoch: 1298/1500 - Training Loss: 0.0079, Testing Loss: 0.0117\n",
            "Epoch: 1299/1500 - Training Loss: 0.0078, Testing Loss: 0.0110\n",
            "Epoch: 1300/1500 - Training Loss: 0.0079, Testing Loss: 0.0111\n",
            "Epoch: 1301/1500 - Training Loss: 0.0079, Testing Loss: 0.0115\n",
            "Epoch: 1302/1500 - Training Loss: 0.0079, Testing Loss: 0.0117\n",
            "Epoch: 1303/1500 - Training Loss: 0.0079, Testing Loss: 0.0113\n",
            "Epoch: 1304/1500 - Training Loss: 0.0078, Testing Loss: 0.0111\n",
            "Epoch: 1305/1500 - Training Loss: 0.0078, Testing Loss: 0.0115\n",
            "Epoch: 1306/1500 - Training Loss: 0.0078, Testing Loss: 0.0113\n",
            "Epoch: 1307/1500 - Training Loss: 0.0078, Testing Loss: 0.0115\n",
            "Epoch: 1308/1500 - Training Loss: 0.0079, Testing Loss: 0.0109\n",
            "Epoch: 1309/1500 - Training Loss: 0.0077, Testing Loss: 0.0109\n",
            "Epoch: 1310/1500 - Training Loss: 0.0078, Testing Loss: 0.0113\n",
            "Epoch: 1311/1500 - Training Loss: 0.0078, Testing Loss: 0.0112\n",
            "Epoch: 1312/1500 - Training Loss: 0.0079, Testing Loss: 0.0110\n",
            "Epoch: 1313/1500 - Training Loss: 0.0079, Testing Loss: 0.0116\n",
            "Epoch: 1314/1500 - Training Loss: 0.0078, Testing Loss: 0.0111\n",
            "Epoch: 1315/1500 - Training Loss: 0.0078, Testing Loss: 0.0109\n",
            "Epoch: 1316/1500 - Training Loss: 0.0077, Testing Loss: 0.0108\n",
            "Epoch: 1317/1500 - Training Loss: 0.0080, Testing Loss: 0.0113\n",
            "Epoch: 1318/1500 - Training Loss: 0.0079, Testing Loss: 0.0113\n",
            "Epoch: 1319/1500 - Training Loss: 0.0080, Testing Loss: 0.0114\n",
            "Epoch: 1320/1500 - Training Loss: 0.0079, Testing Loss: 0.0112\n",
            "Epoch: 1321/1500 - Training Loss: 0.0078, Testing Loss: 0.0113\n",
            "Epoch: 1322/1500 - Training Loss: 0.0077, Testing Loss: 0.0112\n",
            "Epoch: 1323/1500 - Training Loss: 0.0079, Testing Loss: 0.0117\n",
            "Epoch: 1324/1500 - Training Loss: 0.0078, Testing Loss: 0.0110\n",
            "Epoch: 1325/1500 - Training Loss: 0.0078, Testing Loss: 0.0111\n",
            "Epoch: 1326/1500 - Training Loss: 0.0077, Testing Loss: 0.0114\n",
            "Epoch: 1327/1500 - Training Loss: 0.0079, Testing Loss: 0.0111\n",
            "Epoch: 1328/1500 - Training Loss: 0.0078, Testing Loss: 0.0114\n",
            "Epoch: 1329/1500 - Training Loss: 0.0078, Testing Loss: 0.0113\n",
            "Epoch: 1330/1500 - Training Loss: 0.0079, Testing Loss: 0.0111\n",
            "Epoch: 1331/1500 - Training Loss: 0.0078, Testing Loss: 0.0117\n",
            "Epoch: 1332/1500 - Training Loss: 0.0077, Testing Loss: 0.0110\n",
            "Epoch: 1333/1500 - Training Loss: 0.0078, Testing Loss: 0.0112\n",
            "Epoch: 1334/1500 - Training Loss: 0.0077, Testing Loss: 0.0111\n",
            "Epoch: 1335/1500 - Training Loss: 0.0078, Testing Loss: 0.0117\n",
            "Epoch: 1336/1500 - Training Loss: 0.0078, Testing Loss: 0.0113\n",
            "Epoch: 1337/1500 - Training Loss: 0.0076, Testing Loss: 0.0124\n",
            "Epoch: 1338/1500 - Training Loss: 0.0079, Testing Loss: 0.0108\n",
            "Epoch: 1339/1500 - Training Loss: 0.0078, Testing Loss: 0.0108\n",
            "Epoch: 1340/1500 - Training Loss: 0.0079, Testing Loss: 0.0110\n",
            "Epoch: 1341/1500 - Training Loss: 0.0077, Testing Loss: 0.0114\n",
            "Epoch: 1342/1500 - Training Loss: 0.0077, Testing Loss: 0.0114\n",
            "Epoch: 1343/1500 - Training Loss: 0.0077, Testing Loss: 0.0110\n",
            "Epoch: 1344/1500 - Training Loss: 0.0078, Testing Loss: 0.0112\n",
            "Epoch: 1345/1500 - Training Loss: 0.0078, Testing Loss: 0.0113\n",
            "Epoch: 1346/1500 - Training Loss: 0.0077, Testing Loss: 0.0117\n",
            "Epoch: 1347/1500 - Training Loss: 0.0079, Testing Loss: 0.0110\n",
            "Epoch: 1348/1500 - Training Loss: 0.0077, Testing Loss: 0.0112\n",
            "Epoch: 1349/1500 - Training Loss: 0.0077, Testing Loss: 0.0114\n",
            "Epoch: 1350/1500 - Training Loss: 0.0080, Testing Loss: 0.0129\n",
            "Epoch: 1351/1500 - Training Loss: 0.0078, Testing Loss: 0.0111\n",
            "Epoch: 1352/1500 - Training Loss: 0.0078, Testing Loss: 0.0111\n",
            "Epoch: 1353/1500 - Training Loss: 0.0078, Testing Loss: 0.0111\n",
            "Epoch: 1354/1500 - Training Loss: 0.0077, Testing Loss: 0.0109\n",
            "Epoch: 1355/1500 - Training Loss: 0.0078, Testing Loss: 0.0111\n",
            "Epoch: 1356/1500 - Training Loss: 0.0076, Testing Loss: 0.0112\n",
            "Epoch: 1357/1500 - Training Loss: 0.0080, Testing Loss: 0.0116\n",
            "Epoch: 1358/1500 - Training Loss: 0.0078, Testing Loss: 0.0116\n",
            "Epoch: 1359/1500 - Training Loss: 0.0078, Testing Loss: 0.0112\n",
            "Epoch: 1360/1500 - Training Loss: 0.0078, Testing Loss: 0.0119\n",
            "Epoch: 1361/1500 - Training Loss: 0.0077, Testing Loss: 0.0117\n",
            "Epoch: 1362/1500 - Training Loss: 0.0077, Testing Loss: 0.0113\n",
            "Epoch: 1363/1500 - Training Loss: 0.0078, Testing Loss: 0.0114\n",
            "Epoch: 1364/1500 - Training Loss: 0.0077, Testing Loss: 0.0111\n",
            "Epoch: 1365/1500 - Training Loss: 0.0078, Testing Loss: 0.0115\n",
            "Epoch: 1366/1500 - Training Loss: 0.0077, Testing Loss: 0.0113\n",
            "Epoch: 1367/1500 - Training Loss: 0.0079, Testing Loss: 0.0109\n",
            "Epoch: 1368/1500 - Training Loss: 0.0077, Testing Loss: 0.0110\n",
            "Epoch: 1369/1500 - Training Loss: 0.0079, Testing Loss: 0.0112\n",
            "Epoch: 1370/1500 - Training Loss: 0.0077, Testing Loss: 0.0116\n",
            "Epoch: 1371/1500 - Training Loss: 0.0078, Testing Loss: 0.0112\n",
            "Epoch: 1372/1500 - Training Loss: 0.0077, Testing Loss: 0.0111\n",
            "Epoch: 1373/1500 - Training Loss: 0.0076, Testing Loss: 0.0116\n",
            "Epoch: 1374/1500 - Training Loss: 0.0078, Testing Loss: 0.0113\n",
            "Epoch: 1375/1500 - Training Loss: 0.0078, Testing Loss: 0.0110\n",
            "Epoch: 1376/1500 - Training Loss: 0.0078, Testing Loss: 0.0111\n",
            "Epoch: 1377/1500 - Training Loss: 0.0077, Testing Loss: 0.0114\n",
            "Epoch: 1378/1500 - Training Loss: 0.0078, Testing Loss: 0.0108\n",
            "Epoch: 1379/1500 - Training Loss: 0.0077, Testing Loss: 0.0111\n",
            "Epoch: 1380/1500 - Training Loss: 0.0078, Testing Loss: 0.0123\n",
            "Epoch: 1381/1500 - Training Loss: 0.0078, Testing Loss: 0.0115\n",
            "Epoch: 1382/1500 - Training Loss: 0.0076, Testing Loss: 0.0115\n",
            "Epoch: 1383/1500 - Training Loss: 0.0077, Testing Loss: 0.0111\n",
            "Epoch: 1384/1500 - Training Loss: 0.0076, Testing Loss: 0.0112\n",
            "Epoch: 1385/1500 - Training Loss: 0.0078, Testing Loss: 0.0108\n",
            "Epoch: 1386/1500 - Training Loss: 0.0078, Testing Loss: 0.0125\n",
            "Epoch: 1387/1500 - Training Loss: 0.0078, Testing Loss: 0.0111\n",
            "Epoch: 1388/1500 - Training Loss: 0.0078, Testing Loss: 0.0111\n",
            "Epoch: 1389/1500 - Training Loss: 0.0076, Testing Loss: 0.0114\n",
            "Epoch: 1390/1500 - Training Loss: 0.0076, Testing Loss: 0.0111\n",
            "Epoch: 1391/1500 - Training Loss: 0.0077, Testing Loss: 0.0111\n",
            "Epoch: 1392/1500 - Training Loss: 0.0076, Testing Loss: 0.0111\n",
            "Epoch: 1393/1500 - Training Loss: 0.0078, Testing Loss: 0.0112\n",
            "Epoch: 1394/1500 - Training Loss: 0.0077, Testing Loss: 0.0119\n",
            "Epoch: 1395/1500 - Training Loss: 0.0077, Testing Loss: 0.0109\n",
            "Epoch: 1396/1500 - Training Loss: 0.0080, Testing Loss: 0.0111\n",
            "Epoch: 1397/1500 - Training Loss: 0.0076, Testing Loss: 0.0111\n",
            "Epoch: 1398/1500 - Training Loss: 0.0077, Testing Loss: 0.0114\n",
            "Epoch: 1399/1500 - Training Loss: 0.0077, Testing Loss: 0.0110\n",
            "Epoch: 1400/1500 - Training Loss: 0.0077, Testing Loss: 0.0111\n",
            "Epoch: 1401/1500 - Training Loss: 0.0077, Testing Loss: 0.0112\n",
            "Epoch: 1402/1500 - Training Loss: 0.0080, Testing Loss: 0.0112\n",
            "Epoch: 1403/1500 - Training Loss: 0.0078, Testing Loss: 0.0108\n",
            "Epoch: 1404/1500 - Training Loss: 0.0080, Testing Loss: 0.0123\n",
            "Epoch: 1405/1500 - Training Loss: 0.0076, Testing Loss: 0.0116\n",
            "Epoch: 1406/1500 - Training Loss: 0.0077, Testing Loss: 0.0107\n",
            "Epoch: 1407/1500 - Training Loss: 0.0076, Testing Loss: 0.0113\n",
            "Epoch: 1408/1500 - Training Loss: 0.0076, Testing Loss: 0.0115\n",
            "Epoch: 1409/1500 - Training Loss: 0.0078, Testing Loss: 0.0114\n",
            "Epoch: 1410/1500 - Training Loss: 0.0079, Testing Loss: 0.0113\n",
            "Epoch: 1411/1500 - Training Loss: 0.0081, Testing Loss: 0.0111\n",
            "Epoch: 1412/1500 - Training Loss: 0.0076, Testing Loss: 0.0113\n",
            "Epoch: 1413/1500 - Training Loss: 0.0076, Testing Loss: 0.0108\n",
            "Epoch: 1414/1500 - Training Loss: 0.0077, Testing Loss: 0.0107\n",
            "Epoch: 1415/1500 - Training Loss: 0.0080, Testing Loss: 0.0108\n",
            "Epoch: 1416/1500 - Training Loss: 0.0076, Testing Loss: 0.0108\n",
            "Epoch: 1417/1500 - Training Loss: 0.0076, Testing Loss: 0.0120\n",
            "Epoch: 1418/1500 - Training Loss: 0.0077, Testing Loss: 0.0115\n",
            "Epoch: 1419/1500 - Training Loss: 0.0077, Testing Loss: 0.0111\n",
            "Epoch: 1420/1500 - Training Loss: 0.0076, Testing Loss: 0.0114\n",
            "Epoch: 1421/1500 - Training Loss: 0.0077, Testing Loss: 0.0117\n",
            "Epoch: 1422/1500 - Training Loss: 0.0078, Testing Loss: 0.0115\n",
            "Epoch: 1423/1500 - Training Loss: 0.0077, Testing Loss: 0.0113\n",
            "Epoch: 1424/1500 - Training Loss: 0.0079, Testing Loss: 0.0117\n",
            "Epoch: 1425/1500 - Training Loss: 0.0078, Testing Loss: 0.0120\n",
            "Epoch: 1426/1500 - Training Loss: 0.0076, Testing Loss: 0.0113\n",
            "Epoch: 1427/1500 - Training Loss: 0.0076, Testing Loss: 0.0114\n",
            "Epoch: 1428/1500 - Training Loss: 0.0077, Testing Loss: 0.0113\n",
            "Epoch: 1429/1500 - Training Loss: 0.0077, Testing Loss: 0.0119\n",
            "Epoch: 1430/1500 - Training Loss: 0.0077, Testing Loss: 0.0113\n",
            "Epoch: 1431/1500 - Training Loss: 0.0078, Testing Loss: 0.0112\n",
            "Epoch: 1432/1500 - Training Loss: 0.0078, Testing Loss: 0.0113\n",
            "Epoch: 1433/1500 - Training Loss: 0.0077, Testing Loss: 0.0109\n",
            "Epoch: 1434/1500 - Training Loss: 0.0077, Testing Loss: 0.0112\n",
            "Epoch: 1435/1500 - Training Loss: 0.0077, Testing Loss: 0.0114\n",
            "Epoch: 1436/1500 - Training Loss: 0.0076, Testing Loss: 0.0111\n",
            "Epoch: 1437/1500 - Training Loss: 0.0076, Testing Loss: 0.0109\n",
            "Epoch: 1438/1500 - Training Loss: 0.0077, Testing Loss: 0.0111\n",
            "Epoch: 1439/1500 - Training Loss: 0.0079, Testing Loss: 0.0118\n",
            "Epoch: 1440/1500 - Training Loss: 0.0078, Testing Loss: 0.0113\n",
            "Epoch: 1441/1500 - Training Loss: 0.0078, Testing Loss: 0.0117\n",
            "Epoch: 1442/1500 - Training Loss: 0.0077, Testing Loss: 0.0120\n",
            "Epoch: 1443/1500 - Training Loss: 0.0077, Testing Loss: 0.0111\n",
            "Epoch: 1444/1500 - Training Loss: 0.0075, Testing Loss: 0.0111\n",
            "Epoch: 1445/1500 - Training Loss: 0.0077, Testing Loss: 0.0114\n",
            "Epoch: 1446/1500 - Training Loss: 0.0076, Testing Loss: 0.0111\n",
            "Epoch: 1447/1500 - Training Loss: 0.0078, Testing Loss: 0.0109\n",
            "Epoch: 1448/1500 - Training Loss: 0.0080, Testing Loss: 0.0118\n",
            "Epoch: 1449/1500 - Training Loss: 0.0078, Testing Loss: 0.0115\n",
            "Epoch: 1450/1500 - Training Loss: 0.0076, Testing Loss: 0.0110\n",
            "Epoch: 1451/1500 - Training Loss: 0.0076, Testing Loss: 0.0113\n",
            "Epoch: 1452/1500 - Training Loss: 0.0076, Testing Loss: 0.0109\n",
            "Epoch: 1453/1500 - Training Loss: 0.0076, Testing Loss: 0.0111\n",
            "Epoch: 1454/1500 - Training Loss: 0.0076, Testing Loss: 0.0110\n",
            "Epoch: 1455/1500 - Training Loss: 0.0077, Testing Loss: 0.0111\n",
            "Epoch: 1456/1500 - Training Loss: 0.0075, Testing Loss: 0.0114\n",
            "Epoch: 1457/1500 - Training Loss: 0.0077, Testing Loss: 0.0113\n",
            "Epoch: 1458/1500 - Training Loss: 0.0083, Testing Loss: 0.0106\n",
            "Epoch: 1459/1500 - Training Loss: 0.0076, Testing Loss: 0.0113\n",
            "Epoch: 1460/1500 - Training Loss: 0.0077, Testing Loss: 0.0109\n",
            "Epoch: 1461/1500 - Training Loss: 0.0077, Testing Loss: 0.0112\n",
            "Epoch: 1462/1500 - Training Loss: 0.0077, Testing Loss: 0.0114\n",
            "Epoch: 1463/1500 - Training Loss: 0.0077, Testing Loss: 0.0116\n",
            "Epoch: 1464/1500 - Training Loss: 0.0077, Testing Loss: 0.0110\n",
            "Epoch: 1465/1500 - Training Loss: 0.0077, Testing Loss: 0.0110\n",
            "Epoch: 1466/1500 - Training Loss: 0.0077, Testing Loss: 0.0109\n",
            "Epoch: 1467/1500 - Training Loss: 0.0076, Testing Loss: 0.0113\n",
            "Epoch: 1468/1500 - Training Loss: 0.0078, Testing Loss: 0.0118\n",
            "Epoch: 1469/1500 - Training Loss: 0.0076, Testing Loss: 0.0113\n",
            "Epoch: 1470/1500 - Training Loss: 0.0076, Testing Loss: 0.0111\n",
            "Epoch: 1471/1500 - Training Loss: 0.0076, Testing Loss: 0.0112\n",
            "Epoch: 1472/1500 - Training Loss: 0.0077, Testing Loss: 0.0111\n",
            "Epoch: 1473/1500 - Training Loss: 0.0075, Testing Loss: 0.0111\n",
            "Epoch: 1474/1500 - Training Loss: 0.0076, Testing Loss: 0.0109\n",
            "Epoch: 1475/1500 - Training Loss: 0.0075, Testing Loss: 0.0111\n",
            "Epoch: 1476/1500 - Training Loss: 0.0077, Testing Loss: 0.0107\n",
            "Epoch: 1477/1500 - Training Loss: 0.0077, Testing Loss: 0.0112\n",
            "Epoch: 1478/1500 - Training Loss: 0.0076, Testing Loss: 0.0111\n",
            "Epoch: 1479/1500 - Training Loss: 0.0076, Testing Loss: 0.0108\n",
            "Epoch: 1480/1500 - Training Loss: 0.0076, Testing Loss: 0.0114\n",
            "Epoch: 1481/1500 - Training Loss: 0.0075, Testing Loss: 0.0131\n",
            "Epoch: 1482/1500 - Training Loss: 0.0079, Testing Loss: 0.0108\n",
            "Epoch: 1483/1500 - Training Loss: 0.0077, Testing Loss: 0.0108\n",
            "Epoch: 1484/1500 - Training Loss: 0.0077, Testing Loss: 0.0111\n",
            "Epoch: 1485/1500 - Training Loss: 0.0078, Testing Loss: 0.0118\n",
            "Epoch: 1486/1500 - Training Loss: 0.0077, Testing Loss: 0.0112\n",
            "Epoch: 1487/1500 - Training Loss: 0.0075, Testing Loss: 0.0113\n",
            "Epoch: 1488/1500 - Training Loss: 0.0076, Testing Loss: 0.0109\n",
            "Epoch: 1489/1500 - Training Loss: 0.0075, Testing Loss: 0.0118\n",
            "Epoch: 1490/1500 - Training Loss: 0.0076, Testing Loss: 0.0111\n",
            "Epoch: 1491/1500 - Training Loss: 0.0075, Testing Loss: 0.0115\n",
            "Epoch: 1492/1500 - Training Loss: 0.0076, Testing Loss: 0.0109\n",
            "Epoch: 1493/1500 - Training Loss: 0.0076, Testing Loss: 0.0109\n",
            "Epoch: 1494/1500 - Training Loss: 0.0077, Testing Loss: 0.0108\n",
            "Epoch: 1495/1500 - Training Loss: 0.0077, Testing Loss: 0.0110\n",
            "Epoch: 1496/1500 - Training Loss: 0.0077, Testing Loss: 0.0118\n",
            "Epoch: 1497/1500 - Training Loss: 0.0076, Testing Loss: 0.0107\n",
            "Epoch: 1498/1500 - Training Loss: 0.0076, Testing Loss: 0.0111\n",
            "Epoch: 1499/1500 - Training Loss: 0.0075, Testing Loss: 0.0113\n",
            "Epoch: 1500/1500 - Training Loss: 0.0076, Testing Loss: 0.0111\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Making Predictions with Trained Model**"
      ],
      "metadata": {
        "id": "BM8t0SaxdvFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to evaluate the model on testing data\n",
        "def eval_test_data(house_prices_nn, test_loader, device):\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    house_prices_nn.eval()\n",
        "\n",
        "    # Make sure house_prices_nn is on the same device\n",
        "    house_prices_nn.to(device)\n",
        "\n",
        "    # Empty lists to store predictions and true values\n",
        "    predictions = []\n",
        "    true_values = []\n",
        "\n",
        "    with torch.no_grad():  # Turn off gradient calculation for creating predictions\n",
        "        for X_test, y_test in test_loader:\n",
        "            X_test = X_test.to(device)\n",
        "            y_test = y_test.to(device)\n",
        "            y_pred = house_prices_nn(X_test)  # Forward pass\n",
        "\n",
        "            # If needed, use squeeze() to ensure consistent dimensions\n",
        "            y_pred = y_pred.squeeze()\n",
        "            y_test = y_test.squeeze()\n",
        "\n",
        "            # Convert to numpy arrays and store\n",
        "            predictions.append(y_pred.cpu().numpy())\n",
        "            true_values.append(y_test.cpu().numpy())\n",
        "\n",
        "    # Concatenate all batches in a large array of values\n",
        "    predictions = np.concatenate(predictions)\n",
        "    true_values = np.concatenate(true_values)\n",
        "\n",
        "    return predictions, true_values  # Return both predictions and true values\n",
        "\n",
        "def evaluate_metrics(predictions, true_values):\n",
        "    # Calculate MAE, RMSE, and R^2\n",
        "    mae = mean_absolute_error(true_values, predictions)\n",
        "    rmse = np.sqrt(mean_squared_error(true_values, predictions))\n",
        "    r2 = r2_score(true_values, predictions)\n",
        "\n",
        "    return mae, rmse, r2\n",
        "\n",
        "# Call evaluation function\n",
        "predicted_prices, actual_prices = eval_test_data(house_prices_nn, test_loader, device)\n",
        "\n",
        "# Calculate metrics function\n",
        "mae, rmse, r2 = evaluate_metrics(predicted_prices, actual_prices)\n",
        "\n",
        "# Print predicted vs actual prices for the first 20 houses\n",
        "print(\"Predicted vs Actual Housing Prices:\")\n",
        "for i in range(min(50, len(predicted_prices))):\n",
        "    print(f\"Predicted: ${predicted_prices[i] * 100000:.2f}, Actual: ${actual_prices[i] * 100000:.2f}\")\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {mae * 100:.2f}%\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"R²: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "CROhuncRelE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f3a7ec6-7b3b-426b-b975-e339527ea3f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted vs Actual Housing Prices:\n",
            "Predicted: $37185.59, Actual: $34804.19\n",
            "Predicted: $76216.89, Actual: $94102.91\n",
            "Predicted: $19467.07, Actual: $19876.41\n",
            "Predicted: $36331.20, Actual: $32185.64\n",
            "Predicted: $6571.63, Actual: $12927.99\n",
            "Predicted: $40742.75, Actual: $48659.80\n",
            "Predicted: $62670.55, Actual: $100000.00\n",
            "Predicted: $80198.98, Actual: $68968.99\n",
            "Predicted: $86818.34, Actual: $100000.00\n",
            "Predicted: $22411.79, Actual: $26000.10\n",
            "Predicted: $67561.72, Actual: $65566.95\n",
            "Predicted: $55666.51, Actual: $53072.16\n",
            "Predicted: $13815.02, Actual: $14948.60\n",
            "Predicted: $31807.42, Actual: $28886.69\n",
            "Predicted: $49658.48, Actual: $62185.52\n",
            "Predicted: $14773.17, Actual: $16371.27\n",
            "Predicted: $48597.84, Actual: $56556.67\n",
            "Predicted: $54206.32, Actual: $54701.01\n",
            "Predicted: $12662.90, Actual: $17072.30\n",
            "Predicted: $43925.98, Actual: $43484.56\n",
            "Predicted: $58729.17, Actual: $71670.02\n",
            "Predicted: $37169.17, Actual: $22165.06\n",
            "Predicted: $33517.18, Actual: $69072.09\n",
            "Predicted: $19306.96, Actual: $20969.19\n",
            "Predicted: $26837.58, Actual: $27938.23\n",
            "Predicted: $55970.95, Actual: $50082.48\n",
            "Predicted: $38544.46, Actual: $48329.91\n",
            "Predicted: $20364.46, Actual: $15938.29\n",
            "Predicted: $23221.54, Actual: $19897.03\n",
            "Predicted: $23167.21, Actual: $23690.83\n",
            "Predicted: $33020.18, Actual: $26515.56\n",
            "Predicted: $31350.71, Actual: $34701.09\n",
            "Predicted: $11634.98, Actual: $41587.66\n",
            "Predicted: $30330.99, Actual: $31360.90\n",
            "Predicted: $41995.91, Actual: $36082.53\n",
            "Predicted: $37200.94, Actual: $30515.54\n",
            "Predicted: $16798.18, Actual: $32350.59\n",
            "Predicted: $30447.36, Actual: $30412.45\n",
            "Predicted: $22044.10, Actual: $17773.33\n",
            "Predicted: $12048.16, Actual: $10969.23\n",
            "Predicted: $32790.88, Actual: $32989.76\n",
            "Predicted: $64423.80, Actual: $53917.51\n",
            "Predicted: $39097.25, Actual: $45979.40\n",
            "Predicted: $46454.73, Actual: $53340.19\n",
            "Predicted: $91516.66, Actual: $100000.00\n",
            "Predicted: $27034.88, Actual: $32659.87\n",
            "Predicted: $32395.10, Actual: $77484.42\n",
            "Predicted: $49272.39, Actual: $44701.05\n",
            "Predicted: $29634.97, Actual: $2742.46\n",
            "Predicted: $47050.77, Actual: $27360.92\n",
            "Mean Absolute Error (MAE): 6.89%\n",
            "Root Mean Squared Error (RMSE): 0.1054\n",
            "R²: 0.8008\n"
          ]
        }
      ]
    }
  ]
}